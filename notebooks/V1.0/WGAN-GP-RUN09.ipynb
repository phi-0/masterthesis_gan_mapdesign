{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DwarfGAN - Deep Learning based Map Design for Dwarf Fortress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from keras.layers import Add, Concatenate, Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D, LayerNormalization\n",
    "from keras.layers.experimental import preprocessing\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.losses import binary_crossentropy, Loss\n",
    "from keras import metrics\n",
    "from functools import partial\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import random\n",
    "\n",
    "#!pip install boto3\n",
    "import boto3 as b3 \n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "############### CONFIG ###################\n",
    "\n",
    "# model name\n",
    "model_name = 'dwarfganWGANGPR09Tiles'\n",
    "# folder path to input files (map images)\n",
    "fpath = r'/data2/input'\n",
    "# folder path to tensorboard output\n",
    "tboard_dir = '/data2/output/tensorboard'\n",
    "# folder path for saved model output\n",
    "out_model_dir = '/data2/output/models'\n",
    "# folder for images to be saved during training\n",
    "out_img_dir = '/data2/output/images'\n",
    "# frequency of checkpoint saves (images, model weights) in epochs\n",
    "CHECKPOINT = 50\n",
    "# use skip connections (additive)?\n",
    "SKIP_ADD = False\n",
    "SKIP_CONCAT = False\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 1000 \n",
    "#BATCH_PER_EPOCH = 20\n",
    "# pre-processed (cropped) images are 1024x1024. We will later resize the images to 256x256 due to memory restrictions.\n",
    "IMAGE_SIZE = (12,12)\n",
    "BATCH_SIZE = 128\n",
    "CRITIC_FACTOR = 5 # number of times the critic is trained more often than the generator. Recommended = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "RELU_SLOPE_C = 0.2\n",
    "RELU_SLOPE_G = 0.2\n",
    "DROPOUT_C = 0.3\n",
    "MOMENTUM_G = 0.9\n",
    "CRIT_LR = 0.0003 # Adjusted learning rates according to two time-scale update rule (TTUR), see Heusel et al., 2017\n",
    "GEN_LR = 0.0001\n",
    "\n",
    "# NOTE: all extracted map PNGs have been saved on a separate virtual disk mapped to '/data' of the virtual machine in use\n",
    "data_dir = pathlib.Path(fpath + '/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Train / Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map images sourced from the DFMA come in a variety of dimensions. In order to create sample images with constant dimensions, as required by tensors, the 100k input samples were run through a python script to randomly crop 10 1024 x 1024 areas per picture. Of those cropped (sub-)images, only the ones which contain structures were retained. This was achieved by filtering out image crops which only contained two or less different colors. With that, the logic mainly filterd out crops which only contained black. This process resulted in 700'000+ (sub-)image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12068 cropped image samples available\n"
     ]
    }
   ],
   "source": [
    "# use pre-processed (cropped) 128 x 128 images\n",
    "data_dir = pathlib.Path(fpath + '/ascii_crops_12/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "print(f'There are {str(len(imgs))} cropped image samples available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random sample input image:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAIAAADZF8uwAAAAMUlEQVR4nGNgoAJIwCqag0v5fmrYicd8kkA8Gn8fA0M2qogZaQZmE1aCF2C3bh8WMQDzaAUVvKH48QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=12x12 at 0x7FDB900F74A8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show example sample image (cropped to 128x128)\n",
    "print('A random sample input image:')\n",
    "PIL.Image.open(imgs[random.randint(0,len(imgs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12068 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# creating keras datasets for training and validation - refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "dataset_train = tf.keras.preprocessing.image_dataset_from_directory(  fpath+'/ascii_crops_12',\n",
    "                                                                      image_size=IMAGE_SIZE, \n",
    "                                                                      batch_size=BATCH_SIZE, \n",
    "                                                                      labels=[1.] * len(imgs), # setting all labels to 1.0 (for 'real') as float32\n",
    "                                                                      #label_mode=None, # yields float32 type labels\n",
    "                                                                      seed=1234 #,\n",
    "                                                                   )\n",
    "\n",
    "# refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = dataset_train.cache().prefetch(buffer_size=BATCH_SIZE)\n",
    "#val_ds = dataset_val.cache().prefetch(buffer_size=BATCH_SIZE)\n",
    "#val_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAINING = 12068 # = 20% of total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Random Sample from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check random images from prepared batches\n",
    "plt.figure(figsize=(5, 5))\n",
    "for images, labels in train_ds.take(1): # take one batch. Here batch_size = 128 examples per batch\n",
    "    for i in range(9): # show first 9 images of batch\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(f'img {i}')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATED Model Definition\n",
    "\n",
    "Generally the following changes have been implemented to the architectures:\n",
    "\n",
    "### RUN09\n",
    "\n",
    "- instead of generating full maps, this model focuses on learning to recreate single tiles which represent the different objects/elements in the game world\n",
    "- implemented secondary input for both critic and generator which introduces the single tiles cropped from the tileset files (12x10) as a 12x10x256 vector (256 different 12x10 tiles) \n",
    "\n",
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discriminator_model():\n",
    "\n",
    "    # DISCRIMINATOR\n",
    "    # set input variables to variable width + height. Will be cropped in preprocessing [CURRENTLY FIXED TO 256x256]\n",
    "    input_dim = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "\n",
    "    # Input\n",
    "    d_input = Input(shape=input_dim, name='Discriminator_Input')\n",
    "    \n",
    "    \n",
    "\n",
    "    # ---- REMOVED FOR 256x256 NETWORK ----------\n",
    "    # Keras-based preprocessing. Alternative: RandomCrop()\n",
    "    # use smart_resizing?\n",
    "    #x = tf.keras.preprocessing.image.smart_resize(d_input, (1024, 1024))\n",
    "    #x = preprocessing.Resizing(width=512, \n",
    "    #                           height=512, \n",
    "    #                           name='Preprocessing_Resize'\n",
    "    #                          )(d_input) # Resize to 512 x 512 images\n",
    "\n",
    "    #we crop the images to 12x10 to match the tile dimensions\n",
    "    x = preprocessing.RandomCrop(height=12, \n",
    "                                width=10, \n",
    "                                name = 'Preprocessing_RandomCrop'\n",
    "                               )(d_input)\n",
    "\n",
    "    x = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale'\n",
    "                               )(x) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    # START TILES INPUT\n",
    "    tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "    \n",
    "    tiles = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale_Tiles'\n",
    "                               )(tiles_input) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    \n",
    "    #tiles = Conv2D(filters=256, kernel_size=(1,1), strides=1)(tiles)\n",
    "    \n",
    "    x = Concatenate()([x, tiles])\n",
    "    # END TILES INPUT\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 0\n",
    "    x = Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = (3,3), \n",
    "            strides = 1,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_0'\n",
    "    )(x)\n",
    "    \n",
    "    # Activation 0 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_0')(x)\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 1\n",
    "    x = Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_1'\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 1\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 1 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_1')(x)\n",
    "\n",
    "    # Dropout 1\n",
    "    x = Dropout(rate = DROPOUT_C)(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Conv2D Layer 2\n",
    "    x = Conv2D(\n",
    "            filters = 256,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            name = 'Discriminator_Conv2D_Layer_3',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 2\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 2 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_3')(x)\n",
    "\n",
    "\n",
    "    # OUTPUT\n",
    "    x = Flatten()(x)\n",
    "    #x = Dropout(DROPOUT_C)(x)\n",
    "    \n",
    "    d_output = Dense(1, \n",
    "                     #activation='sigmoid', \n",
    "                     kernel_initializer = RandomNormal(mean=0, stddev=0.02) # random initialization of weights with normal distribution around 0 with small SD\n",
    "                    )(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Discriminator Model intialization\n",
    "    discriminator = Model([d_input, tiles_input], d_output, name='Discriminator')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Discriminator_Input (InputLayer [(None, 12, 12, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_RandomCrop (Rando (None, 12, 10, 3)    0           Discriminator_Input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Tiles_Input (InputLayer)        [(None, 12, 10, 256) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_Rescale (Rescalin (None, 12, 10, 3)    0           Preprocessing_RandomCrop[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_Rescale_Tiles (Re (None, 12, 10, 256)  0           Tiles_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 12, 10, 259)  0           Preprocessing_Rescale[0][0]      \n",
      "                                                                 Preprocessing_Rescale_Tiles[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_0 (C (None, 12, 10, 64)   149248      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_0 (LeakyReLU)        (None, 12, 10, 64)   0           Discriminator_Conv2D_Layer_0[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_1 (C (None, 6, 5, 128)    73856       Activation_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_1 (LeakyReLU)        (None, 6, 5, 128)    0           Discriminator_Conv2D_Layer_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 6, 5, 128)    0           Activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_3 (C (None, 3, 3, 256)    295168      dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Activation_3 (LeakyReLU)        (None, 3, 3, 256)    0           Discriminator_Conv2D_Layer_3[0][0\n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 2304)         0           Activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 1)            2305        flatten_22[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 520,577\n",
      "Trainable params: 520,577\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = discriminator_model()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    # GENERATOR\n",
    "\n",
    "    # set input variable dimensions. Here we will start out with a vector of length 100 for each sample (sampled from a normal distribution, representing the learned latent space)\n",
    "    input_dim = (LATENT_DIM)\n",
    "\n",
    "    # Input\n",
    "    g_input = Input(shape=input_dim, name='Generator_Input')\n",
    "\n",
    "    # Dense Layer 1\n",
    "    x = Dense(np.prod([3,3,512]), kernel_initializer = RandomNormal(mean=0., stddev=0.02), \n",
    "              use_bias=False)(g_input) # use_bias=False see https://keras.io/examples/generative/wgan_gp/\n",
    "\n",
    "    # Batch Norm Layer 1\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    \n",
    "    # Activation Layer 1\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Reshape into 3D tensor\n",
    "    x = Reshape((3,3,512))(x)\n",
    "\n",
    "    # Upsampling Layer 1 + Conv2D Layer1\n",
    "    x = Conv2DTranspose(filters=512, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=256, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=(3,3), padding='same', strides=(1,1), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    # START TILES\n",
    "    tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "    tiles = preprocessing.Resizing(width=12, height=12)(tiles_input)\n",
    "    \n",
    "    x = Concatenate()([x, tiles])\n",
    "    \n",
    "    # END TILES\n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "    \n",
    "    # reduce output dimensions via 1x1 convolution\n",
    "    x = Conv2D(filters=3, kernel_size=(1,1), padding='same', strides=(1,1),\n",
    "               kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "        \n",
    "    # tanh activation layer to scale values to [-1:1]\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    # Batch Norm Layer 7\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "\n",
    "    # Output - Rescale Values back to [0:255] since the discriminator will automatically rescale back down to [-1:1] as part of the pre-processing pipeline\n",
    "    g_output = (255 / 2) * (x + 1) \n",
    "\n",
    "\n",
    "    # Generator Model initialization\n",
    "    generator = Model([g_input, tiles_input], g_output, name='Generator')\n",
    "    \n",
    "    \n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Generator_Input (InputLayer)    [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 4608)         589824      Generator_Input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4608)         18432       dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, 4608)         0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 3, 3, 512)    0           leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_39 (Conv2DTran (None, 6, 6, 512)    2359296     reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, 6, 6, 512)    1024        conv2d_transpose_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, 6, 6, 512)    0           layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_40 (Conv2DTran (None, 12, 12, 256)  1179648     leaky_re_lu_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, 12, 12, 256)  512         conv2d_transpose_40[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, 12, 12, 256)  0           layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Tiles_Input (InputLayer)        [(None, 12, 10, 256) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_41 (Conv2DTran (None, 12, 12, 128)  294912      leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "resizing (Resizing)             (None, 12, 12, 256)  0           Tiles_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 12, 12, 384)  0           conv2d_transpose_41[0][0]        \n",
      "                                                                 resizing[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, 12, 12, 384)  768         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, 12, 12, 384)  0           layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 12, 12, 3)    1152        leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 12, 12, 3)    0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_15 (TensorFlo [(None, 12, 12, 3)]  0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_15 (TensorFlowO [(None, 12, 12, 3)]  0           tf_op_layer_AddV2_15[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 4,445,568\n",
      "Trainable params: 4,436,352\n",
      "Non-trainable params: 9,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen = generator_model()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tiles\n",
    "\n",
    "We load the 256 split 12x10 tiles into a (12,10,256) numpy array to feed to the network at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random sample input image:\n",
      "Putting together a (12,10,256) np.array\n",
      "Created numpy array with shape: (12, 10, 256)\n"
     ]
    }
   ],
   "source": [
    "tiles = []\n",
    "\n",
    "data_dir = pathlib.Path('/data2/input/tiles/800x600/')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "# show example sample image (cropped to 128x128)\n",
    "print('A random sample input image:')\n",
    "PIL.Image.open(imgs[random.randint(0,len(imgs))])\n",
    "\n",
    "print('Putting together a (12,10,256) np.array')\n",
    "for img in imgs:\n",
    "    tiles.append(np.asarray(PIL.Image.open(img)).astype('uint8'))\n",
    "    \n",
    "\n",
    "ds_tiles = np.array(tiles)\n",
    "ds_tiles = ds_tiles.reshape((12,10,256))\n",
    "print(f'Created numpy array with shape: {ds_tiles.shape}')\n",
    "\n",
    "\n",
    "\n",
    "tiles_ds = tf.data.Dataset.from_tensor_slices(ds_tiles)\n",
    "\n",
    "tiles_ds = tiles_ds.cache().prefetch(buffer_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP (Full) Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we compile the models, we need to implement a custom loss function which uses the Wasserstein distance and a gradient penalty term in order to ensure 1 Lipschitz constraints are followed. A WGAN with GP further involves a slightly more complicated training process which trains the critic (discriminator without sigmoid activation function) by feeding three different kinds of images:\n",
    "\n",
    "1. real images (i.e. available samples)\n",
    "2. 'fake' images (i.e. constructed by the generator)\n",
    "3. random interpolations between real and fake images (i.e. random samples interpolated from values between the fake and real images)\n",
    "\n",
    "The full training process of a critic is depicted below (source: Foster, 2019, p. 122):\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"wgan_gp_critic_training.png\"></img>\n",
    "    <i>Computational Graph for one Discriminator Training Epoch. (Source: Foster, 2019, p.122)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below roughly follows the OOP-based framework set by keras see https://keras.io/examples/generative/wgan_gp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope(): \n",
    "\n",
    "    critic = discriminator_model()\n",
    "    generator = generator_model()\n",
    "\n",
    "    class WGANGP(keras.Model):\n",
    "        def __init__(\n",
    "            self,\n",
    "            critic,\n",
    "            generator,\n",
    "            latent_dim,\n",
    "            tensorboard_callback,\n",
    "            critic_extra_steps=5,\n",
    "            gp_weight=10.0\n",
    "        ):\n",
    "            super(WGANGP, self).__init__()\n",
    "            self.critic = critic\n",
    "            self.generator = generator\n",
    "            self.latent_dim = latent_dim\n",
    "            self.tensorboard_callback = tensorboard_callback\n",
    "            self.d_steps = critic_extra_steps\n",
    "            self.gp_weight = gp_weight\n",
    "            \n",
    "\n",
    "        def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "            super(WGANGP, self).compile()\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.g_optimizer = g_optimizer\n",
    "            self.d_loss_fn = d_loss_fn\n",
    "            self.g_loss_fn = g_loss_fn\n",
    "\n",
    "        def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "            \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "            This loss is calculated on an interpolated image\n",
    "            and added to the discriminator loss.\n",
    "            \"\"\"\n",
    "            # Get the interpolated image\n",
    "            alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "            diff = fake_images - real_images\n",
    "            interpolated = real_images + alpha * diff\n",
    "\n",
    "            with tf.GradientTape() as gp_tape:\n",
    "                gp_tape.watch(interpolated)\n",
    "                # 1. Get the discriminator output for this interpolated image.\n",
    "                pred = self.critic(interpolated, training=True)\n",
    "\n",
    "            # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "            grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "            # 3. Calculate the norm of the gradients.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "            gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "            return gp\n",
    "\n",
    "        def train_step(self, real_images):\n",
    "            #checking whether we handed a tuple of (numpy) data to .fit().\n",
    "            #if not, the data must be a tf.data.Dataset generator that yields batches of datasets (data, labels)\n",
    "            if isinstance(real_images, tuple):\n",
    "                real_images = real_images[0]\n",
    "\n",
    "            # Get the batch size\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "            # For each batch, we are going to perform the\n",
    "            # following steps as laid out in the original paper:\n",
    "            # 1. Train the generator and get the generator loss\n",
    "            # 2. Train the discriminator and get the discriminator loss\n",
    "            # 3. Calculate the gradient penalty\n",
    "            # 4. Multiply this gradient penalty with a constant weight factor = self.discriminator_extra_steps = 5 (default value)\n",
    "            # 5. Add the gradient penalty to the discriminator loss\n",
    "            # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "            # Train the discriminator first. The original paper recommends training\n",
    "            # the discriminator for `x` more steps (typically 5) as compared to generator\n",
    "            \n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.critic)\n",
    "            \n",
    "            for i in range(self.d_steps):\n",
    "                # Get the latent vector\n",
    "                random_latent_vectors = tf.random.normal(\n",
    "                    shape=(batch_size, self.latent_dim)\n",
    "                )\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Generate fake images from the latent vector\n",
    "                    fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                    # Get the logits for the fake images\n",
    "                    fake_logits = self.critic(fake_images, training=True)\n",
    "                    # Get the logits for the real images\n",
    "                    real_logits = self.critic(real_images, training=True)\n",
    "\n",
    "                    # Calculate the discriminator loss using the fake and real image logits\n",
    "                    d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                    # Calculate the gradient penalty\n",
    "                    gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                    # Add the gradient penalty to the original discriminator loss\n",
    "                    d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "                # Get the gradients w.r.t the discriminator loss\n",
    "                d_gradient = tape.gradient(d_loss, self.critic.trainable_variables)\n",
    "                # Update the weights of the discriminator using the discriminator optimizer\n",
    "                self.d_optimizer.apply_gradients(\n",
    "                    zip(d_gradient, self.critic.trainable_variables)\n",
    "                )\n",
    "\n",
    "            # Train the generator\n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.generator)\n",
    "            \n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images using the generator\n",
    "                generated_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the discriminator logits for fake images\n",
    "                gen_img_logits = self.critic(generated_images, training=True)\n",
    "                # Calculate the generator loss\n",
    "                g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "            # Get the gradients w.r.t the generator loss\n",
    "            gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "            # Update the weights of the generator using the generator optimizer\n",
    "            self.g_optimizer.apply_gradients(\n",
    "                zip(gen_gradient, self.generator.trainable_variables)\n",
    "            )\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "        \n",
    "    class GANMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, num_img=5, latent_dim=128):\n",
    "            self.num_img = num_img\n",
    "            self.latent_dim = latent_dim\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None): #on_epoch_end(self, epoch, logs=None):\n",
    "            '''\n",
    "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "            generated_images = self.model.generator(random_latent_vectors)\n",
    "            #generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "            for i in range(self.num_img):\n",
    "                img = generated_images[i].numpy()\n",
    "                img = keras.preprocessing.image.array_to_img(img)\n",
    "                img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "            '''\n",
    "            \n",
    "            # Sample generator output for num_img images\n",
    "            noise = np.random.normal(0, 1, (self.num_img, self.latent_dim))\n",
    "            gen_imgs = generator.predict(noise)\n",
    "            gen_imgs = gen_imgs.astype('uint8')\n",
    "\n",
    "            #!!!NOT NECESSARY ANYMORE AS IMPLEMENTED AS PART OF THE MODEL!!!\n",
    "            #gen_imgs = 0.5 * (gen_imgs + 1)  #scale back to [0:1]\n",
    "            gen_imgs = gen_imgs.reshape((self.num_img, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "            # save n example images\n",
    "            for i in range(self.num_img):\n",
    "                plt.figure(figsize=(5, 5))\n",
    "                plt.imshow(gen_imgs[i])\n",
    "                #plt.title(f'Example Generator Output')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # adjust path based on whether execution is local or on linux VM\n",
    "                if pathlib.Path(f'{out_img_dir}/{model_name}').exists():\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    #mkdir\n",
    "                    os.mkdir(f'{out_img_dir}/{model_name}')\n",
    "                    #save\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                    \n",
    "            # save corresponding model\n",
    "            now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "            \n",
    "            if pathlib.Path(f'{out_model_dir}/{model_name}').exists():\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5')        \n",
    "            else:\n",
    "                #make dir\n",
    "                os.mkdir(f'{out_model_dir}/{model_name}')\n",
    "                #write\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5') \n",
    "        \n",
    "        \n",
    "    # Instantiate the optimizer for both networks\n",
    "    # (learning_rate=0.0002, beta_1=0.5 are recommended) as per Radford et al. 2016 pp. 3-4\n",
    "    generator_optimizer = Adam(\n",
    "        learning_rate=GEN_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "    critic_optimizer = Adam(\n",
    "        learning_rate=CRIT_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def critic_loss(real_img, fake_img):\n",
    "        real_loss = tf.reduce_mean(real_img)\n",
    "        fake_loss = tf.reduce_mean(fake_img)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(fake_img):\n",
    "        return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "    # Instantiate the custome `GANMonitor` Keras callback.\n",
    "    cbk = GANMonitor(num_img=5, latent_dim=LATENT_DIM)\n",
    "    \n",
    "    # Instantiate the tensorboard tf.keras callback\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "    tb_cbk = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = f'{tboard_dir}/{model_name}_{now}', \n",
    "        write_graph = False,\n",
    "        write_images = True,\n",
    "        histogram_freq = 1) \n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGANGP(\n",
    "        critic=critic,\n",
    "        generator=generator,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        tensorboard_callback=tb_cbk,\n",
    "        critic_extra_steps=CRITIC_FACTOR,\n",
    "        gp_weight=GRADIENT_PENALTY_WEIGHT\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "        d_optimizer=critic_optimizer,\n",
    "        g_optimizer=generator_optimizer,\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=critic_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\"}), <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-280-bb34805db6c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start training the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiles_ds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_cbk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 971\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    972\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\"}), <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Start training the model.\n",
    "wgan.fit([train_ds, tiles_ds], batch_size=BATCH_SIZE, epochs=2000, callbacks=[cbk, tb_cbk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Examples using learned Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 11.5, 11.5, -0.5)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load generator\n",
    "'''\n",
    "generator.compile(optimizer=Adam(lr=0.0008), # per Foster, 2017 RMSprop(lr=0.0008)\n",
    "                          loss=binary_crossentropy,\n",
    "                          metrics=['accuracy'])\n",
    "generator = tf.keras.models.load_model('/data/output/models/dwarfganWGANGPR02/generator-2021-04-04_025322.h5')\n",
    "'''\n",
    "# generate new example of learned representation in latent space\n",
    "try:\n",
    "    generator\n",
    "except NameError:\n",
    "    #get latest generator model save file\n",
    "    folder = pathlib.Path(f'{out_model_dir}/{model_name}')\n",
    "    saves = list(folder.glob('generator*'))\n",
    "    latest = max(saves, key=os.path.getctime)\n",
    "    #load latest generator save file\n",
    "    generator = tf.keras.models.load_model(latest)\n",
    "        \n",
    "noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "res = np.array(generator(noise, training=False)).astype('uint8')\n",
    "\n",
    "#Rescale\n",
    "res = res.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Visualize result\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(res)\n",
    "plt.title(f'Example Generator Output')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
