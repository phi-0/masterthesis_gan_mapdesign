{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DwarfGAN - Deep Learning based Map Design for Dwarf Fortress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "Number of GPUs found: 2\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from keras.layers import Add, Concatenate, Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D, LayerNormalization, Cropping2D\n",
    "from keras.layers.experimental import preprocessing\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.losses import binary_crossentropy, Loss\n",
    "from keras import metrics\n",
    "from functools import partial\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import random\n",
    "\n",
    "#!pip install boto3\n",
    "import boto3 as b3 \n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "NUM_GPUS = len(tf.config.list_physical_devices('GPU'))\n",
    "print(f'Number of GPUs found: {NUM_GPUS}')\n",
    "\n",
    "############### CONFIG ###################\n",
    "\n",
    "# model name\n",
    "model_name = 'dwarfganWGANGPR09Tiles02'\n",
    "# folder path to input files (map images)\n",
    "fpath = r'/data2/input'\n",
    "# folder path to tensorboard output\n",
    "tboard_dir = '/data2/output/tensorboard'\n",
    "# folder path for saved model output\n",
    "out_model_dir = '/data2/output/models'\n",
    "# folder for images to be saved during training\n",
    "out_img_dir = '/data2/output/images'\n",
    "# use skip connections (additive/concatenate)?\n",
    "SKIP_ADD = False\n",
    "SKIP_CONCAT = False\n",
    "# use tileset input?\n",
    "USE_TILESET = False\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 1000 \n",
    "#BATCH_PER_EPOCH = 20\n",
    "# pre-processed (cropped) tiles are 12x12 pixels but will be cropped by the critic to 12x10\n",
    "IMAGE_SIZE = (12,12)\n",
    "BATCH_SIZE = 256\n",
    "CRITIC_FACTOR = 5 # number of times the critic is trained more often than the generator. Recommended = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "RELU_SLOPE_C = 0.2\n",
    "RELU_SLOPE_G = 0.2\n",
    "DROPOUT_C = 0.3\n",
    "MOMENTUM_G = 0.9\n",
    "CRIT_LR = 0.0003 # Adjusted learning rates according to two time-scale update rule (TTUR), see Heusel et al., 2017\n",
    "GEN_LR = 0.0001\n",
    "\n",
    "# NOTE: all extracted map PNGs have been saved on a separate virtual disk mapped to '/data' or '/data2' of the virtual machine in use\n",
    "data_dir = pathlib.Path(fpath + '/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Train / Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map images sourced from the DFMA come in a variety of dimensions. In order to create sample images with constant dimensions, as required by tensors, the 100k input samples were run through a python script to randomly crop 10 1024 x 1024 areas per picture. Of those cropped (sub-)images, only the ones which contain structures were retained. This was achieved by filtering out image crops which only contained two or less different colors. With that, the logic mainly filterd out crops which only contained black. This process resulted in 700'000+ (sub-)image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11441 cropped image samples available\n"
     ]
    }
   ],
   "source": [
    "# use pre-processed (cropped) 12 x 12 images\n",
    "data_dir = pathlib.Path(fpath + '/ascii_crops_tiles/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "print(f'There are {str(len(imgs))} cropped image samples available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random sample input image:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAIAAADZF8uwAAAAKElEQVR4nGNgwA/qCcjTAjBCqH//oHwmJiyKsIkNalAPDUyiHE6UIgBjFAOFRQvi8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=12x12 at 0x7F683403A9B0>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show example sample image (cropped to 128x128)\n",
    "print('A random sample input image:')\n",
    "PIL.Image.open(imgs[random.randint(0,len(imgs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11441 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# creating keras datasets for training and validation - refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "dataset_train = tf.keras.preprocessing.image_dataset_from_directory(  fpath+'/ascii_crops_tiles',\n",
    "                                                                      image_size=IMAGE_SIZE, \n",
    "                                                                      batch_size=BATCH_SIZE, \n",
    "                                                                      #labels=[1.] * len(imgs), # setting all labels to 1.0 (for 'real') as float32\n",
    "                                                                      #label_mode=None, # yields float32 type labels\n",
    "                                                                      seed=875 #,\n",
    "                                                                   )\n",
    "\n",
    "#drop last batch that contains less samples (to match the constant input shape of the tile data)\n",
    "dataset_train = dataset_train.take(len(dataset_train)-1)\n",
    "# refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = dataset_train.cache().prefetch(buffer_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAINING = 12068 # = 20% of total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Random Sample from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check random images from prepared batches\n",
    "plt.figure(figsize=(5, 5))\n",
    "for images, labels in train_ds.take(1): # take one batch. Here batch_size = 128 examples per batch\n",
    "    for i in range(9): # show first 9 images of batch\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(f'img {i}')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATED Model Definition\n",
    "\n",
    "Generally the following changes have been implemented to the architectures:\n",
    "\n",
    "### RUN09\n",
    "\n",
    "- instead of generating full maps, this model focuses on learning to recreate single tiles which represent the different objects/elements in the game world\n",
    "- implemented secondary input for both critic and generator which introduces the single tiles cropped from the tileset files (12x10) as a 12x10x256 vector (256 different 12x10 tiles) \n",
    "- **NEXT RUN**: \n",
    "    - add a Dense layer or 1x1 CONV between Tiles input and concat layer to allow more flexibility for the model to learn which parts are considered\n",
    "    - increase batch size as much as possible\n",
    "\n",
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discriminator_model():\n",
    "\n",
    "    # DISCRIMINATOR\n",
    "    # set input variables to variable width + height. Will be cropped in preprocessing [CURRENTLY FIXED TO 256x256]\n",
    "    input_dim = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "\n",
    "    # Input\n",
    "    d_input = Input(shape=input_dim, name='Discriminator_Input')\n",
    "    \n",
    "    \n",
    "\n",
    "    # ---- REMOVED FOR 256x256 NETWORK ----------\n",
    "    # Keras-based preprocessing. Alternative: RandomCrop()\n",
    "    # use smart_resizing?\n",
    "    #x = tf.keras.preprocessing.image.smart_resize(d_input, (1024, 1024))\n",
    "    #x = preprocessing.Resizing(width=512, \n",
    "    #                           height=512, \n",
    "    #                           name='Preprocessing_Resize'\n",
    "    #                          )(d_input) # Resize to 512 x 512 images\n",
    "\n",
    "    #we crop the images y dimension to 12x10 to match the tile dimensions. To stabilize the output we do NOT use random crop.\n",
    "    x = Cropping2D(((0,0),(0,2)))(d_input) #cropping details ((top, bottom),(left, right))\n",
    "\n",
    "    x = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale'\n",
    "                               )(x) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    # START TILES INPUT\n",
    "    if USE_TILESET:\n",
    "        tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "\n",
    "        tiles = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                    offset=-1,\n",
    "                                    name='Preprocessing_Rescale_Tiles'\n",
    "                                   )(tiles_input) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "\n",
    "        tiles = Conv2D(filters=256, kernel_size=(1,1), strides=1)(tiles)\n",
    "\n",
    "        x = Concatenate()([x, tiles])\n",
    "    # END TILES INPUT\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 0\n",
    "    x = Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = (3,3), \n",
    "            strides = 1,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_0'\n",
    "    )(x)\n",
    "    \n",
    "    # Activation 0 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_0')(x)\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 1\n",
    "    x = Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_1'\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 1\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 1 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_1')(x)\n",
    "\n",
    "    # Dropout 1\n",
    "    x = Dropout(rate = DROPOUT_C)(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Conv2D Layer 2\n",
    "    x = Conv2D(\n",
    "            filters = 256,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            name = 'Discriminator_Conv2D_Layer_3',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 2\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 2 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_3')(x)\n",
    "\n",
    "\n",
    "    # OUTPUT\n",
    "    x = Flatten()(x)\n",
    "    #x = Dropout(DROPOUT_C)(x)\n",
    "    \n",
    "    d_output = Dense(1, \n",
    "                     #activation='sigmoid', \n",
    "                     kernel_initializer = RandomNormal(mean=0, stddev=0.02) # random initialization of weights with normal distribution around 0 with small SD\n",
    "                    )(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Discriminator Model intialization\n",
    "    if USE_TILESET:\n",
    "        discriminator = Model([d_input, tiles_input], d_output, name='Discriminator')\n",
    "    else:\n",
    "        discriminator = Model(d_input, d_output, name='Discriminator')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Discriminator_Input (InputLa [(None, 12, 12, 3)]       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_10 (Cropping2D)   (None, 12, 10, 3)         0         \n",
      "_________________________________________________________________\n",
      "Preprocessing_Rescale (Resca (None, 12, 10, 3)         0         \n",
      "_________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_0 (None, 12, 10, 64)        1792      \n",
      "_________________________________________________________________\n",
      "Activation_0 (LeakyReLU)     (None, 12, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_1 (None, 6, 5, 128)         73856     \n",
      "_________________________________________________________________\n",
      "Activation_1 (LeakyReLU)     (None, 6, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 6, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_3 (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "Activation_3 (LeakyReLU)     (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 2305      \n",
      "=================================================================\n",
      "Total params: 373,121\n",
      "Trainable params: 373,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = discriminator_model()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    # GENERATOR\n",
    "\n",
    "    # set input variable dimensions. Here we will start out with a vector of length 100 for each sample (sampled from a normal distribution, representing the learned latent space)\n",
    "    input_dim = (LATENT_DIM)\n",
    "\n",
    "    # Input\n",
    "    g_input = Input(shape=input_dim, name='Generator_Input')\n",
    "\n",
    "    # Dense Layer 1\n",
    "    x = Dense(np.prod([3,3,512]), kernel_initializer = RandomNormal(mean=0., stddev=0.02), \n",
    "              use_bias=False)(g_input) # use_bias=False see https://keras.io/examples/generative/wgan_gp/\n",
    "\n",
    "    # Batch Norm Layer 1\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    \n",
    "    # Activation Layer 1\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Reshape into 3D tensor\n",
    "    x = Reshape((3,3,512))(x)\n",
    "\n",
    "    # Upsampling Layer 1 + Conv2D Layer1\n",
    "    x = Conv2DTranspose(filters=512, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=256, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=(3,3), padding='same', strides=(1,1), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    # START TILES\n",
    "    if USE_TILESET:\n",
    "        tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "        tiles = preprocessing.Resizing(width=12, height=12)(tiles_input)\n",
    "        tiles = Conv2D(filters=256, kernel_size=(1,1), strides=1)(tiles)\n",
    "\n",
    "        x = Concatenate()([x, tiles])\n",
    "    \n",
    "    # END TILES\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "    \n",
    "    # reduce output dimensions via 1x1 convolution\n",
    "    x = Conv2D(filters=3, kernel_size=(1,1), padding='same', strides=(1,1),\n",
    "               kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "        \n",
    "    # tanh activation layer to scale values to [-1:1]\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    # Batch Norm Layer 7\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "\n",
    "    # Output - Rescale Values back to [0:255] since the discriminator will automatically rescale back down to [-1:1] as part of the pre-processing pipeline\n",
    "    g_output = (255 / 2) * (x + 1) \n",
    "\n",
    "\n",
    "    # Generator Model initialization\n",
    "    if USE_TILESET:\n",
    "        generator = Model([g_input, tiles_input], g_output, name='Generator')\n",
    "    else:\n",
    "        generator = Model(g_input, g_output, name='Generator')\n",
    "    \n",
    "    \n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Generator_Input (InputLayer) [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4608)              589824    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4608)              18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DT (None, 6, 6, 512)         2359296   \n",
      "_________________________________________________________________\n",
      "layer_normalization_9 (Layer (None, 6, 6, 512)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DT (None, 12, 12, 256)       1179648   \n",
      "_________________________________________________________________\n",
      "layer_normalization_10 (Laye (None, 12, 12, 256)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DT (None, 12, 12, 128)       294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 12, 12, 3)         384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 12, 12, 3)         0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_3 (TensorF [(None, 12, 12, 3)]       0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Mul_3 (TensorFlo [(None, 12, 12, 3)]       0         \n",
      "=================================================================\n",
      "Total params: 4,444,544\n",
      "Trainable params: 4,435,072\n",
      "Non-trainable params: 9,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen = generator_model()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tiles\n",
    "\n",
    "We load the 256 split 12x10 tiles into a (12,10,256) numpy array to feed to the network at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiles():\n",
    "    tiles = []\n",
    "\n",
    "    data_dir = pathlib.Path('/data2/input/tiles/800x600/')\n",
    "    imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "    # show example sample image (cropped to 128x128)\n",
    "    PIL.Image.open(imgs[random.randint(0,len(imgs))])\n",
    "\n",
    "    for img in imgs:\n",
    "        tiles.append(np.asarray(PIL.Image.open(img)).astype('uint8'))\n",
    "\n",
    "    #reshape\n",
    "    ds_tiles = np.array(tiles)\n",
    "    ds_tiles = ds_tiles.reshape((1,12,10,256))\n",
    "    ds_tiles = ds_tiles.repeat(repeats=BATCH_SIZE//NUM_GPUS, axis=0)\n",
    "\n",
    "    return ds_tiles\n",
    "\n",
    "\n",
    "    #repeate same 256 12x10 input images as many times as there are batches in the training dataset (N_TRAINING//BATCH_SIZE)\n",
    "    \n",
    "    #print(f'Created numpy array with shape: {ds_tiles.shape}')\n",
    "\n",
    "    #create tf dataset\n",
    "    #tiles_ds = tf.data.Dataset.from_tensor_slices(ds_tiles).batch(BATCH_SIZE)\n",
    "    #train_ds = np.array(train_ds)\n",
    "\n",
    "    #Combine\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices({'Discriminator_Input':train_ds,'Tiles_Input':tiles_ds})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP (Full) Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we compile the models, we need to implement a custom loss function which uses the Wasserstein distance and a gradient penalty term in order to ensure 1 Lipschitz constraints are followed. A WGAN with GP further involves a slightly more complicated training process which trains the critic (discriminator without sigmoid activation function) by feeding three different kinds of images:\n",
    "\n",
    "1. real images (i.e. available samples)\n",
    "2. 'fake' images (i.e. constructed by the generator)\n",
    "3. random interpolations between real and fake images (i.e. random samples interpolated from values between the fake and real images)\n",
    "\n",
    "The full training process of a critic is depicted below (source: Foster, 2019, p. 122):\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"wgan_gp_critic_training.png\"></img>\n",
    "    <i>Computational Graph for one Discriminator Training Epoch. (Source: Foster, 2019, p.122)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below roughly follows the OOP-based framework set by keras see https://keras.io/examples/generative/wgan_gp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope(): \n",
    "\n",
    "    critic = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    if USE_TILESET:\n",
    "        ds_tiles = load_tiles()\n",
    "    else:\n",
    "        ds_tiles = None\n",
    "\n",
    "    class WGANGP(keras.Model):\n",
    "        def __init__(\n",
    "            self,\n",
    "            critic,\n",
    "            generator,\n",
    "            latent_dim,\n",
    "            tensorboard_callback,\n",
    "            critic_extra_steps=5,\n",
    "            gp_weight=10.0,\n",
    "            tiles=None\n",
    "        ):\n",
    "            super(WGANGP, self).__init__()\n",
    "            self.critic = critic\n",
    "            self.generator = generator\n",
    "            self.latent_dim = latent_dim\n",
    "            self.tensorboard_callback = tensorboard_callback\n",
    "            self.d_steps = critic_extra_steps\n",
    "            self.gp_weight = gp_weight\n",
    "            self.tiles=tiles\n",
    "            \n",
    "\n",
    "        def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "            super(WGANGP, self).compile()\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.g_optimizer = g_optimizer\n",
    "            self.d_loss_fn = d_loss_fn\n",
    "            self.g_loss_fn = g_loss_fn\n",
    "\n",
    "        def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "            \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "            This loss is calculated on an interpolated image\n",
    "            and added to the discriminator loss.\n",
    "            \"\"\"\n",
    "            # Get the interpolated image\n",
    "            alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "            diff = fake_images - real_images\n",
    "            interpolated = real_images + alpha * diff\n",
    "\n",
    "            with tf.GradientTape() as gp_tape:\n",
    "                gp_tape.watch(interpolated)\n",
    "                # 1. Get the discriminator output for this interpolated image.\n",
    "                if USE_TILESET:\n",
    "                    pred = self.critic({'Discriminator_Input':interpolated,'Tiles_Input':ds_tiles}, training=True)\n",
    "                else:\n",
    "                    pred = self.critic(interpolated, training=True)\n",
    "\n",
    "            # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "            grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "            # 3. Calculate the norm of the gradients.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "            gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "            return gp\n",
    "\n",
    "        def train_step(self, real_images):\n",
    "            #checking whether we handed a tuple of (numpy) data to .fit().\n",
    "            #if not, the data must be a tf.data.Dataset generator that yields batches of datasets (data, labels)\n",
    "            if isinstance(real_images, tuple):\n",
    "                real_images = real_images[0]\n",
    "\n",
    "            # Get the batch size\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "            # For each batch, we are going to perform the\n",
    "            # following steps as laid out in the original paper:\n",
    "            # 1. Train the generator and get the generator loss\n",
    "            # 2. Train the discriminator and get the discriminator loss\n",
    "            # 3. Calculate the gradient penalty\n",
    "            # 4. Multiply this gradient penalty with a constant weight factor = self.discriminator_extra_steps = 5 (default value)\n",
    "            # 5. Add the gradient penalty to the discriminator loss\n",
    "            # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "            # Train the discriminator first. The original paper recommends training\n",
    "            # the discriminator for `x` more steps (typically 5) as compared to generator\n",
    "            \n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.critic)\n",
    "            \n",
    "            for i in range(self.d_steps):\n",
    "                # Get the latent vector\n",
    "                random_latent_vectors = tf.random.normal(\n",
    "                    shape=(batch_size, self.latent_dim)\n",
    "                )\n",
    "                with tf.GradientTape() as tape:\n",
    "                    if USE_TILESET:\n",
    "                        # Generate fake images from the latent vector\n",
    "                        fake_images = self.generator({'Generator_Input':random_latent_vectors,'Tiles_Input':ds_tiles}, training=True)\n",
    "                        # Get the logits for the fake images\n",
    "                        fake_logits = self.critic({'Discriminator_Input':fake_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "                        # Get the logits for the real images\n",
    "                        real_logits = self.critic({'Discriminator_Input':real_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "                    else:\n",
    "                        # Generate fake images from the latent vector\n",
    "                        fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                        # Get the logits for the fake images\n",
    "                        fake_logits = self.critic(fake_images, training=True)\n",
    "                        # Get the logits for the real images\n",
    "                        real_logits = self.critic(real_images, training=True)\n",
    "\n",
    "                    # Calculate the discriminator loss using the fake and real image logits\n",
    "                    d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                    # Calculate the gradient penalty\n",
    "                    gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                    # Add the gradient penalty to the original discriminator loss\n",
    "                    d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "                # Get the gradients w.r.t the discriminator loss\n",
    "                d_gradient = tape.gradient(d_loss, self.critic.trainable_variables)\n",
    "                # Update the weights of the discriminator using the discriminator optimizer\n",
    "                self.d_optimizer.apply_gradients(\n",
    "                    zip(d_gradient, self.critic.trainable_variables)\n",
    "                )\n",
    "\n",
    "            # Train the generator\n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.generator)\n",
    "            \n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                if USE_TILESET:\n",
    "                    # Generate fake images using the generator\n",
    "                    generated_images = self.generator({'Generator_Input':random_latent_vectors,'Tiles_Input':ds_tiles}, training=True)\n",
    "                    # Get the discriminator logits for fake images\n",
    "                    gen_img_logits = self.critic({'Discriminator_Input':generated_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "                else:\n",
    "                    # Generate fake images using the generator\n",
    "                    generated_images = self.generator(random_latent_vectors, training=True)\n",
    "                    # Get the discriminator logits for fake images\n",
    "                    gen_img_logits = self.critic(generated_images, training=True)\n",
    "                    \n",
    "                # Calculate the generator loss\n",
    "                g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "            # Get the gradients w.r.t the generator loss\n",
    "            gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "            # Update the weights of the generator using the generator optimizer\n",
    "            self.g_optimizer.apply_gradients(\n",
    "                zip(gen_gradient, self.generator.trainable_variables)\n",
    "            )\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "        \n",
    "    class GANMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, num_img=5, latent_dim=128):\n",
    "            self.num_img = num_img\n",
    "            self.latent_dim = latent_dim\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None): #on_epoch_end(self, epoch, logs=None):\n",
    "            '''\n",
    "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "            generated_images = self.model.generator(random_latent_vectors)\n",
    "            #generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "            for i in range(self.num_img):\n",
    "                img = generated_images[i].numpy()\n",
    "                img = keras.preprocessing.image.array_to_img(img)\n",
    "                img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "            '''\n",
    "            \n",
    "            # Sample generator output for num_img images\n",
    "            noise = np.random.normal(0, 1, (self.num_img, self.latent_dim))\n",
    "            if USE_TILESET:\n",
    "                gen_imgs = generator.predict({'Generator_Input':noise,'Tiles_Input':ds_tiles[0:5]})\n",
    "            else:\n",
    "                gen_imgs = generator.predict(noise)\n",
    "                \n",
    "            gen_imgs = gen_imgs.astype('uint8')\n",
    "\n",
    "            #!!!NOT NECESSARY ANYMORE AS IMPLEMENTED AS PART OF THE MODEL!!!\n",
    "            #gen_imgs = 0.5 * (gen_imgs + 1)  #scale back to [0:1]\n",
    "            gen_imgs = gen_imgs.reshape((self.num_img, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "            # save n example images\n",
    "            for i in range(self.num_img):\n",
    "                plt.figure(figsize=(5, 5))\n",
    "                plt.imshow(gen_imgs[i])\n",
    "                #plt.title(f'Example Generator Output')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # adjust path based on whether execution is local or on linux VM\n",
    "                if pathlib.Path(f'{out_img_dir}/{model_name}').exists():\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    #mkdir\n",
    "                    os.mkdir(f'{out_img_dir}/{model_name}')\n",
    "                    #save\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                    \n",
    "            # save corresponding model\n",
    "            now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "            \n",
    "            if pathlib.Path(f'{out_model_dir}/{model_name}').exists():\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5')        \n",
    "            else:\n",
    "                #make dir\n",
    "                os.mkdir(f'{out_model_dir}/{model_name}')\n",
    "                #write\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5') \n",
    "        \n",
    "        \n",
    "    # Instantiate the optimizer for both networks\n",
    "    # (learning_rate=0.0002, beta_1=0.5 are recommended) as per Radford et al. 2016 pp. 3-4\n",
    "    generator_optimizer = Adam(\n",
    "        learning_rate=GEN_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "    critic_optimizer = Adam(\n",
    "        learning_rate=CRIT_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def critic_loss(real_img, fake_img):\n",
    "        real_loss = tf.reduce_mean(real_img)\n",
    "        fake_loss = tf.reduce_mean(fake_img)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(fake_img):\n",
    "        return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "    # Instantiate the custome `GANMonitor` Keras callback.\n",
    "    cbk = GANMonitor(num_img=5, latent_dim=LATENT_DIM)\n",
    "    \n",
    "    # Instantiate the tensorboard tf.keras callback\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "    tb_cbk = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = f'{tboard_dir}/{model_name}_{now}', \n",
    "        write_graph = False, \n",
    "        write_images = True,\n",
    "        histogram_freq = 1) \n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGANGP(\n",
    "        critic=critic,\n",
    "        generator=generator,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        tensorboard_callback=tb_cbk,\n",
    "        critic_extra_steps=CRITIC_FACTOR,\n",
    "        gp_weight=GRADIENT_PENALTY_WEIGHT,\n",
    "        tiles=ds_tiles       \n",
    "        \n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "        d_optimizer=critic_optimizer,\n",
    "        g_optimizer=generator_optimizer,\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=critic_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 2/44 [>.............................] - ETA: 49s - d_loss: 8.8349 - g_loss: -0.1570WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2138s vs `on_train_batch_end` time: 2.1266s). Check your callbacks.\n",
      "44/44 [==============================] - 14s 313ms/step - d_loss: -6150.0800 - g_loss: -12701.0840\n",
      "Epoch 2/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -9079.9300 - g_loss: -22920.4351\n",
      "Epoch 3/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -4979.3657 - g_loss: -12409.3236\n",
      "Epoch 4/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -6458.1926 - g_loss: -27718.5838\n",
      "Epoch 5/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -6233.5418 - g_loss: -29186.8162\n",
      "Epoch 6/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -3420.7920 - g_loss: -8159.8271\n",
      "Epoch 7/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -429.6134 - g_loss: -28014.6531\n",
      "Epoch 8/2000\n",
      "44/44 [==============================] - 10s 235ms/step - d_loss: -2844.1130 - g_loss: -16793.4849\n",
      "Epoch 9/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -2879.6082 - g_loss: -9657.6700\n",
      "Epoch 10/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1036.8891 - g_loss: -11087.3856\n",
      "Epoch 11/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -672.7870 - g_loss: -10026.2164\n",
      "Epoch 12/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -867.0932 - g_loss: -8164.0411\n",
      "Epoch 13/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -788.7467 - g_loss: -7111.4521\n",
      "Epoch 14/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1159.6063 - g_loss: -9213.1629\n",
      "Epoch 15/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1435.7508 - g_loss: -9988.4922\n",
      "Epoch 16/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1287.1270 - g_loss: -8703.3068\n",
      "Epoch 17/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -1374.5785 - g_loss: -9971.3857\n",
      "Epoch 18/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1375.1885 - g_loss: -10020.0602\n",
      "Epoch 19/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1654.7265 - g_loss: -9776.3589\n",
      "Epoch 20/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1599.2639 - g_loss: -9695.8619\n",
      "Epoch 21/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1739.6039 - g_loss: -12689.7631\n",
      "Epoch 22/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1950.3744 - g_loss: -13748.2732\n",
      "Epoch 23/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1795.8049 - g_loss: -13211.0872\n",
      "Epoch 24/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1759.7316 - g_loss: -14907.3825\n",
      "Epoch 25/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -1506.8973 - g_loss: -16821.7712\n",
      "Epoch 26/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1374.0147 - g_loss: -16995.4614\n",
      "Epoch 27/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1241.1753 - g_loss: -17945.9425\n",
      "Epoch 28/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1080.4952 - g_loss: -17146.2900\n",
      "Epoch 29/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1043.7619 - g_loss: -15121.2504\n",
      "Epoch 30/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1108.0518 - g_loss: -15283.6859\n",
      "Epoch 31/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1118.9835 - g_loss: -16453.2559\n",
      "Epoch 32/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1285.3579 - g_loss: -17208.1601\n",
      "Epoch 33/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1186.6688 - g_loss: -17245.7994\n",
      "Epoch 34/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -1231.2995 - g_loss: -16414.1645\n",
      "Epoch 35/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1230.2480 - g_loss: -16960.4596\n",
      "Epoch 36/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1148.3646 - g_loss: -14199.9635\n",
      "Epoch 37/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1130.0547 - g_loss: -15024.6285\n",
      "Epoch 38/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1127.6243 - g_loss: -14599.1785\n",
      "Epoch 39/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1061.6090 - g_loss: -15151.1346\n",
      "Epoch 40/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1120.7596 - g_loss: -13585.6256\n",
      "Epoch 41/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1138.1658 - g_loss: -14094.5130\n",
      "Epoch 42/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1113.2795 - g_loss: -13964.2083\n",
      "Epoch 43/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1232.6025 - g_loss: -13390.4841\n",
      "Epoch 44/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1187.5882 - g_loss: -13561.7631\n",
      "Epoch 45/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1193.6664 - g_loss: -13694.4276\n",
      "Epoch 46/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1177.0358 - g_loss: -14363.0326\n",
      "Epoch 47/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1203.8805 - g_loss: -15368.6873\n",
      "Epoch 48/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -1217.6784 - g_loss: -14869.2611\n",
      "Epoch 49/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1224.2643 - g_loss: -14966.2052\n",
      "Epoch 50/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1184.4982 - g_loss: -15103.7509\n",
      "Epoch 51/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -1146.3829 - g_loss: -14871.8957\n",
      "Epoch 52/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1129.2958 - g_loss: -14860.4072\n",
      "Epoch 53/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1071.8929 - g_loss: -14542.4855\n",
      "Epoch 54/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1152.1563 - g_loss: -14260.5505\n",
      "Epoch 55/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -1297.9328 - g_loss: -15417.3299\n",
      "Epoch 56/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -1129.1066 - g_loss: -15099.0339\n",
      "Epoch 57/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1221.5088 - g_loss: -16731.4442\n",
      "Epoch 58/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -1246.1443 - g_loss: -15908.3323\n",
      "Epoch 59/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -1280.0878 - g_loss: -14719.0497\n",
      "Epoch 60/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -1302.2012 - g_loss: -16092.5371\n",
      "Epoch 61/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1229.4306 - g_loss: -17182.4585\n",
      "Epoch 62/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1263.4755 - g_loss: -16575.4749\n",
      "Epoch 63/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1227.6866 - g_loss: -16348.7877\n",
      "Epoch 64/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -1262.4389 - g_loss: -16712.5912\n",
      "Epoch 65/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1218.9387 - g_loss: -17393.1558\n",
      "Epoch 66/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1324.6747 - g_loss: -16802.2031\n",
      "Epoch 67/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1223.5828 - g_loss: -18264.2160\n",
      "Epoch 68/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1154.4188 - g_loss: -17815.8690\n",
      "Epoch 69/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -1204.9180 - g_loss: -18296.0273\n",
      "Epoch 70/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1196.4476 - g_loss: -17767.1696\n",
      "Epoch 71/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1108.1562 - g_loss: -17911.0696\n",
      "Epoch 72/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1110.1334 - g_loss: -17324.0407\n",
      "Epoch 73/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -1154.2043 - g_loss: -18490.3815\n",
      "Epoch 74/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -1133.7197 - g_loss: -18346.3508\n",
      "Epoch 75/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1172.1655 - g_loss: -18704.7779\n",
      "Epoch 76/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1177.1902 - g_loss: -18829.9546\n",
      "Epoch 77/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1140.6461 - g_loss: -17329.5713\n",
      "Epoch 78/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1205.5212 - g_loss: -17481.2905\n",
      "Epoch 79/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1155.1067 - g_loss: -16327.5280\n",
      "Epoch 80/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -1159.1855 - g_loss: -16097.5657\n",
      "Epoch 81/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -1125.8511 - g_loss: -16309.7530\n",
      "Epoch 82/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1251.4088 - g_loss: -16050.2077\n",
      "Epoch 83/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1147.3836 - g_loss: -15688.2069\n",
      "Epoch 84/2000\n",
      "44/44 [==============================] - 10s 225ms/step - d_loss: -1071.5769 - g_loss: -15826.3551\n",
      "Epoch 85/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1095.6220 - g_loss: -15093.3993\n",
      "Epoch 86/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1099.6417 - g_loss: -15181.1657\n",
      "Epoch 87/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1123.2473 - g_loss: -15472.7159\n",
      "Epoch 88/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1098.2434 - g_loss: -16131.1341\n",
      "Epoch 89/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1207.6692 - g_loss: -15565.6542\n",
      "Epoch 90/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1167.6701 - g_loss: -16157.9340\n",
      "Epoch 91/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1178.8002 - g_loss: -15812.0565\n",
      "Epoch 92/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1168.4342 - g_loss: -15918.8772\n",
      "Epoch 93/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1153.9082 - g_loss: -14779.8887\n",
      "Epoch 94/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1186.0795 - g_loss: -14608.1161\n",
      "Epoch 95/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1117.4488 - g_loss: -15235.6288\n",
      "Epoch 96/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1100.8646 - g_loss: -14631.4984\n",
      "Epoch 97/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1122.9771 - g_loss: -14659.6254\n",
      "Epoch 98/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1070.3201 - g_loss: -14216.4743\n",
      "Epoch 99/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -1154.8630 - g_loss: -14675.0962\n",
      "Epoch 100/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1107.5427 - g_loss: -14563.2127\n",
      "Epoch 101/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1075.5363 - g_loss: -14051.1704\n",
      "Epoch 102/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1124.6330 - g_loss: -14473.4311\n",
      "Epoch 103/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1110.4594 - g_loss: -13941.6243\n",
      "Epoch 104/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1113.4631 - g_loss: -14124.5020\n",
      "Epoch 105/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1062.4096 - g_loss: -13893.9680\n",
      "Epoch 106/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1100.3547 - g_loss: -14315.8422\n",
      "Epoch 107/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1116.2655 - g_loss: -13953.3131\n",
      "Epoch 108/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1125.2825 - g_loss: -13611.2561\n",
      "Epoch 109/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1130.9049 - g_loss: -13689.6509\n",
      "Epoch 110/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1213.5780 - g_loss: -14019.6318\n",
      "Epoch 111/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1074.3842 - g_loss: -14019.6992\n",
      "Epoch 112/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1119.5283 - g_loss: -14128.8945\n",
      "Epoch 113/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1154.6307 - g_loss: -14337.8819\n",
      "Epoch 114/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1083.8361 - g_loss: -14045.4975\n",
      "Epoch 115/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1127.0921 - g_loss: -13849.8258\n",
      "Epoch 116/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1038.5863 - g_loss: -13670.4848\n",
      "Epoch 117/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -1046.2519 - g_loss: -13559.7749\n",
      "Epoch 118/2000\n",
      "44/44 [==============================] - 10s 238ms/step - d_loss: -1097.2360 - g_loss: -13857.6250\n",
      "Epoch 119/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1099.0847 - g_loss: -13799.7469\n",
      "Epoch 120/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1048.3494 - g_loss: -13800.3092\n",
      "Epoch 121/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1083.8604 - g_loss: -13765.2767\n",
      "Epoch 122/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -1061.1538 - g_loss: -13066.1007\n",
      "Epoch 123/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1086.7920 - g_loss: -12960.8160\n",
      "Epoch 124/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1045.5663 - g_loss: -13512.0664\n",
      "Epoch 125/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -950.3910 - g_loss: -13717.6631\n",
      "Epoch 126/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -997.6329 - g_loss: -14420.4120\n",
      "Epoch 127/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1053.8814 - g_loss: -14547.1411\n",
      "Epoch 128/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1073.2114 - g_loss: -14810.1293\n",
      "Epoch 129/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -978.1095 - g_loss: -14311.4764\n",
      "Epoch 130/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1038.1746 - g_loss: -15267.5465\n",
      "Epoch 131/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -983.8092 - g_loss: -15932.1553\n",
      "Epoch 132/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -1015.5029 - g_loss: -16625.3487\n",
      "Epoch 133/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -972.8051 - g_loss: -16649.6509\n",
      "Epoch 134/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -963.7302 - g_loss: -16337.2018\n",
      "Epoch 135/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -1008.2951 - g_loss: -16512.5156\n",
      "Epoch 136/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1022.6521 - g_loss: -16156.4911\n",
      "Epoch 137/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -957.3344 - g_loss: -16413.1703\n",
      "Epoch 138/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -912.4675 - g_loss: -16113.2138\n",
      "Epoch 139/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1018.4676 - g_loss: -16475.5794\n",
      "Epoch 140/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1000.4740 - g_loss: -15621.4114\n",
      "Epoch 141/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -1017.7434 - g_loss: -15970.6896\n",
      "Epoch 142/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -970.5142 - g_loss: -15369.1024\n",
      "Epoch 143/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1001.6870 - g_loss: -15091.6705\n",
      "Epoch 144/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -936.1250 - g_loss: -14670.6840\n",
      "Epoch 145/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -1019.1174 - g_loss: -14956.9551\n",
      "Epoch 146/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -931.4250 - g_loss: -15188.1664\n",
      "Epoch 147/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -1012.1294 - g_loss: -14668.4194\n",
      "Epoch 148/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -947.3180 - g_loss: -14346.0263\n",
      "Epoch 149/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -936.8469 - g_loss: -14595.8162\n",
      "Epoch 150/2000\n",
      "44/44 [==============================] - 10s 235ms/step - d_loss: -919.7036 - g_loss: -14037.4180\n",
      "Epoch 151/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -959.3033 - g_loss: -13911.2095\n",
      "Epoch 152/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -958.8043 - g_loss: -13778.0701\n",
      "Epoch 153/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -990.4145 - g_loss: -14037.4282\n",
      "Epoch 154/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -1031.2720 - g_loss: -13360.1681\n",
      "Epoch 155/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -962.0027 - g_loss: -13334.7873\n",
      "Epoch 156/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -933.6540 - g_loss: -13093.2126\n",
      "Epoch 157/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -957.3223 - g_loss: -13161.4504\n",
      "Epoch 158/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -893.4746 - g_loss: -13124.8269\n",
      "Epoch 159/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1015.2101 - g_loss: -14031.3003\n",
      "Epoch 160/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -963.5271 - g_loss: -13272.4641\n",
      "Epoch 161/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -983.2148 - g_loss: -13311.7125\n",
      "Epoch 162/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -934.4565 - g_loss: -13732.8318\n",
      "Epoch 163/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -967.6287 - g_loss: -13470.4895\n",
      "Epoch 164/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -1018.0066 - g_loss: -13704.9172\n",
      "Epoch 165/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -847.6913 - g_loss: -12742.9605\n",
      "Epoch 166/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -940.6074 - g_loss: -13275.6702\n",
      "Epoch 167/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -983.8264 - g_loss: -12777.4303\n",
      "Epoch 168/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -988.1492 - g_loss: -12996.1850\n",
      "Epoch 169/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -945.6603 - g_loss: -13057.0160\n",
      "Epoch 170/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -911.5074 - g_loss: -12888.1049\n",
      "Epoch 171/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -951.1715 - g_loss: -12660.2554\n",
      "Epoch 172/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -916.7208 - g_loss: -12741.6287\n",
      "Epoch 173/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -959.5658 - g_loss: -12288.3613\n",
      "Epoch 174/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -932.2441 - g_loss: -12075.1386\n",
      "Epoch 175/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -1025.7390 - g_loss: -11725.0674\n",
      "Epoch 176/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -955.1599 - g_loss: -11924.1874\n",
      "Epoch 177/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -880.7703 - g_loss: -11561.0419\n",
      "Epoch 178/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -935.9946 - g_loss: -11777.0391\n",
      "Epoch 179/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -969.2471 - g_loss: -11246.8219\n",
      "Epoch 180/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -940.6425 - g_loss: -11211.3833\n",
      "Epoch 181/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -925.1312 - g_loss: -11685.7215\n",
      "Epoch 182/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -903.6370 - g_loss: -10598.2547\n",
      "Epoch 183/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -988.2968 - g_loss: -11090.0039\n",
      "Epoch 184/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -926.7685 - g_loss: -11057.7610\n",
      "Epoch 185/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -973.8059 - g_loss: -10821.8163\n",
      "Epoch 186/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -944.9509 - g_loss: -11152.8258\n",
      "Epoch 187/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -935.8299 - g_loss: -10757.8904\n",
      "Epoch 188/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -942.1122 - g_loss: -11353.4417\n",
      "Epoch 189/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -856.6799 - g_loss: -10490.5769\n",
      "Epoch 190/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -897.4446 - g_loss: -10230.7175\n",
      "Epoch 191/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -913.9980 - g_loss: -10618.9254\n",
      "Epoch 192/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -897.1899 - g_loss: -10302.1527\n",
      "Epoch 193/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -946.5849 - g_loss: -10215.1249\n",
      "Epoch 194/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -909.6902 - g_loss: -10292.6832\n",
      "Epoch 195/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -859.1300 - g_loss: -10205.0797\n",
      "Epoch 196/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -937.0870 - g_loss: -10139.0974\n",
      "Epoch 197/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -938.8758 - g_loss: -10330.0083\n",
      "Epoch 198/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -911.3246 - g_loss: -9359.8247\n",
      "Epoch 199/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -977.9371 - g_loss: -9629.6726\n",
      "Epoch 200/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -905.3818 - g_loss: -9695.0794\n",
      "Epoch 201/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -922.6287 - g_loss: -9664.7738\n",
      "Epoch 202/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -880.3809 - g_loss: -9353.4815\n",
      "Epoch 203/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -886.4620 - g_loss: -9679.0818\n",
      "Epoch 204/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -907.9656 - g_loss: -9900.4786\n",
      "Epoch 205/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -901.0269 - g_loss: -9291.9197\n",
      "Epoch 206/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -858.8747 - g_loss: -9528.9111\n",
      "Epoch 207/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -869.9096 - g_loss: -8942.4713\n",
      "Epoch 208/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -864.7988 - g_loss: -8393.0652\n",
      "Epoch 209/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -932.2317 - g_loss: -9019.9504\n",
      "Epoch 210/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -903.7788 - g_loss: -9277.7638\n",
      "Epoch 211/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -916.4458 - g_loss: -9414.5511\n",
      "Epoch 212/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -947.9872 - g_loss: -8931.3217\n",
      "Epoch 213/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -857.3260 - g_loss: -9164.3314\n",
      "Epoch 214/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -845.1488 - g_loss: -9559.3212\n",
      "Epoch 215/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -880.3890 - g_loss: -9454.1550\n",
      "Epoch 216/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -900.0690 - g_loss: -9414.8333\n",
      "Epoch 217/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -920.0597 - g_loss: -9731.3405\n",
      "Epoch 218/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -909.9615 - g_loss: -9705.9621\n",
      "Epoch 219/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -925.1706 - g_loss: -9920.5665\n",
      "Epoch 220/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -873.1355 - g_loss: -10102.8377\n",
      "Epoch 221/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -893.0320 - g_loss: -9672.8076\n",
      "Epoch 222/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -834.2175 - g_loss: -9663.4932\n",
      "Epoch 223/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -921.3671 - g_loss: -9517.8625\n",
      "Epoch 224/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -901.2909 - g_loss: -9379.3890\n",
      "Epoch 225/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -916.3742 - g_loss: -9402.0235\n",
      "Epoch 226/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -893.0008 - g_loss: -9638.7107\n",
      "Epoch 227/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -882.6483 - g_loss: -9364.7815\n",
      "Epoch 228/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -872.7042 - g_loss: -9168.6867\n",
      "Epoch 229/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -901.1697 - g_loss: -9441.8571\n",
      "Epoch 230/2000\n",
      "44/44 [==============================] - 10s 236ms/step - d_loss: -880.1211 - g_loss: -9241.5344\n",
      "Epoch 231/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -912.9513 - g_loss: -9461.5922\n",
      "Epoch 232/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -861.6381 - g_loss: -9088.6432\n",
      "Epoch 233/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -852.0804 - g_loss: -9522.4056\n",
      "Epoch 234/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -839.9688 - g_loss: -9299.6336\n",
      "Epoch 235/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -920.4553 - g_loss: -8880.9864\n",
      "Epoch 236/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -868.1706 - g_loss: -9328.1184\n",
      "Epoch 237/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -844.7557 - g_loss: -9562.7525\n",
      "Epoch 238/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -838.9597 - g_loss: -9480.7125\n",
      "Epoch 239/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -871.2743 - g_loss: -9811.1074\n",
      "Epoch 240/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -844.0692 - g_loss: -9877.7505\n",
      "Epoch 241/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -902.6886 - g_loss: -9896.6078\n",
      "Epoch 242/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -900.2241 - g_loss: -9354.5700\n",
      "Epoch 243/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -872.7630 - g_loss: -9723.2001\n",
      "Epoch 244/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -912.5725 - g_loss: -9172.4322\n",
      "Epoch 245/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -926.3568 - g_loss: -9139.5156\n",
      "Epoch 246/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -763.4281 - g_loss: -8972.1667\n",
      "Epoch 247/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -864.5186 - g_loss: -8929.7145\n",
      "Epoch 248/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -857.5928 - g_loss: -8975.9265\n",
      "Epoch 249/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -822.3263 - g_loss: -9195.5153\n",
      "Epoch 250/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -912.7876 - g_loss: -8864.9502\n",
      "Epoch 251/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -864.5473 - g_loss: -9234.4073\n",
      "Epoch 252/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -880.2227 - g_loss: -8716.0553\n",
      "Epoch 253/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -860.5104 - g_loss: -8878.7276\n",
      "Epoch 254/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -877.5201 - g_loss: -9082.2114\n",
      "Epoch 255/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -883.6439 - g_loss: -9072.9943\n",
      "Epoch 256/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -861.9947 - g_loss: -8812.5210\n",
      "Epoch 257/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -817.7444 - g_loss: -8808.5333\n",
      "Epoch 258/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -883.3152 - g_loss: -8584.2181\n",
      "Epoch 259/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -947.5022 - g_loss: -8794.0203\n",
      "Epoch 260/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -907.6182 - g_loss: -8349.5340\n",
      "Epoch 261/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -860.5420 - g_loss: -8772.7034\n",
      "Epoch 262/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -863.7632 - g_loss: -8440.4901\n",
      "Epoch 263/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -829.8455 - g_loss: -8416.5285\n",
      "Epoch 264/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -903.3911 - g_loss: -8678.4722\n",
      "Epoch 265/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -893.4355 - g_loss: -8666.2126\n",
      "Epoch 266/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -809.7504 - g_loss: -8437.7107\n",
      "Epoch 267/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -829.1345 - g_loss: -8270.3197\n",
      "Epoch 268/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -871.6474 - g_loss: -7931.9449\n",
      "Epoch 269/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -843.6145 - g_loss: -7793.3852\n",
      "Epoch 270/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -857.8424 - g_loss: -7965.9715\n",
      "Epoch 271/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -825.7850 - g_loss: -7512.6986\n",
      "Epoch 272/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -826.9328 - g_loss: -7330.3888\n",
      "Epoch 273/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -847.3877 - g_loss: -7581.6194\n",
      "Epoch 274/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -868.9135 - g_loss: -7457.3301\n",
      "Epoch 275/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -859.4390 - g_loss: -7579.1081\n",
      "Epoch 276/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -816.8839 - g_loss: -7237.7172\n",
      "Epoch 277/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -892.5381 - g_loss: -6867.7221\n",
      "Epoch 278/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -818.1416 - g_loss: -6803.4119\n",
      "Epoch 279/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -880.7490 - g_loss: -6922.5233\n",
      "Epoch 280/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -787.6728 - g_loss: -6944.4271\n",
      "Epoch 281/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -838.3871 - g_loss: -7172.4751\n",
      "Epoch 282/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -841.6560 - g_loss: -7490.1042\n",
      "Epoch 283/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -836.6213 - g_loss: -7346.4764\n",
      "Epoch 284/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -889.8426 - g_loss: -6923.6395\n",
      "Epoch 285/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -848.0187 - g_loss: -6956.4912\n",
      "Epoch 286/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -828.5656 - g_loss: -6744.6344\n",
      "Epoch 287/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -881.5223 - g_loss: -6672.8782\n",
      "Epoch 288/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -906.2395 - g_loss: -6706.9514\n",
      "Epoch 289/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -828.4443 - g_loss: -6656.8518\n",
      "Epoch 290/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -916.6822 - g_loss: -6588.1908\n",
      "Epoch 291/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -871.7321 - g_loss: -6625.6120\n",
      "Epoch 292/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -871.0895 - g_loss: -6135.9978\n",
      "Epoch 293/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -829.5979 - g_loss: -6594.3647\n",
      "Epoch 294/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -894.0313 - g_loss: -6437.8494\n",
      "Epoch 295/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -860.2947 - g_loss: -6292.0624\n",
      "Epoch 296/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -863.3831 - g_loss: -6330.0630\n",
      "Epoch 297/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -910.3811 - g_loss: -5960.0660\n",
      "Epoch 298/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -913.6147 - g_loss: -6432.7401\n",
      "Epoch 299/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -883.2458 - g_loss: -6378.8113\n",
      "Epoch 300/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -873.8655 - g_loss: -6240.6806\n",
      "Epoch 301/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -852.4181 - g_loss: -6419.3205\n",
      "Epoch 302/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -869.7374 - g_loss: -6346.5599\n",
      "Epoch 303/2000\n",
      "44/44 [==============================] - 10s 235ms/step - d_loss: -844.9845 - g_loss: -6842.8604\n",
      "Epoch 304/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -867.6793 - g_loss: -6772.3379\n",
      "Epoch 305/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -800.2361 - g_loss: -7126.6785\n",
      "Epoch 306/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -908.8549 - g_loss: -6939.4304\n",
      "Epoch 307/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -856.8121 - g_loss: -6316.8163\n",
      "Epoch 308/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -878.6844 - g_loss: -5972.3681\n",
      "Epoch 309/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -820.4533 - g_loss: -5930.6909\n",
      "Epoch 310/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -834.8475 - g_loss: -6049.7155\n",
      "Epoch 311/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -795.5509 - g_loss: -6156.8226\n",
      "Epoch 312/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -879.2968 - g_loss: -5846.1306\n",
      "Epoch 313/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -873.3650 - g_loss: -5918.2029\n",
      "Epoch 314/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -863.6845 - g_loss: -6368.7196\n",
      "Epoch 315/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -864.8789 - g_loss: -6257.1806\n",
      "Epoch 316/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -783.4508 - g_loss: -6645.0431\n",
      "Epoch 317/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -841.8384 - g_loss: -6180.5992\n",
      "Epoch 318/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -803.7056 - g_loss: -6077.5979\n",
      "Epoch 319/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -825.0815 - g_loss: -6195.8894\n",
      "Epoch 320/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -851.3517 - g_loss: -5948.7582\n",
      "Epoch 321/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -811.0204 - g_loss: -6334.2722\n",
      "Epoch 322/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -852.8130 - g_loss: -6510.5094\n",
      "Epoch 323/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -807.8384 - g_loss: -6056.6084\n",
      "Epoch 324/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -843.8874 - g_loss: -6460.2944\n",
      "Epoch 325/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -837.1755 - g_loss: -6779.6367\n",
      "Epoch 326/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -857.9500 - g_loss: -6768.3400\n",
      "Epoch 327/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -841.2156 - g_loss: -7005.2761\n",
      "Epoch 328/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -875.9437 - g_loss: -7241.2766\n",
      "Epoch 329/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -819.3862 - g_loss: -6566.1494\n",
      "Epoch 330/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -911.7297 - g_loss: -6457.9839\n",
      "Epoch 331/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -856.1233 - g_loss: -7204.1745\n",
      "Epoch 332/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -844.7507 - g_loss: -7264.2192\n",
      "Epoch 333/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -787.6252 - g_loss: -7624.2724\n",
      "Epoch 334/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -841.3494 - g_loss: -7681.2134\n",
      "Epoch 335/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -860.8978 - g_loss: -7114.0903\n",
      "Epoch 336/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -840.5867 - g_loss: -7691.8272\n",
      "Epoch 337/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -852.4561 - g_loss: -7097.3573\n",
      "Epoch 338/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -850.0276 - g_loss: -7042.2917\n",
      "Epoch 339/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -838.6330 - g_loss: -7219.8959\n",
      "Epoch 340/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -773.7836 - g_loss: -6617.6186\n",
      "Epoch 341/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -825.3566 - g_loss: -6618.3438\n",
      "Epoch 342/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -812.8306 - g_loss: -7031.3082\n",
      "Epoch 343/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -843.3079 - g_loss: -6859.6769\n",
      "Epoch 344/2000\n",
      "44/44 [==============================] - 10s 236ms/step - d_loss: -779.0096 - g_loss: -6764.4354\n",
      "Epoch 345/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -805.0336 - g_loss: -6323.7990\n",
      "Epoch 346/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -834.1706 - g_loss: -6380.8678\n",
      "Epoch 347/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -844.0000 - g_loss: -6389.3003\n",
      "Epoch 348/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -821.1619 - g_loss: -6261.7811\n",
      "Epoch 349/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -813.2516 - g_loss: -6420.3526\n",
      "Epoch 350/2000\n",
      "44/44 [==============================] - 10s 235ms/step - d_loss: -832.0437 - g_loss: -6118.6092\n",
      "Epoch 351/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -807.2495 - g_loss: -6180.1608\n",
      "Epoch 352/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -840.9112 - g_loss: -6480.3833\n",
      "Epoch 353/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -868.4024 - g_loss: -6919.4963\n",
      "Epoch 354/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -873.7376 - g_loss: -6799.2623\n",
      "Epoch 355/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -835.4056 - g_loss: -6293.4493\n",
      "Epoch 356/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -857.3086 - g_loss: -5862.2205\n",
      "Epoch 357/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -853.7214 - g_loss: -6494.5781\n",
      "Epoch 358/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -782.3847 - g_loss: -6466.8073\n",
      "Epoch 359/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -814.6264 - g_loss: -6873.4588\n",
      "Epoch 360/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -855.4802 - g_loss: -6655.3763\n",
      "Epoch 361/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -849.1755 - g_loss: -7248.8304\n",
      "Epoch 362/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -799.8397 - g_loss: -7348.2672\n",
      "Epoch 363/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -821.5754 - g_loss: -7344.5647\n",
      "Epoch 364/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -838.6907 - g_loss: -7418.8282\n",
      "Epoch 365/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -831.9110 - g_loss: -7200.6806\n",
      "Epoch 366/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -887.6563 - g_loss: -7608.5370\n",
      "Epoch 367/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -849.8186 - g_loss: -6936.7253\n",
      "Epoch 368/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -817.1422 - g_loss: -6764.5911\n",
      "Epoch 369/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -834.3505 - g_loss: -6668.5329\n",
      "Epoch 370/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -802.3065 - g_loss: -7124.7820\n",
      "Epoch 371/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -890.4224 - g_loss: -7123.6487\n",
      "Epoch 372/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -816.3902 - g_loss: -7401.9443\n",
      "Epoch 373/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -831.3632 - g_loss: -7067.0122\n",
      "Epoch 374/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -873.7004 - g_loss: -6563.7940\n",
      "Epoch 375/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -866.8132 - g_loss: -6953.1518\n",
      "Epoch 376/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -799.7671 - g_loss: -6671.0308\n",
      "Epoch 377/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -804.6989 - g_loss: -6127.4643\n",
      "Epoch 378/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -816.9837 - g_loss: -6240.4519\n",
      "Epoch 379/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -841.3905 - g_loss: -6373.5090\n",
      "Epoch 380/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -817.2838 - g_loss: -6440.9802\n",
      "Epoch 381/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -829.6140 - g_loss: -6044.0389\n",
      "Epoch 382/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -875.6823 - g_loss: -6131.5675\n",
      "Epoch 383/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -819.5249 - g_loss: -6464.6168\n",
      "Epoch 384/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -788.6692 - g_loss: -5984.9375\n",
      "Epoch 385/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -859.7602 - g_loss: -5687.0537\n",
      "Epoch 386/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -827.6333 - g_loss: -5841.5652\n",
      "Epoch 387/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -878.6948 - g_loss: -6011.3265\n",
      "Epoch 388/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -864.8016 - g_loss: -6030.1047\n",
      "Epoch 389/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -840.3674 - g_loss: -6069.3775\n",
      "Epoch 390/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -859.8153 - g_loss: -5787.6534\n",
      "Epoch 391/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -866.4632 - g_loss: -6077.0869\n",
      "Epoch 392/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -916.9127 - g_loss: -5867.1610\n",
      "Epoch 393/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -857.4398 - g_loss: -5840.5548\n",
      "Epoch 394/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -865.5283 - g_loss: -5520.8120\n",
      "Epoch 395/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -842.9605 - g_loss: -5760.8922\n",
      "Epoch 396/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -821.5536 - g_loss: -5932.1645\n",
      "Epoch 397/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -800.5157 - g_loss: -5537.5587\n",
      "Epoch 398/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -859.5044 - g_loss: -5678.3926\n",
      "Epoch 399/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -854.0851 - g_loss: -5412.8247\n",
      "Epoch 400/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -885.5677 - g_loss: -5243.6587\n",
      "Epoch 401/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -813.3476 - g_loss: -5420.9993\n",
      "Epoch 402/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -872.4580 - g_loss: -5249.2875\n",
      "Epoch 403/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -801.8071 - g_loss: -5610.5064\n",
      "Epoch 404/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -835.5626 - g_loss: -5673.1515\n",
      "Epoch 405/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -846.4713 - g_loss: -5537.5000\n",
      "Epoch 406/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -799.4415 - g_loss: -5487.0483\n",
      "Epoch 407/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -824.5962 - g_loss: -5113.6395\n",
      "Epoch 408/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -832.7070 - g_loss: -5022.9025\n",
      "Epoch 409/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -846.3778 - g_loss: -5124.7766\n",
      "Epoch 410/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -767.2688 - g_loss: -4512.6499\n",
      "Epoch 411/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -879.7773 - g_loss: -4114.8421\n",
      "Epoch 412/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -792.2280 - g_loss: -4663.2223\n",
      "Epoch 413/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -846.4881 - g_loss: -5023.2293\n",
      "Epoch 414/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -837.4007 - g_loss: -5151.8303\n",
      "Epoch 415/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -806.0868 - g_loss: -5339.9307\n",
      "Epoch 416/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -806.1092 - g_loss: -5112.1477\n",
      "Epoch 417/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -804.7469 - g_loss: -5048.9767\n",
      "Epoch 418/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -844.8168 - g_loss: -5186.3942\n",
      "Epoch 419/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -837.2556 - g_loss: -5080.8405\n",
      "Epoch 420/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -893.7127 - g_loss: -5562.5334\n",
      "Epoch 421/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -838.6568 - g_loss: -5841.4781\n",
      "Epoch 422/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -861.1174 - g_loss: -5274.8390\n",
      "Epoch 423/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -851.6160 - g_loss: -5107.3883\n",
      "Epoch 424/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -844.1621 - g_loss: -4719.5656\n",
      "Epoch 425/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -881.8797 - g_loss: -5070.0687\n",
      "Epoch 426/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -876.1002 - g_loss: -4653.8669\n",
      "Epoch 427/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -815.9074 - g_loss: -3905.0659\n",
      "Epoch 428/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -870.5806 - g_loss: -3896.3533\n",
      "Epoch 429/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -840.5796 - g_loss: -4154.9115\n",
      "Epoch 430/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -854.3767 - g_loss: -4177.2997\n",
      "Epoch 431/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -842.9088 - g_loss: -4275.6161\n",
      "Epoch 432/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -843.7079 - g_loss: -4311.1924\n",
      "Epoch 433/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -866.0029 - g_loss: -4342.6254\n",
      "Epoch 434/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -803.2765 - g_loss: -3976.3986\n",
      "Epoch 435/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -824.8141 - g_loss: -3691.4795\n",
      "Epoch 436/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -833.5630 - g_loss: -4034.6646\n",
      "Epoch 437/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -842.1661 - g_loss: -3669.3873\n",
      "Epoch 438/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -829.7617 - g_loss: -3641.1730\n",
      "Epoch 439/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -800.3567 - g_loss: -3561.4772\n",
      "Epoch 440/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -817.5804 - g_loss: -3409.6250\n",
      "Epoch 441/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -876.3978 - g_loss: -4004.6922\n",
      "Epoch 442/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -815.9986 - g_loss: -3891.0385\n",
      "Epoch 443/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -869.8583 - g_loss: -3239.9565\n",
      "Epoch 444/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -802.7217 - g_loss: -3320.5655\n",
      "Epoch 445/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -830.9709 - g_loss: -3461.7971\n",
      "Epoch 446/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -848.6678 - g_loss: -3540.4777\n",
      "Epoch 447/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -833.0720 - g_loss: -3743.7689\n",
      "Epoch 448/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -851.6446 - g_loss: -4081.3382\n",
      "Epoch 449/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -896.4390 - g_loss: -4238.5623\n",
      "Epoch 450/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -871.4281 - g_loss: -4045.7410\n",
      "Epoch 451/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -849.0591 - g_loss: -3926.6174\n",
      "Epoch 452/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -879.4111 - g_loss: -4009.9054\n",
      "Epoch 453/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -804.5092 - g_loss: -3928.7427\n",
      "Epoch 454/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -862.5911 - g_loss: -4176.2372\n",
      "Epoch 455/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -830.7875 - g_loss: -3861.4125\n",
      "Epoch 456/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -848.1546 - g_loss: -3585.6711\n",
      "Epoch 457/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -879.4813 - g_loss: -3863.0491\n",
      "Epoch 458/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -794.9758 - g_loss: -3806.1993\n",
      "Epoch 459/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -852.7487 - g_loss: -3734.5118\n",
      "Epoch 460/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -824.3840 - g_loss: -3970.8429\n",
      "Epoch 461/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -872.4478 - g_loss: -3892.9803\n",
      "Epoch 462/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -835.5699 - g_loss: -3745.2081\n",
      "Epoch 463/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -805.7806 - g_loss: -3830.4545\n",
      "Epoch 464/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -876.1969 - g_loss: -4193.0269\n",
      "Epoch 465/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -822.2124 - g_loss: -4041.9003\n",
      "Epoch 466/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -825.0868 - g_loss: -4576.3655\n",
      "Epoch 467/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -853.2945 - g_loss: -3679.4882\n",
      "Epoch 468/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -848.6822 - g_loss: -3982.2254\n",
      "Epoch 469/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -843.4125 - g_loss: -4046.3621\n",
      "Epoch 470/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -855.3955 - g_loss: -4006.1225\n",
      "Epoch 471/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -807.7516 - g_loss: -3591.1847\n",
      "Epoch 472/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -873.2671 - g_loss: -3801.2653\n",
      "Epoch 473/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -860.5508 - g_loss: -3494.6190\n",
      "Epoch 474/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -879.5102 - g_loss: -3417.5956\n",
      "Epoch 475/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -885.2588 - g_loss: -3718.6365\n",
      "Epoch 476/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -818.2631 - g_loss: -3572.9758\n",
      "Epoch 477/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -879.6784 - g_loss: -3369.2890\n",
      "Epoch 478/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -853.1194 - g_loss: -3284.6048\n",
      "Epoch 479/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -932.1108 - g_loss: -3540.6113\n",
      "Epoch 480/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -809.1647 - g_loss: -3207.2439\n",
      "Epoch 481/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -869.0393 - g_loss: -3361.1744\n",
      "Epoch 482/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -885.8808 - g_loss: -3268.4952\n",
      "Epoch 483/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -850.6182 - g_loss: -3124.7562\n",
      "Epoch 484/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -868.3965 - g_loss: -3321.4307\n",
      "Epoch 485/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -821.4656 - g_loss: -2786.8128\n",
      "Epoch 486/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -884.6145 - g_loss: -3008.2041\n",
      "Epoch 487/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -884.4253 - g_loss: -3066.7395\n",
      "Epoch 488/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -875.0557 - g_loss: -2896.6199\n",
      "Epoch 489/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -837.9903 - g_loss: -2811.0068\n",
      "Epoch 490/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -835.9381 - g_loss: -3096.0732\n",
      "Epoch 491/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -876.5567 - g_loss: -2846.2448\n",
      "Epoch 492/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -817.1676 - g_loss: -2694.6430\n",
      "Epoch 493/2000\n",
      "44/44 [==============================] - 10s 235ms/step - d_loss: -844.1945 - g_loss: -2790.0626\n",
      "Epoch 494/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -843.1792 - g_loss: -2793.0865\n",
      "Epoch 495/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -844.1711 - g_loss: -2953.7749\n",
      "Epoch 496/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -870.3856 - g_loss: -3113.9349\n",
      "Epoch 497/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -832.0586 - g_loss: -2884.4051\n",
      "Epoch 498/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -840.0565 - g_loss: -2580.8618\n",
      "Epoch 499/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -849.0709 - g_loss: -2091.8162\n",
      "Epoch 500/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -819.5743 - g_loss: -2276.2315\n",
      "Epoch 501/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -891.0153 - g_loss: -1847.5284\n",
      "Epoch 502/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -862.5397 - g_loss: -1674.7361\n",
      "Epoch 503/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -883.5016 - g_loss: -1650.1852\n",
      "Epoch 504/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -829.2106 - g_loss: -1409.1591\n",
      "Epoch 505/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -851.1572 - g_loss: -1193.7864\n",
      "Epoch 506/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -892.4984 - g_loss: -1028.4731\n",
      "Epoch 507/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -827.5427 - g_loss: -1275.6794\n",
      "Epoch 508/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -825.5994 - g_loss: -1299.2632\n",
      "Epoch 509/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -817.9514 - g_loss: -1605.6302\n",
      "Epoch 510/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -839.1538 - g_loss: -965.3775\n",
      "Epoch 511/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -845.2354 - g_loss: -837.4865\n",
      "Epoch 512/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -892.8315 - g_loss: -927.0916\n",
      "Epoch 513/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -821.7955 - g_loss: -1248.8412\n",
      "Epoch 514/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -842.1499 - g_loss: -1679.1783\n",
      "Epoch 515/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -866.8462 - g_loss: -1598.3033\n",
      "Epoch 516/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -828.0421 - g_loss: -1473.0295\n",
      "Epoch 517/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -822.8519 - g_loss: -1064.4652\n",
      "Epoch 518/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -831.7924 - g_loss: -713.7453\n",
      "Epoch 519/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -884.1239 - g_loss: -1076.0743\n",
      "Epoch 520/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -860.9150 - g_loss: -1635.6122\n",
      "Epoch 521/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -884.7427 - g_loss: -1490.1432\n",
      "Epoch 522/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -831.0174 - g_loss: -1514.5319\n",
      "Epoch 523/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -862.6097 - g_loss: -1280.6481\n",
      "Epoch 524/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -841.5239 - g_loss: -1314.5905\n",
      "Epoch 525/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -793.5451 - g_loss: -1307.3896\n",
      "Epoch 526/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -832.4476 - g_loss: -1642.9174\n",
      "Epoch 527/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -851.8095 - g_loss: -1181.1546\n",
      "Epoch 528/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -865.5959 - g_loss: -1624.8651\n",
      "Epoch 529/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -869.1277 - g_loss: -1825.4185\n",
      "Epoch 530/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -799.7167 - g_loss: -1768.9564\n",
      "Epoch 531/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -864.1019 - g_loss: -1195.8260\n",
      "Epoch 532/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -837.0621 - g_loss: -1277.2891\n",
      "Epoch 533/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -815.7708 - g_loss: -1360.2477\n",
      "Epoch 534/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -887.1523 - g_loss: -544.9081\n",
      "Epoch 535/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -841.1134 - g_loss: -605.7111\n",
      "Epoch 536/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -858.2995 - g_loss: -680.3199\n",
      "Epoch 537/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -844.9772 - g_loss: -1182.7954\n",
      "Epoch 538/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -896.2276 - g_loss: -1053.2160\n",
      "Epoch 539/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -889.0960 - g_loss: -670.7906\n",
      "Epoch 540/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -852.0950 - g_loss: -1206.9643\n",
      "Epoch 541/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -888.5458 - g_loss: -1017.5891\n",
      "Epoch 542/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -866.7613 - g_loss: -689.9264\n",
      "Epoch 543/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -878.6598 - g_loss: -528.7904\n",
      "Epoch 544/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -854.1369 - g_loss: -757.5584\n",
      "Epoch 545/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -797.9606 - g_loss: -670.6196\n",
      "Epoch 546/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -841.0684 - g_loss: -650.7222\n",
      "Epoch 547/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -893.4665 - g_loss: -789.3377\n",
      "Epoch 548/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -806.4107 - g_loss: -728.2199\n",
      "Epoch 549/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -829.1218 - g_loss: -985.4543\n",
      "Epoch 550/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -828.8483 - g_loss: -1327.6826\n",
      "Epoch 551/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -841.9910 - g_loss: -1000.8772\n",
      "Epoch 552/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -849.7655 - g_loss: -971.0175\n",
      "Epoch 553/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -840.6155 - g_loss: -1148.2699\n",
      "Epoch 554/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -829.6793 - g_loss: -1484.6761\n",
      "Epoch 555/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -835.3035 - g_loss: -1202.6097\n",
      "Epoch 556/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -828.2334 - g_loss: -427.6649\n",
      "Epoch 557/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -836.8351 - g_loss: -732.6745\n",
      "Epoch 558/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -852.4268 - g_loss: -1258.4603\n",
      "Epoch 559/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -855.5487 - g_loss: -1447.8145\n",
      "Epoch 560/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -823.6686 - g_loss: -1062.0854\n",
      "Epoch 561/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -866.3967 - g_loss: -726.5288\n",
      "Epoch 562/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -832.2890 - g_loss: -964.8062\n",
      "Epoch 563/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -811.3090 - g_loss: -1366.9592\n",
      "Epoch 564/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -844.5238 - g_loss: -1343.1262\n",
      "Epoch 565/2000\n",
      "44/44 [==============================] - 10s 225ms/step - d_loss: -822.1898 - g_loss: -1549.3712\n",
      "Epoch 566/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -868.0950 - g_loss: -1956.5210\n",
      "Epoch 567/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -869.3426 - g_loss: -1795.4532\n",
      "Epoch 568/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -858.4878 - g_loss: -1929.4009\n",
      "Epoch 569/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -820.1506 - g_loss: -1688.0411\n",
      "Epoch 570/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -842.3462 - g_loss: -1953.3678\n",
      "Epoch 571/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -825.8839 - g_loss: -1838.3275\n",
      "Epoch 572/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -913.3026 - g_loss: -1334.3970\n",
      "Epoch 573/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -868.8866 - g_loss: -1422.1871\n",
      "Epoch 574/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -899.8004 - g_loss: -1174.1120\n",
      "Epoch 575/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -873.9630 - g_loss: -1246.1813\n",
      "Epoch 576/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -882.5597 - g_loss: -1234.9042\n",
      "Epoch 577/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -860.7117 - g_loss: -1173.0812\n",
      "Epoch 578/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -871.5955 - g_loss: -948.1982\n",
      "Epoch 579/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -865.3828 - g_loss: -1088.1994\n",
      "Epoch 580/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -832.6806 - g_loss: -1562.5004\n",
      "Epoch 581/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -820.2347 - g_loss: -1262.6962\n",
      "Epoch 582/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -864.3952 - g_loss: -1488.6372\n",
      "Epoch 583/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -891.4833 - g_loss: -1074.6915\n",
      "Epoch 584/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -924.0428 - g_loss: -1015.8574\n",
      "Epoch 585/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -857.2187 - g_loss: -1105.7904\n",
      "Epoch 586/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -873.8584 - g_loss: -1107.4151\n",
      "Epoch 587/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -883.1408 - g_loss: -1141.1085\n",
      "Epoch 588/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -845.7590 - g_loss: -1024.7650\n",
      "Epoch 589/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -837.3651 - g_loss: -793.8939\n",
      "Epoch 590/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -898.6305 - g_loss: -950.4413\n",
      "Epoch 591/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -869.5999 - g_loss: -1045.4947\n",
      "Epoch 592/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -911.2268 - g_loss: -1122.9832\n",
      "Epoch 593/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -909.2720 - g_loss: -516.2039\n",
      "Epoch 594/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -833.0251 - g_loss: -601.2357\n",
      "Epoch 595/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -865.9991 - g_loss: -150.8944\n",
      "Epoch 596/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -895.0357 - g_loss: -294.9807\n",
      "Epoch 597/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -913.1848 - g_loss: 123.5195\n",
      "Epoch 598/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -919.0547 - g_loss: 226.3736\n",
      "Epoch 599/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -847.7146 - g_loss: -140.0112\n",
      "Epoch 600/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -845.2605 - g_loss: -251.7500\n",
      "Epoch 601/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -846.1337 - g_loss: -139.7330\n",
      "Epoch 602/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -837.4876 - g_loss: -190.1521\n",
      "Epoch 603/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -869.8487 - g_loss: -393.8759\n",
      "Epoch 604/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -879.2888 - g_loss: -190.4968\n",
      "Epoch 605/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -849.8186 - g_loss: -133.0446\n",
      "Epoch 606/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -912.2283 - g_loss: -177.5601\n",
      "Epoch 607/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -870.4504 - g_loss: 83.6057\n",
      "Epoch 608/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -879.3515 - g_loss: -195.5893\n",
      "Epoch 609/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -912.1273 - g_loss: 46.7193\n",
      "Epoch 610/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -894.1084 - g_loss: -214.1148\n",
      "Epoch 611/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -896.1059 - g_loss: -196.2042\n",
      "Epoch 612/2000\n",
      "44/44 [==============================] - 10s 235ms/step - d_loss: -854.1067 - g_loss: -158.6745\n",
      "Epoch 613/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -910.3682 - g_loss: -75.5014\n",
      "Epoch 614/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -880.9251 - g_loss: 164.2590\n",
      "Epoch 615/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -895.0022 - g_loss: 402.0204\n",
      "Epoch 616/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -855.6635 - g_loss: 268.5396\n",
      "Epoch 617/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -866.5829 - g_loss: 44.1068\n",
      "Epoch 618/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -913.6490 - g_loss: -78.9751\n",
      "Epoch 619/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -895.7526 - g_loss: 324.1659\n",
      "Epoch 620/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -889.3801 - g_loss: 342.4085\n",
      "Epoch 621/2000\n",
      "44/44 [==============================] - 17s 391ms/step - d_loss: -866.0188 - g_loss: 232.8624\n",
      "Epoch 622/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -827.6982 - g_loss: 250.7456\n",
      "Epoch 623/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -860.1850 - g_loss: 720.8924\n",
      "Epoch 624/2000\n",
      "44/44 [==============================] - 10s 236ms/step - d_loss: -864.7615 - g_loss: 403.0475\n",
      "Epoch 625/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -901.7429 - g_loss: -80.8628\n",
      "Epoch 626/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -863.2611 - g_loss: -56.5752\n",
      "Epoch 627/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -876.5048 - g_loss: 40.0515\n",
      "Epoch 628/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -932.5611 - g_loss: -18.9437\n",
      "Epoch 629/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -893.0102 - g_loss: 49.7356\n",
      "Epoch 630/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -937.6200 - g_loss: 225.7114\n",
      "Epoch 631/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -878.8253 - g_loss: 510.0448\n",
      "Epoch 632/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -897.1571 - g_loss: 160.1998\n",
      "Epoch 633/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -899.0081 - g_loss: -163.9561\n",
      "Epoch 634/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -925.5869 - g_loss: -247.2723\n",
      "Epoch 635/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -912.2526 - g_loss: 636.0929\n",
      "Epoch 636/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -868.2738 - g_loss: 73.5141\n",
      "Epoch 637/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -852.4893 - g_loss: -499.5834\n",
      "Epoch 638/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -895.8875 - g_loss: -301.0032\n",
      "Epoch 639/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -932.9044 - g_loss: -210.6981\n",
      "Epoch 640/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -876.1704 - g_loss: 168.9536\n",
      "Epoch 641/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -900.0815 - g_loss: -58.8356\n",
      "Epoch 642/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -883.9319 - g_loss: -13.1289\n",
      "Epoch 643/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -897.7782 - g_loss: 333.2907\n",
      "Epoch 644/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -906.1166 - g_loss: 35.0861\n",
      "Epoch 645/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -894.6418 - g_loss: -139.8476\n",
      "Epoch 646/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -866.6571 - g_loss: -340.8817\n",
      "Epoch 647/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -894.4350 - g_loss: -97.9008\n",
      "Epoch 648/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -876.1798 - g_loss: 165.0900\n",
      "Epoch 649/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -851.0671 - g_loss: 44.6331\n",
      "Epoch 650/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -884.1944 - g_loss: 339.0724\n",
      "Epoch 651/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -911.3061 - g_loss: 801.3712\n",
      "Epoch 652/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -916.6459 - g_loss: 361.4326\n",
      "Epoch 653/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -914.7247 - g_loss: 224.8778\n",
      "Epoch 654/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -896.9130 - g_loss: 196.9629\n",
      "Epoch 655/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -889.4557 - g_loss: 121.9768\n",
      "Epoch 656/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -925.0978 - g_loss: -299.3434\n",
      "Epoch 657/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -873.6016 - g_loss: 74.9868\n",
      "Epoch 658/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -857.4415 - g_loss: 489.0962\n",
      "Epoch 659/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -857.2178 - g_loss: 465.2983\n",
      "Epoch 660/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -904.7477 - g_loss: 168.5727\n",
      "Epoch 661/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -897.9593 - g_loss: 541.6897\n",
      "Epoch 662/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -883.1666 - g_loss: 122.2160\n",
      "Epoch 663/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -940.6749 - g_loss: 567.7841\n",
      "Epoch 664/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -903.9096 - g_loss: 669.7439\n",
      "Epoch 665/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -915.7668 - g_loss: 346.6531\n",
      "Epoch 666/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -894.3060 - g_loss: 958.6265\n",
      "Epoch 667/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -864.3695 - g_loss: 660.5952\n",
      "Epoch 668/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -917.5755 - g_loss: 336.0268\n",
      "Epoch 669/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -929.2687 - g_loss: 660.2354\n",
      "Epoch 670/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -877.2490 - g_loss: 932.2636\n",
      "Epoch 671/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -886.2070 - g_loss: 549.1905\n",
      "Epoch 672/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -909.8333 - g_loss: 1160.1097\n",
      "Epoch 673/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -903.8419 - g_loss: 922.8659\n",
      "Epoch 674/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -872.9737 - g_loss: 1235.4397\n",
      "Epoch 675/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -870.4592 - g_loss: 1317.9496\n",
      "Epoch 676/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -862.8131 - g_loss: 798.6674\n",
      "Epoch 677/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -899.0419 - g_loss: 569.9277\n",
      "Epoch 678/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -867.5275 - g_loss: 794.0962\n",
      "Epoch 679/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -939.9048 - g_loss: 837.1793\n",
      "Epoch 680/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -920.0268 - g_loss: 1139.8866\n",
      "Epoch 681/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -938.9971 - g_loss: 775.3913\n",
      "Epoch 682/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -899.6787 - g_loss: 753.2241\n",
      "Epoch 683/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -896.1478 - g_loss: 1028.6323\n",
      "Epoch 684/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -879.2266 - g_loss: 796.0230\n",
      "Epoch 685/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -934.8953 - g_loss: 436.7557\n",
      "Epoch 686/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -864.8576 - g_loss: 203.7608\n",
      "Epoch 687/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -897.6073 - g_loss: 772.6901\n",
      "Epoch 688/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -838.1631 - g_loss: 645.9319\n",
      "Epoch 689/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -879.7362 - g_loss: 348.9911\n",
      "Epoch 690/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -852.7787 - g_loss: 568.1561\n",
      "Epoch 691/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -911.3321 - g_loss: 1154.7186\n",
      "Epoch 692/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -939.4475 - g_loss: 1127.4008\n",
      "Epoch 693/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -884.9856 - g_loss: 1392.8826\n",
      "Epoch 694/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -908.2615 - g_loss: 1721.2515\n",
      "Epoch 695/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -867.9035 - g_loss: 1406.6569\n",
      "Epoch 696/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -902.8481 - g_loss: 1626.3034\n",
      "Epoch 697/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -846.8495 - g_loss: 1459.8079\n",
      "Epoch 698/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -913.9903 - g_loss: 1833.7374\n",
      "Epoch 699/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -896.2267 - g_loss: 1854.0622\n",
      "Epoch 700/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -907.6808 - g_loss: 1193.2453\n",
      "Epoch 701/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -905.1462 - g_loss: 1163.8745\n",
      "Epoch 702/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -908.6632 - g_loss: 1236.4029\n",
      "Epoch 703/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -911.4811 - g_loss: 1074.9476\n",
      "Epoch 704/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -886.4890 - g_loss: 1700.7523\n",
      "Epoch 705/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -922.6046 - g_loss: 1465.3826\n",
      "Epoch 706/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -890.3126 - g_loss: 1712.7980\n",
      "Epoch 707/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -925.4247 - g_loss: 1735.2799\n",
      "Epoch 708/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -921.6839 - g_loss: 1628.8835\n",
      "Epoch 709/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -873.1888 - g_loss: 1325.3537\n",
      "Epoch 710/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -867.3783 - g_loss: 1598.5889\n",
      "Epoch 711/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -947.0762 - g_loss: 1436.8558\n",
      "Epoch 712/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -878.7163 - g_loss: 1722.2264\n",
      "Epoch 713/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -848.5631 - g_loss: 1085.2004\n",
      "Epoch 714/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -909.6608 - g_loss: 1730.8694\n",
      "Epoch 715/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -959.8458 - g_loss: 1429.5863\n",
      "Epoch 716/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -920.5125 - g_loss: 1947.2877\n",
      "Epoch 717/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -931.0017 - g_loss: 1658.0949\n",
      "Epoch 718/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -916.6892 - g_loss: 2308.3709\n",
      "Epoch 719/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -884.8370 - g_loss: 2219.1702\n",
      "Epoch 720/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -900.7380 - g_loss: 2259.4024\n",
      "Epoch 721/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -947.3196 - g_loss: 2328.7225\n",
      "Epoch 722/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -896.6237 - g_loss: 2436.1492\n",
      "Epoch 723/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -915.1905 - g_loss: 2795.1616\n",
      "Epoch 724/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -918.9931 - g_loss: 3066.4645\n",
      "Epoch 725/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -925.4328 - g_loss: 2904.3130\n",
      "Epoch 726/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -884.5974 - g_loss: 2380.5061\n",
      "Epoch 727/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -937.9806 - g_loss: 1967.6215\n",
      "Epoch 728/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -931.9158 - g_loss: 1765.3070\n",
      "Epoch 729/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -924.1973 - g_loss: 1842.2106\n",
      "Epoch 730/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -882.1017 - g_loss: 1804.0489\n",
      "Epoch 731/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -938.2809 - g_loss: 1974.7118\n",
      "Epoch 732/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -895.2565 - g_loss: 1883.0984\n",
      "Epoch 733/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -910.0096 - g_loss: 2156.0095\n",
      "Epoch 734/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -956.8743 - g_loss: 2153.0026\n",
      "Epoch 735/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -920.2050 - g_loss: 2572.5657\n",
      "Epoch 736/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -885.5722 - g_loss: 2358.2325\n",
      "Epoch 737/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -909.8736 - g_loss: 2280.8109\n",
      "Epoch 738/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -896.7424 - g_loss: 2163.1758\n",
      "Epoch 739/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -906.3422 - g_loss: 2388.1573\n",
      "Epoch 740/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -894.5582 - g_loss: 1752.9147\n",
      "Epoch 741/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -909.2893 - g_loss: 2347.2753\n",
      "Epoch 742/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -893.0470 - g_loss: 2223.3985\n",
      "Epoch 743/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -886.2459 - g_loss: 2265.4308\n",
      "Epoch 744/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -926.8436 - g_loss: 2292.2378\n",
      "Epoch 745/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -878.5580 - g_loss: 2456.2645\n",
      "Epoch 746/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -923.7564 - g_loss: 2376.5703\n",
      "Epoch 747/2000\n",
      "44/44 [==============================] - 10s 236ms/step - d_loss: -896.5281 - g_loss: 2729.3644\n",
      "Epoch 748/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -919.8752 - g_loss: 2371.4713\n",
      "Epoch 749/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -907.4923 - g_loss: 2715.7711\n",
      "Epoch 750/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -908.4449 - g_loss: 2278.6205\n",
      "Epoch 751/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -869.5873 - g_loss: 2376.7903\n",
      "Epoch 752/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -855.7937 - g_loss: 2586.2229\n",
      "Epoch 753/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -868.8312 - g_loss: 2677.1911\n",
      "Epoch 754/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -901.8591 - g_loss: 2688.3014\n",
      "Epoch 755/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -902.3768 - g_loss: 2424.9049\n",
      "Epoch 756/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -935.8447 - g_loss: 2834.2442\n",
      "Epoch 757/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -915.1914 - g_loss: 2789.9646\n",
      "Epoch 758/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -928.4243 - g_loss: 3185.2251\n",
      "Epoch 759/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -899.0532 - g_loss: 2950.5385\n",
      "Epoch 760/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -947.6781 - g_loss: 3368.4710\n",
      "Epoch 761/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -956.8407 - g_loss: 2931.8124\n",
      "Epoch 762/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -950.0955 - g_loss: 3348.6294\n",
      "Epoch 763/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -940.0325 - g_loss: 3124.2335\n",
      "Epoch 764/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -921.6687 - g_loss: 2871.1625\n",
      "Epoch 765/2000\n",
      "44/44 [==============================] - 10s 226ms/step - d_loss: -890.1947 - g_loss: 3316.1536\n",
      "Epoch 766/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -893.7324 - g_loss: 3148.5000\n",
      "Epoch 767/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -864.5354 - g_loss: 3294.5122\n",
      "Epoch 768/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -979.9572 - g_loss: 3075.9901\n",
      "Epoch 769/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -913.2992 - g_loss: 3116.5303\n",
      "Epoch 770/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -889.1989 - g_loss: 2914.9619\n",
      "Epoch 771/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -899.9280 - g_loss: 2841.7866\n",
      "Epoch 772/2000\n",
      "44/44 [==============================] - 14s 324ms/step - d_loss: -904.1381 - g_loss: 2969.6529\n",
      "Epoch 773/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -953.2236 - g_loss: 3226.3647\n",
      "Epoch 774/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -977.7665 - g_loss: 3617.7466\n",
      "Epoch 775/2000\n",
      "44/44 [==============================] - 10s 235ms/step - d_loss: -915.8213 - g_loss: 3551.3339\n",
      "Epoch 776/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -909.5413 - g_loss: 3485.3680\n",
      "Epoch 777/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -952.4464 - g_loss: 3511.1988\n",
      "Epoch 778/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -961.3459 - g_loss: 3289.7385\n",
      "Epoch 779/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -924.1878 - g_loss: 2795.4310\n",
      "Epoch 780/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -907.5170 - g_loss: 2800.9462\n",
      "Epoch 781/2000\n",
      "44/44 [==============================] - 10s 235ms/step - d_loss: -935.8306 - g_loss: 2871.0320\n",
      "Epoch 782/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -953.7402 - g_loss: 3185.7018\n",
      "Epoch 783/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -951.5244 - g_loss: 2868.7737\n",
      "Epoch 784/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -912.3620 - g_loss: 3363.2104\n",
      "Epoch 785/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -917.8761 - g_loss: 3199.3974\n",
      "Epoch 786/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -923.3612 - g_loss: 3541.9553\n",
      "Epoch 787/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -936.4709 - g_loss: 3707.4434\n",
      "Epoch 788/2000\n",
      "44/44 [==============================] - 10s 234ms/step - d_loss: -927.9381 - g_loss: 3476.2841\n",
      "Epoch 789/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -934.0814 - g_loss: 3790.6646\n",
      "Epoch 790/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -953.5314 - g_loss: 3375.1999\n",
      "Epoch 791/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -918.4765 - g_loss: 3371.5038\n",
      "Epoch 792/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -909.0761 - g_loss: 3551.4867\n",
      "Epoch 793/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -966.4762 - g_loss: 3235.4724\n",
      "Epoch 794/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -916.0029 - g_loss: 3351.0615\n",
      "Epoch 795/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -903.1119 - g_loss: 3252.3489\n",
      "Epoch 796/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -917.9492 - g_loss: 3349.2598\n",
      "Epoch 797/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -887.0080 - g_loss: 3217.0720\n",
      "Epoch 798/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -953.8868 - g_loss: 3155.9363\n",
      "Epoch 799/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -921.7328 - g_loss: 3297.1288\n",
      "Epoch 800/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -953.5651 - g_loss: 3266.1893\n",
      "Epoch 801/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -955.8941 - g_loss: 2953.9984\n",
      "Epoch 802/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -885.1612 - g_loss: 3318.8276\n",
      "Epoch 803/2000\n",
      "44/44 [==============================] - 10s 228ms/step - d_loss: -910.8954 - g_loss: 3103.3490\n",
      "Epoch 804/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -942.3019 - g_loss: 3182.1864\n",
      "Epoch 805/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -986.5200 - g_loss: 3225.5080\n",
      "Epoch 806/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -935.7254 - g_loss: 3207.0882\n",
      "Epoch 807/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -919.1816 - g_loss: 3124.5727\n",
      "Epoch 808/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -895.6029 - g_loss: 3057.8441\n",
      "Epoch 809/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -925.7474 - g_loss: 3111.6787\n",
      "Epoch 810/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -967.6824 - g_loss: 3441.7524\n",
      "Epoch 811/2000\n",
      "44/44 [==============================] - 10s 229ms/step - d_loss: -887.3698 - g_loss: 2871.2669\n",
      "Epoch 812/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -912.4999 - g_loss: 3110.5063\n",
      "Epoch 813/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -948.5448 - g_loss: 3102.5487\n",
      "Epoch 814/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -933.2863 - g_loss: 3353.1946\n",
      "Epoch 815/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -936.9025 - g_loss: 2687.1775\n",
      "Epoch 816/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -937.5653 - g_loss: 3286.6803\n",
      "Epoch 817/2000\n",
      "44/44 [==============================] - 10s 227ms/step - d_loss: -976.3356 - g_loss: 3181.4277\n",
      "Epoch 818/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -976.8191 - g_loss: 3172.5557\n",
      "Epoch 819/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -857.0116 - g_loss: 3192.7893\n",
      "Epoch 820/2000\n",
      "44/44 [==============================] - 10s 231ms/step - d_loss: -946.6798 - g_loss: 2988.6963\n",
      "Epoch 821/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -925.1018 - g_loss: 3264.6273\n",
      "Epoch 822/2000\n",
      "44/44 [==============================] - 10s 230ms/step - d_loss: -965.6389 - g_loss: 2877.3918\n",
      "Epoch 823/2000\n",
      "44/44 [==============================] - 10s 233ms/step - d_loss: -892.0510 - g_loss: 2853.1823\n",
      "Epoch 824/2000\n",
      "44/44 [==============================] - 10s 236ms/step - d_loss: -939.6443 - g_loss: 2835.4780\n",
      "Epoch 825/2000\n",
      "44/44 [==============================] - 10s 232ms/step - d_loss: -957.2226 - g_loss: 3202.1654\n",
      "Epoch 826/2000\n",
      "17/44 [==========>...................] - ETA: 5s - d_loss: -923.8150 - g_loss: 2237.6381"
     ]
    }
   ],
   "source": [
    "# Start training the model.\n",
    "wgan.fit(train_ds, batch_size=BATCH_SIZE, epochs=2000, callbacks=[cbk, tb_cbk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Examples using learned Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 11.5, 11.5, -0.5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load generator\n",
    "'''\n",
    "generator.compile(optimizer=Adam(lr=0.0008), # per Foster, 2017 RMSprop(lr=0.0008)\n",
    "                          loss=binary_crossentropy,\n",
    "                          metrics=['accuracy'])\n",
    "generator = tf.keras.models.load_model('/data/output/models/dwarfganWGANGPR02/generator-2021-04-04_025322.h5')\n",
    "'''\n",
    "# generate new example of learned representation in latent space\n",
    "try:\n",
    "    generator\n",
    "except NameError:\n",
    "    #get latest generator model save file\n",
    "    folder = pathlib.Path(f'{out_model_dir}/{model_name}')\n",
    "    saves = list(folder.glob('generator*'))\n",
    "    latest = max(saves, key=os.path.getctime)\n",
    "    #load latest generator save file\n",
    "    generator = tf.keras.models.load_model(latest)\n",
    "        \n",
    "noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "tiles = load_tiles()\n",
    "tiles = tiles[0].reshape((1,12,10,256)) #resphae into same shape as noise input\n",
    "res = np.array(generator.predict({'Generator_Input':noise,'Tiles_Input':tiles})).astype('uint8')\n",
    "\n",
    "#Rescale\n",
    "res = res.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Visualize result\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(res)\n",
    "plt.title(f'Example Generator Output')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
