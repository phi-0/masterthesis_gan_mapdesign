{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DwarfGAN - Deep Learning based Map Design for Dwarf Fortress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "Number of GPUs found: 2\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from keras.layers import Add, Concatenate, Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D, LayerNormalization, Cropping2D\n",
    "from keras.layers.experimental import preprocessing\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.losses import binary_crossentropy, Loss\n",
    "from keras import metrics\n",
    "from functools import partial\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import random\n",
    "\n",
    "#!pip install boto3\n",
    "import boto3 as b3 \n",
    "\n",
    "\n",
    "print(f'tensorflow version: {tf.__version__}')\n",
    "NUM_GPUS = len(tf.config.list_physical_devices('GPU'))\n",
    "print(f'Number of GPUs found: {NUM_GPUS}')\n",
    "\n"
   ]
  },
  {
   "source": [
    "# CONFIGURATION\n",
    "Please adjust the following settings, especially the path specifications.\n",
    "\n",
    "### NOTE: \n",
    "- input file path (*fpath*): the implementation of the keras preprocessing layer used to load image files from a directory requires the image files to reside within a single folder named **maps** nested into a parent directory. The filepath specified below should refer to the **parent directory**\n",
    "- tensorboard directory (*tboard_dir*): this folder will house the tensorboard files to visualize the training progress in tensorboard\n",
    "- model saves (*out_model_dir*): this folder will be used to save the model save files (.h5) after each epoch. A sub-folder will be created automatically named after the configure *model_name*\n",
    "- image saves (*out_img_dir*): the directory where 5 generated images are saved per epoch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### CONFIG ###################\n",
    "\n",
    "# model name\n",
    "model_name = 'dwarfganWGANGPR09Tiles02'\n",
    "# folder path to input files (map images)\n",
    "fpath = r'/data2/input'\n",
    "# folder path to tensorboard output\n",
    "tboard_dir = '/data2/output/tensorboard'\n",
    "# folder path for saved model output\n",
    "out_model_dir = '/data2/output/models'\n",
    "# folder for images to be saved during training\n",
    "out_img_dir = '/data2/output/images'\n",
    "# use skip connections (additive/concatenate)?\n",
    "SKIP_ADD = False\n",
    "SKIP_CONCAT = False\n",
    "# use tileset input?\n",
    "USE_TILESET = True\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 4000 \n",
    "#BATCH_PER_EPOCH = 20\n",
    "# pre-processed (cropped) tiles are 12x12 pixels but will be cropped by the critic to 12x10\n",
    "IMAGE_SIZE = (12,12)\n",
    "BATCH_SIZE = 512\n",
    "CRITIC_FACTOR = 5 # number of times the critic is trained more often than the generator. Recommended = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "RELU_SLOPE_C = 0.2\n",
    "RELU_SLOPE_G = 0.2\n",
    "DROPOUT_C = 0.3\n",
    "MOMENTUM_G = 0.9\n",
    "CRIT_LR = 0.0002 # Adjusted learning rates according to two time-scale update rule (TTUR), see Heusel et al., 2017\n",
    "GEN_LR = 0.0001\n",
    "\n",
    "# NOTE: all extracted map PNGs have been saved on a separate virtual disk mapped to '/data' or '/data2' of the virtual machine in use\n",
    "data_dir = pathlib.Path(fpath + '/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Train / Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map images sourced from the DFMA come in a variety of dimensions. In order to create sample images with constant dimensions, as required by tensors, a selection of input map images based on the default ASCII tileset were run through a structured cropping script to extract 12x12 tiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12854 cropped image samples available\n"
     ]
    }
   ],
   "source": [
    "# use pre-processed (cropped) 12 x 12 images\n",
    "data_dir = pathlib.Path(fpath + '/ascii_crops_tiles/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "print(f'There are {str(len(imgs))} cropped image samples available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random sample input image:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAIAAADZF8uwAAAALElEQVR4nGNkwAvKY/UYGBiY8CsiFkAMG5qAEUI1XQ6EMOp012MqolI4EQsAkZIFCwij75oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=12x12 at 0x7FADD008BEB8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show example sample image (cropped to 128x128)\n",
    "print('A random sample input image:')\n",
    "PIL.Image.open(imgs[random.randint(0,len(imgs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12912 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# creating keras datasets for training and validation - refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "dataset_train = tf.keras.preprocessing.image_dataset_from_directory(  fpath+'/ascii_crops_tiles',\n",
    "                                                                      image_size=IMAGE_SIZE, \n",
    "                                                                      batch_size=BATCH_SIZE, \n",
    "                                                                      #labels=[1.] * len(imgs), # setting all labels to 1.0 (for 'real') as float32\n",
    "                                                                      #label_mode=None, # yields float32 type labels\n",
    "                                                                      seed=875 #,\n",
    "                                                                   )\n",
    "\n",
    "#drop last batch that contains less samples (to match the constant input shape of the tile data)\n",
    "dataset_train = dataset_train.take(len(dataset_train)-1)\n",
    "# refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = dataset_train.cache().prefetch(buffer_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAINING = 12912 # = 20% of total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Random Sample from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check random images from prepared batches\n",
    "plt.figure(figsize=(5, 5))\n",
    "for images, labels in train_ds.take(1): # take one batch. Here batch_size = 128 examples per batch\n",
    "    for i in range(9): # show first 9 images of batch\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(f'img {i}')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATED Model Definition\n",
    "\n",
    "Generally the following changes have been implemented to the architectures:\n",
    "\n",
    "### RUN09NoTiles\n",
    "\n",
    "- instead of generating full maps, this model focuses on learning to recreate single tiles which represent the different objects/elements in the game world\n",
    "- This model does NOT use the tileset as a second input\n",
    "\n",
    "\n",
    "### RUN09Tiles\n",
    "- implemented secondary input for both critic and generator which introduces the single tiles cropped from the tileset files (12x10) as a 12x10x256 vector (256 different 12x10 tiles) \n",
    "- add a Dense layer or 1x1 CONV between Tiles input and concat layer to allow more flexibility for the model to learn which parts are considered (the model was run both with and without this layer)\n",
    "- increase batch size as much as possible (currently 512)\n",
    "\n",
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discriminator_model():\n",
    "\n",
    "    # DISCRIMINATOR\n",
    "    # set input variables to variable width + height. Will be cropped in preprocessing [CURRENTLY FIXED TO 256x256]\n",
    "    input_dim = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "\n",
    "    # Input\n",
    "    d_input = Input(shape=input_dim, name='Discriminator_Input')\n",
    "    \n",
    "    \n",
    "    #we crop the images y dimension to 12x10 to match the tile dimensions. To stabilize the output we do NOT use random crop.\n",
    "    x = Cropping2D(((0,0),(0,2)))(d_input) #cropping details ((top, bottom),(left, right))\n",
    "\n",
    "    x = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale'\n",
    "                               )(x) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    # START TILES INPUT\n",
    "    if USE_TILESET:\n",
    "        tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "\n",
    "        tiles = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                    offset=-1,\n",
    "                                    name='Preprocessing_Rescale_Tiles'\n",
    "                                   )(tiles_input) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "\n",
    "        #tiles = Conv2D(filters=256, kernel_size=(1,1), strides=1)(tiles)\n",
    "\n",
    "        x = Concatenate()([x, tiles])\n",
    "    # END TILES INPUT\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 0\n",
    "    x = Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = (3,3), \n",
    "            strides = 1,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_0'\n",
    "    )(x)\n",
    "    \n",
    "    # Activation 0 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_0')(x)\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 1\n",
    "    x = Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_1'\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 1\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 1 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_1')(x)\n",
    "\n",
    "    # Dropout 1\n",
    "    x = Dropout(rate = DROPOUT_C)(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Conv2D Layer 2\n",
    "    x = Conv2D(\n",
    "            filters = 256,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            name = 'Discriminator_Conv2D_Layer_3',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 2\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 2 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_3')(x)\n",
    "\n",
    "\n",
    "    # OUTPUT\n",
    "    x = Flatten()(x)\n",
    "    #x = Dropout(DROPOUT_C)(x)\n",
    "    \n",
    "    d_output = Dense(1, \n",
    "                     #activation='sigmoid', \n",
    "                     kernel_initializer = RandomNormal(mean=0, stddev=0.02) # random initialization of weights with normal distribution around 0 with small SD\n",
    "                    )(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Discriminator Model intialization\n",
    "    if USE_TILESET:\n",
    "        discriminator = Model([d_input, tiles_input], d_output, name='Discriminator')\n",
    "    else:\n",
    "        discriminator = Model(d_input, d_output, name='Discriminator')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Discriminator_Input (InputLayer [(None, 12, 12, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d (Cropping2D)         (None, 12, 10, 3)    0           Discriminator_Input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Tiles_Input (InputLayer)        [(None, 12, 10, 256) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_Rescale (Rescalin (None, 12, 10, 3)    0           cropping2d[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_Rescale_Tiles (Re (None, 12, 10, 256)  0           Tiles_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 12, 10, 259)  0           Preprocessing_Rescale[0][0]      \n",
      "                                                                 Preprocessing_Rescale_Tiles[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_0 (C (None, 12, 10, 64)   149248      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Activation_0 (LeakyReLU)        (None, 12, 10, 64)   0           Discriminator_Conv2D_Layer_0[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_1 (C (None, 6, 5, 128)    73856       Activation_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_1 (LeakyReLU)        (None, 6, 5, 128)    0           Discriminator_Conv2D_Layer_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 6, 5, 128)    0           Activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_3 (C (None, 3, 3, 256)    295168      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Activation_3 (LeakyReLU)        (None, 3, 3, 256)    0           Discriminator_Conv2D_Layer_3[0][0\n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2304)         0           Activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2305        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 520,577\n",
      "Trainable params: 520,577\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = discriminator_model()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    # GENERATOR\n",
    "\n",
    "    # set input variable dimensions. Here we will start out with a vector of length 100 for each sample (sampled from a normal distribution, representing the learned latent space)\n",
    "    input_dim = (LATENT_DIM)\n",
    "\n",
    "    # Input\n",
    "    g_input = Input(shape=input_dim, name='Generator_Input')\n",
    "\n",
    "    # Dense Layer 1\n",
    "    x = Dense(np.prod([3,3,512]), kernel_initializer = RandomNormal(mean=0., stddev=0.02), \n",
    "              use_bias=False)(g_input) # use_bias=False see https://keras.io/examples/generative/wgan_gp/\n",
    "\n",
    "    # Batch Norm Layer 1\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 1\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Reshape into 3D tensor\n",
    "    x = Reshape((3,3,512))(x)\n",
    "\n",
    "    # Upsampling Layer 1 + Conv2D Layer1\n",
    "    x = Conv2DTranspose(filters=512, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=256, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=(3,3), padding='same', strides=(1,1), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    # START TILES\n",
    "    if USE_TILESET:\n",
    "        tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "        tiles = preprocessing.Resizing(width=12, height=12)(tiles_input)\n",
    "        #tiles = Conv2D(filters=256, kernel_size=(1,1), strides=1)(tiles)\n",
    "\n",
    "        x = Concatenate()([x, tiles])\n",
    "    \n",
    "    # END TILES\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "    \n",
    "    # reduce output dimensions via 1x1 convolution\n",
    "    x = Conv2D(filters=3, kernel_size=(1,1), padding='same', strides=(1,1),\n",
    "               kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "        \n",
    "    # tanh activation layer to scale values to [-1:1]\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    # Batch Norm Layer 7\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "\n",
    "    # Output - Rescale Values back to [0:255] since the discriminator will automatically rescale back down to [-1:1] as part of the pre-processing pipeline\n",
    "    g_output = (255 / 2) * (x + 1) \n",
    "\n",
    "\n",
    "    # Generator Model initialization\n",
    "    if USE_TILESET:\n",
    "        generator = Model([g_input, tiles_input], g_output, name='Generator')\n",
    "    else:\n",
    "        generator = Model(g_input, g_output, name='Generator')\n",
    "    \n",
    "    \n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Generator_Input (InputLayer)    [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4608)         589824      Generator_Input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 4608)         18432       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 4608)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 3, 3, 512)    0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 6, 6, 512)    2359296     reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 6, 6, 512)    2048        conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 6, 6, 512)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 12, 12, 256)  1179648     leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 12, 12, 256)  1024        conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Tiles_Input (InputLayer)        [(None, 12, 10, 256) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 12, 12, 128)  294912      leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "resizing (Resizing)             (None, 12, 12, 256)  0           Tiles_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12, 12, 384)  0           conv2d_transpose_2[0][0]         \n",
      "                                                                 resizing[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 12, 12, 384)  1536        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 12, 12, 384)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 12, 12, 3)    1152        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 12, 12, 3)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 12, 12, 3)]  0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 12, 12, 3)]  0           tf_op_layer_AddV2[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 4,447,872\n",
      "Trainable params: 4,436,352\n",
      "Non-trainable params: 11,520\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen = generator_model()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tiles\n",
    "\n",
    "We load the 256 split 12x10 tiles into a (12,10,256) numpy array to feed to the network at training time. This input is only used for the \"RUN09Tiles\" model, when global option **USE_TILES** is set to *true*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiles():\n",
    "    tiles = []\n",
    "\n",
    "    data_dir = pathlib.Path('/data2/input/tiles/800x600/')\n",
    "    imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "    # show example sample image (cropped to 128x128)\n",
    "    PIL.Image.open(imgs[random.randint(0,len(imgs))])\n",
    "\n",
    "    for img in imgs:\n",
    "        tiles.append(np.asarray(PIL.Image.open(img)).astype('uint8'))\n",
    "\n",
    "    #reshape\n",
    "    ds_tiles = np.array(tiles)\n",
    "    ds_tiles = ds_tiles.reshape((1,12,10,256))\n",
    "    ds_tiles = ds_tiles.repeat(repeats=BATCH_SIZE//NUM_GPUS, axis=0)\n",
    "\n",
    "    return ds_tiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP (Full) Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we compile the models, we need to implement a custom loss function which uses the Wasserstein distance and a gradient penalty term in order to ensure 1 Lipschitz constraints are followed. A WGAN with GP further involves a slightly more complicated training process which trains the critic (discriminator without sigmoid activation function) by feeding three different kinds of images:\n",
    "\n",
    "1. real images (i.e. available samples)\n",
    "2. 'fake' images (i.e. constructed by the generator)\n",
    "3. random interpolations between real and fake images (i.e. random samples interpolated from values between the fake and real images)\n",
    "\n",
    "The full training process of a critic is depicted below (source: Foster, 2019, p. 122):\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"wgan_gp_critic_training.png\"></img>\n",
    "    <i>Computational Graph for one Discriminator Training Epoch. (Source: Foster, 2019, p.122)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below roughly follows the OOP-based framework set by keras see https://keras.io/examples/generative/wgan_gp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope(): \n",
    "\n",
    "    critic = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    if USE_TILESET:\n",
    "        ds_tiles = load_tiles()\n",
    "    else:\n",
    "        ds_tiles = None\n",
    "\n",
    "    class WGANGP(keras.Model):\n",
    "        def __init__(\n",
    "            self,\n",
    "            critic,\n",
    "            generator,\n",
    "            latent_dim,\n",
    "            tensorboard_callback,\n",
    "            critic_extra_steps=5,\n",
    "            gp_weight=10.0,\n",
    "            tiles=None\n",
    "        ):\n",
    "            super(WGANGP, self).__init__()\n",
    "            self.critic = critic\n",
    "            self.generator = generator\n",
    "            self.latent_dim = latent_dim\n",
    "            self.tensorboard_callback = tensorboard_callback\n",
    "            self.d_steps = critic_extra_steps\n",
    "            self.gp_weight = gp_weight\n",
    "            self.tiles=tiles\n",
    "            \n",
    "\n",
    "        def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "            super(WGANGP, self).compile()\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.g_optimizer = g_optimizer\n",
    "            self.d_loss_fn = d_loss_fn\n",
    "            self.g_loss_fn = g_loss_fn\n",
    "\n",
    "        def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "            \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "            This loss is calculated on an interpolated image\n",
    "            and added to the discriminator loss.\n",
    "            \"\"\"\n",
    "            # Get the interpolated image\n",
    "            alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "            diff = fake_images - real_images\n",
    "            interpolated = real_images + alpha * diff\n",
    "\n",
    "            with tf.GradientTape() as gp_tape:\n",
    "                gp_tape.watch(interpolated)\n",
    "                # 1. Get the discriminator output for this interpolated image.\n",
    "                if USE_TILESET:\n",
    "                    pred = self.critic({'Discriminator_Input':interpolated,'Tiles_Input':ds_tiles}, training=True)\n",
    "                else:\n",
    "                    pred = self.critic(interpolated, training=True)\n",
    "\n",
    "            # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "            grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "            # 3. Calculate the norm of the gradients.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "            gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "            return gp\n",
    "\n",
    "        def train_step(self, real_images):\n",
    "            #checking whether we handed a tuple of (numpy) data to .fit().\n",
    "            #if not, the data must be a tf.data.Dataset generator that yields batches of datasets (data, labels)\n",
    "            if isinstance(real_images, tuple):\n",
    "                real_images = real_images[0]\n",
    "\n",
    "            # Get the batch size\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "            # For each batch, we are going to perform the\n",
    "            # following steps as laid out in the original paper:\n",
    "            # 1. Train the generator and get the generator loss\n",
    "            # 2. Train the discriminator and get the discriminator loss\n",
    "            # 3. Calculate the gradient penalty\n",
    "            # 4. Multiply this gradient penalty with a constant weight factor = self.discriminator_extra_steps = 5 (default value)\n",
    "            # 5. Add the gradient penalty to the discriminator loss\n",
    "            # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "            # Train the discriminator first. The original paper recommends training\n",
    "            # the discriminator for `x` more steps (typically 5) as compared to generator\n",
    "            \n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.critic)\n",
    "            \n",
    "            for i in range(self.d_steps):\n",
    "                # Get the latent vector\n",
    "                random_latent_vectors = tf.random.normal(\n",
    "                    shape=(batch_size, self.latent_dim)\n",
    "                )\n",
    "                with tf.GradientTape() as tape:\n",
    "                    if USE_TILESET:\n",
    "                        # Generate fake images from the latent vector\n",
    "                        fake_images = self.generator({'Generator_Input':random_latent_vectors,'Tiles_Input':ds_tiles}, training=True)\n",
    "                        # Get the logits for the fake images\n",
    "                        fake_logits = self.critic({'Discriminator_Input':fake_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "                        # Get the logits for the real images\n",
    "                        real_logits = self.critic({'Discriminator_Input':real_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "                    else:\n",
    "                        # Generate fake images from the latent vector\n",
    "                        fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                        # Get the logits for the fake images\n",
    "                        fake_logits = self.critic(fake_images, training=True)\n",
    "                        # Get the logits for the real images\n",
    "                        real_logits = self.critic(real_images, training=True)\n",
    "\n",
    "                    # Calculate the discriminator loss using the fake and real image logits\n",
    "                    d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                    # Calculate the gradient penalty\n",
    "                    gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                    # Add the gradient penalty to the original discriminator loss\n",
    "                    d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "                # Get the gradients w.r.t the discriminator loss\n",
    "                d_gradient = tape.gradient(d_loss, self.critic.trainable_variables)\n",
    "                # Update the weights of the discriminator using the discriminator optimizer\n",
    "                self.d_optimizer.apply_gradients(\n",
    "                    zip(d_gradient, self.critic.trainable_variables)\n",
    "                )\n",
    "\n",
    "            # Train the generator\n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.generator)\n",
    "            \n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                if USE_TILESET:\n",
    "                    # Generate fake images using the generator\n",
    "                    generated_images = self.generator({'Generator_Input':random_latent_vectors,'Tiles_Input':ds_tiles}, training=True)\n",
    "                    # Get the discriminator logits for fake images\n",
    "                    gen_img_logits = self.critic({'Discriminator_Input':generated_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "                else:\n",
    "                    # Generate fake images using the generator\n",
    "                    generated_images = self.generator(random_latent_vectors, training=True)\n",
    "                    # Get the discriminator logits for fake images\n",
    "                    gen_img_logits = self.critic(generated_images, training=True)\n",
    "                    \n",
    "                # Calculate the generator loss\n",
    "                g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "            # Get the gradients w.r.t the generator loss\n",
    "            gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "            # Update the weights of the generator using the generator optimizer\n",
    "            self.g_optimizer.apply_gradients(\n",
    "                zip(gen_gradient, self.generator.trainable_variables)\n",
    "            )\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "        \n",
    "    class GANMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, num_img=5, latent_dim=128):\n",
    "            self.num_img = num_img\n",
    "            self.latent_dim = latent_dim\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None): #on_epoch_end(self, epoch, logs=None):\n",
    "            '''\n",
    "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "            generated_images = self.model.generator(random_latent_vectors)\n",
    "            #generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "            for i in range(self.num_img):\n",
    "                img = generated_images[i].numpy()\n",
    "                img = keras.preprocessing.image.array_to_img(img)\n",
    "                img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "            '''\n",
    "            \n",
    "            # Sample generator output for num_img images\n",
    "            noise = np.random.normal(0, 1, (self.num_img, self.latent_dim))\n",
    "            if USE_TILESET:\n",
    "                gen_imgs = generator.predict({'Generator_Input':noise,'Tiles_Input':ds_tiles[0:5]})\n",
    "            else:\n",
    "                gen_imgs = generator.predict(noise)\n",
    "                \n",
    "            gen_imgs = gen_imgs.astype('uint8')\n",
    "\n",
    "            #!!!NOT NECESSARY ANYMORE AS IMPLEMENTED AS PART OF THE MODEL!!!\n",
    "            #gen_imgs = 0.5 * (gen_imgs + 1)  #scale back to [0:1]\n",
    "            gen_imgs = gen_imgs.reshape((self.num_img, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "            # save n example images\n",
    "            for i in range(self.num_img):\n",
    "                plt.figure(figsize=(5, 5))\n",
    "                plt.imshow(gen_imgs[i])\n",
    "                #plt.title(f'Example Generator Output')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # adjust path based on whether execution is local or on linux VM\n",
    "                if pathlib.Path(f'{out_img_dir}/{model_name}').exists():\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    #mkdir\n",
    "                    os.mkdir(f'{out_img_dir}/{model_name}')\n",
    "                    #save\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                    \n",
    "            # save corresponding model\n",
    "            now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "            \n",
    "            if pathlib.Path(f'{out_model_dir}/{model_name}').exists():\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5')        \n",
    "            else:\n",
    "                #make dir\n",
    "                os.mkdir(f'{out_model_dir}/{model_name}')\n",
    "                #write\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5') \n",
    "        \n",
    "        \n",
    "    # Instantiate the optimizer for both networks\n",
    "    # (learning_rate=0.0002, beta_1=0.5 are recommended) as per Radford et al. 2016 pp. 3-4\n",
    "    generator_optimizer = Adam(\n",
    "        learning_rate=GEN_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "    critic_optimizer = Adam(\n",
    "        learning_rate=CRIT_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def critic_loss(real_img, fake_img):\n",
    "        real_loss = tf.reduce_mean(real_img)\n",
    "        fake_loss = tf.reduce_mean(fake_img)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(fake_img):\n",
    "        return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "    # Instantiate the custome `GANMonitor` Keras callback.\n",
    "    cbk = GANMonitor(num_img=5, latent_dim=LATENT_DIM)\n",
    "    \n",
    "    # Instantiate the tensorboard tf.keras callback\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "    tb_cbk = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = f'{tboard_dir}/{model_name}_{now}', \n",
    "        write_graph = False, \n",
    "        write_images = True,\n",
    "        histogram_freq = 1) \n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGANGP(\n",
    "        critic=critic,\n",
    "        generator=generator,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        tensorboard_callback=tb_cbk,\n",
    "        critic_extra_steps=CRITIC_FACTOR,\n",
    "        gp_weight=GRADIENT_PENALTY_WEIGHT,\n",
    "        tiles=ds_tiles       \n",
    "        \n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "        d_optimizer=critic_optimizer,\n",
    "        g_optimizer=generator_optimizer,\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=critic_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model.\n",
    "wgan.fit(train_ds, batch_size=BATCH_SIZE, epochs=4000, callbacks=[cbk, tb_cbk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Examples using learned Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 10.5, 11.5, -0.5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load generator\n",
    "'''\n",
    "generator.compile(optimizer=Adam(lr=0.0008), # per Foster, 2017 RMSprop(lr=0.0008)\n",
    "                          loss=binary_crossentropy,\n",
    "                          metrics=['accuracy'])\n",
    "generator = tf.keras.models.load_model('/data/output/models/dwarfganWGANGPR02/generator-2021-04-04_025322.h5')\n",
    "'''\n",
    "# generate new example of learned representation in latent space\n",
    "try:\n",
    "    generator\n",
    "except NameError:\n",
    "    #get latest generator model save file\n",
    "    folder = pathlib.Path(f'{out_model_dir}/{model_name}')\n",
    "    saves = list(folder.glob('generator*'))\n",
    "    latest = max(saves, key=os.path.getctime)\n",
    "    #load latest generator save file\n",
    "    generator = tf.keras.models.load_model(latest)\n",
    "        \n",
    "noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "\n",
    "if USE_TILESET:\n",
    "    tiles = load_tiles()\n",
    "    tiles = tiles[0].reshape((1,12,10,256)) #resphae into same shape as noise input\n",
    "    res = np.array(generator.predict({'Generator_Input':noise,'Tiles_Input':tiles})).astype('uint8')\n",
    "else:\n",
    "    res = np.array(generator.predict(noise)).astype('uint8')\n",
    "\n",
    "#Rescale\n",
    "res = res.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Visualize result\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(res[:,0:11,:]) #cropping the image to 12x10 \n",
    "plt.title(f'Example Generator Output')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}