{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DwarfGAN - Deep Learning based Map Design for Dwarf Fortress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "Number of GPUs found: 2\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from keras.layers import Add, Concatenate, Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D, LayerNormalization\n",
    "from keras.layers.experimental import preprocessing\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.losses import binary_crossentropy, Loss\n",
    "from keras import metrics\n",
    "from functools import partial\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import random\n",
    "\n",
    "#!pip install boto3\n",
    "import boto3 as b3 \n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "NUM_GPUS = len(tf.config.list_physical_devices('GPU'))\n",
    "print(f'Number of GPUs found: {NUM_GPUS}')\n",
    "\n",
    "############### CONFIG ###################\n",
    "\n",
    "# model name\n",
    "model_name = 'dwarfganWGANGPR09Tiles'\n",
    "# folder path to input files (map images)\n",
    "fpath = r'/data2/input'\n",
    "# folder path to tensorboard output\n",
    "tboard_dir = '/data2/output/tensorboard'\n",
    "# folder path for saved model output\n",
    "out_model_dir = '/data2/output/models'\n",
    "# folder for images to be saved during training\n",
    "out_img_dir = '/data2/output/images'\n",
    "# use skip connections (additive/concatenate)?\n",
    "SKIP_ADD = False\n",
    "SKIP_CONCAT = False\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 1000 \n",
    "#BATCH_PER_EPOCH = 20\n",
    "# pre-processed (cropped) tiles are 12x12 pixels\n",
    "IMAGE_SIZE = (12,12)\n",
    "BATCH_SIZE = 128\n",
    "CRITIC_FACTOR = 5 # number of times the critic is trained more often than the generator. Recommended = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "RELU_SLOPE_C = 0.2\n",
    "RELU_SLOPE_G = 0.2\n",
    "DROPOUT_C = 0.3\n",
    "MOMENTUM_G = 0.9\n",
    "CRIT_LR = 0.0003 # Adjusted learning rates according to two time-scale update rule (TTUR), see Heusel et al., 2017\n",
    "GEN_LR = 0.0001\n",
    "\n",
    "# NOTE: all extracted map PNGs have been saved on a separate virtual disk mapped to '/data' or '/data2' of the virtual machine in use\n",
    "data_dir = pathlib.Path(fpath + '/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Train / Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map images sourced from the DFMA come in a variety of dimensions. In order to create sample images with constant dimensions, as required by tensors, the 100k input samples were run through a python script to randomly crop 10 1024 x 1024 areas per picture. Of those cropped (sub-)images, only the ones which contain structures were retained. This was achieved by filtering out image crops which only contained two or less different colors. With that, the logic mainly filterd out crops which only contained black. This process resulted in 700'000+ (sub-)image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12068 cropped image samples available\n"
     ]
    }
   ],
   "source": [
    "# use pre-processed (cropped) 128 x 128 images\n",
    "data_dir = pathlib.Path(fpath + '/ascii_crops_12/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "print(f'There are {str(len(imgs))} cropped image samples available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random sample input image:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAIAAADZF8uwAAAANElEQVR4nOXMsQnAMAADwSP7D6XNlMKNwU48gL96HiFI0rbtVpLAz2KImU0Cz5rOfJ3dzgt+tkfNlSu6KQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=12x12 at 0x7F0858272198>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show example sample image (cropped to 128x128)\n",
    "print('A random sample input image:')\n",
    "PIL.Image.open(imgs[random.randint(0,len(imgs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12068 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# creating keras datasets for training and validation - refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "dataset_train = tf.keras.preprocessing.image_dataset_from_directory(  fpath+'/ascii_crops_12',\n",
    "                                                                      image_size=IMAGE_SIZE, \n",
    "                                                                      batch_size=BATCH_SIZE, \n",
    "                                                                      #labels=[1.] * len(imgs), # setting all labels to 1.0 (for 'real') as float32\n",
    "                                                                      #label_mode=None, # yields float32 type labels\n",
    "                                                                      seed=1234 #,\n",
    "                                                                   )\n",
    "\n",
    "#drop last batch that contains less samples (to match the constant input shape of the tile data)\n",
    "dataset_train = dataset_train.take(len(dataset_train)-1)\n",
    "# refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = dataset_train.cache().prefetch(buffer_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAINING = 12068 # = 20% of total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Random Sample from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check random images from prepared batches\n",
    "plt.figure(figsize=(5, 5))\n",
    "for images, labels in train_ds.take(1): # take one batch. Here batch_size = 128 examples per batch\n",
    "    for i in range(9): # show first 9 images of batch\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(f'img {i}')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATED Model Definition\n",
    "\n",
    "Generally the following changes have been implemented to the architectures:\n",
    "\n",
    "### RUN09\n",
    "\n",
    "- instead of generating full maps, this model focuses on learning to recreate single tiles which represent the different objects/elements in the game world\n",
    "- implemented secondary input for both critic and generator which introduces the single tiles cropped from the tileset files (12x10) as a 12x10x256 vector (256 different 12x10 tiles) \n",
    "- **NEXT RUN**: \n",
    "    - add a Dense layer or 1x1 CONV between Tiles input and concat layer to allow more flexibility for the model to learn which parts are considered\n",
    "    - increase batch size as much as possible\n",
    "\n",
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discriminator_model():\n",
    "\n",
    "    # DISCRIMINATOR\n",
    "    # set input variables to variable width + height. Will be cropped in preprocessing [CURRENTLY FIXED TO 256x256]\n",
    "    input_dim = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "\n",
    "    # Input\n",
    "    d_input = Input(shape=input_dim, name='Discriminator_Input')\n",
    "    \n",
    "    \n",
    "\n",
    "    # ---- REMOVED FOR 256x256 NETWORK ----------\n",
    "    # Keras-based preprocessing. Alternative: RandomCrop()\n",
    "    # use smart_resizing?\n",
    "    #x = tf.keras.preprocessing.image.smart_resize(d_input, (1024, 1024))\n",
    "    #x = preprocessing.Resizing(width=512, \n",
    "    #                           height=512, \n",
    "    #                           name='Preprocessing_Resize'\n",
    "    #                          )(d_input) # Resize to 512 x 512 images\n",
    "\n",
    "    #we crop the images to 12x10 to match the tile dimensions\n",
    "    x = preprocessing.RandomCrop(height=12, \n",
    "                                width=10, \n",
    "                                name = 'Preprocessing_RandomCrop'\n",
    "                               )(d_input)\n",
    "\n",
    "    x = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale'\n",
    "                               )(x) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    # START TILES INPUT\n",
    "    tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "    \n",
    "    tiles = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale_Tiles'\n",
    "                               )(tiles_input) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    \n",
    "    #tiles = Conv2D(filters=256, kernel_size=(1,1), strides=1)(tiles)\n",
    "    \n",
    "    x = Concatenate()([x, tiles])\n",
    "    # END TILES INPUT\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 0\n",
    "    x = Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = (3,3), \n",
    "            strides = 1,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_0'\n",
    "    )(x)\n",
    "    \n",
    "    # Activation 0 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_0')(x)\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 1\n",
    "    x = Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_1'\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 1\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 1 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_1')(x)\n",
    "\n",
    "    # Dropout 1\n",
    "    x = Dropout(rate = DROPOUT_C)(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Conv2D Layer 2\n",
    "    x = Conv2D(\n",
    "            filters = 256,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            name = 'Discriminator_Conv2D_Layer_3',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 2\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 2 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_3')(x)\n",
    "\n",
    "\n",
    "    # OUTPUT\n",
    "    x = Flatten()(x)\n",
    "    #x = Dropout(DROPOUT_C)(x)\n",
    "    \n",
    "    d_output = Dense(1, \n",
    "                     #activation='sigmoid', \n",
    "                     kernel_initializer = RandomNormal(mean=0, stddev=0.02) # random initialization of weights with normal distribution around 0 with small SD\n",
    "                    )(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Discriminator Model intialization\n",
    "    discriminator = Model([d_input, tiles_input], d_output, name='Discriminator')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Discriminator_Input (InputLayer [(None, 12, 12, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_RandomCrop (Rando (None, 12, 10, 3)    0           Discriminator_Input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Tiles_Input (InputLayer)        [(None, 12, 10, 256) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_Rescale (Rescalin (None, 12, 10, 3)    0           Preprocessing_RandomCrop[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_Rescale_Tiles (Re (None, 12, 10, 256)  0           Tiles_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 12, 10, 259)  0           Preprocessing_Rescale[0][0]      \n",
      "                                                                 Preprocessing_Rescale_Tiles[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_0 (C (None, 12, 10, 64)   149248      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Activation_0 (LeakyReLU)        (None, 12, 10, 64)   0           Discriminator_Conv2D_Layer_0[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_1 (C (None, 6, 5, 128)    73856       Activation_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_1 (LeakyReLU)        (None, 6, 5, 128)    0           Discriminator_Conv2D_Layer_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 6, 5, 128)    0           Activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_3 (C (None, 3, 3, 256)    295168      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Activation_3 (LeakyReLU)        (None, 3, 3, 256)    0           Discriminator_Conv2D_Layer_3[0][0\n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2304)         0           Activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2305        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 520,577\n",
      "Trainable params: 520,577\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = discriminator_model()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    # GENERATOR\n",
    "\n",
    "    # set input variable dimensions. Here we will start out with a vector of length 100 for each sample (sampled from a normal distribution, representing the learned latent space)\n",
    "    input_dim = (LATENT_DIM)\n",
    "\n",
    "    # Input\n",
    "    g_input = Input(shape=input_dim, name='Generator_Input')\n",
    "\n",
    "    # Dense Layer 1\n",
    "    x = Dense(np.prod([3,3,512]), kernel_initializer = RandomNormal(mean=0., stddev=0.02), \n",
    "              use_bias=False)(g_input) # use_bias=False see https://keras.io/examples/generative/wgan_gp/\n",
    "\n",
    "    # Batch Norm Layer 1\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    \n",
    "    # Activation Layer 1\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Reshape into 3D tensor\n",
    "    x = Reshape((3,3,512))(x)\n",
    "\n",
    "    # Upsampling Layer 1 + Conv2D Layer1\n",
    "    x = Conv2DTranspose(filters=512, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=256, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=(3,3), padding='same', strides=(1,1), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    # START TILES\n",
    "    tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "    tiles = preprocessing.Resizing(width=12, height=12)(tiles_input)\n",
    "    \n",
    "    x = Concatenate()([x, tiles])\n",
    "    \n",
    "    # END TILES\n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "    \n",
    "    # reduce output dimensions via 1x1 convolution\n",
    "    x = Conv2D(filters=3, kernel_size=(1,1), padding='same', strides=(1,1),\n",
    "               kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "        \n",
    "    # tanh activation layer to scale values to [-1:1]\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    # Batch Norm Layer 7\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "\n",
    "    # Output - Rescale Values back to [0:255] since the discriminator will automatically rescale back down to [-1:1] as part of the pre-processing pipeline\n",
    "    g_output = (255 / 2) * (x + 1) \n",
    "\n",
    "\n",
    "    # Generator Model initialization\n",
    "    generator = Model([g_input, tiles_input], g_output, name='Generator')\n",
    "    \n",
    "    \n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Generator_Input (InputLayer)    [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4608)         589824      Generator_Input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 4608)         18432       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 4608)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 3, 3, 512)    0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 6, 6, 512)    2359296     reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 6, 6, 512)    1024        conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 6, 6, 512)    0           layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 12, 12, 256)  1179648     leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 12, 12, 256)  512         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 256)  0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Tiles_Input (InputLayer)        [(None, 12, 10, 256) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 12, 12, 128)  294912      leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "resizing (Resizing)             (None, 12, 12, 256)  0           Tiles_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12, 12, 384)  0           conv2d_transpose_2[0][0]         \n",
      "                                                                 resizing[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 12, 12, 384)  768         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 12, 12, 384)  0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 12, 12, 3)    1152        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 12, 12, 3)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 12, 12, 3)]  0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 12, 12, 3)]  0           tf_op_layer_AddV2[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 4,445,568\n",
      "Trainable params: 4,436,352\n",
      "Non-trainable params: 9,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen = generator_model()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tiles\n",
    "\n",
    "We load the 256 split 12x10 tiles into a (12,10,256) numpy array to feed to the network at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiles():\n",
    "    tiles = []\n",
    "\n",
    "    data_dir = pathlib.Path('/data2/input/tiles/800x600/')\n",
    "    imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "    # show example sample image (cropped to 128x128)\n",
    "    PIL.Image.open(imgs[random.randint(0,len(imgs))])\n",
    "\n",
    "    for img in imgs:\n",
    "        tiles.append(np.asarray(PIL.Image.open(img)).astype('uint8'))\n",
    "\n",
    "    #reshape\n",
    "    ds_tiles = np.array(tiles)\n",
    "    ds_tiles = ds_tiles.reshape((1,12,10,256))\n",
    "    ds_tiles = ds_tiles.repeat(repeats=BATCH_SIZE//NUM_GPUS, axis=0)\n",
    "\n",
    "    return ds_tiles\n",
    "\n",
    "\n",
    "    #repeate same 256 12x10 input images as many times as there are batches in the training dataset (N_TRAINING//BATCH_SIZE)\n",
    "    \n",
    "    #print(f'Created numpy array with shape: {ds_tiles.shape}')\n",
    "\n",
    "    #create tf dataset\n",
    "    #tiles_ds = tf.data.Dataset.from_tensor_slices(ds_tiles).batch(BATCH_SIZE)\n",
    "    #train_ds = np.array(train_ds)\n",
    "\n",
    "    #Combine\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices({'Discriminator_Input':train_ds,'Tiles_Input':tiles_ds})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP (Full) Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we compile the models, we need to implement a custom loss function which uses the Wasserstein distance and a gradient penalty term in order to ensure 1 Lipschitz constraints are followed. A WGAN with GP further involves a slightly more complicated training process which trains the critic (discriminator without sigmoid activation function) by feeding three different kinds of images:\n",
    "\n",
    "1. real images (i.e. available samples)\n",
    "2. 'fake' images (i.e. constructed by the generator)\n",
    "3. random interpolations between real and fake images (i.e. random samples interpolated from values between the fake and real images)\n",
    "\n",
    "The full training process of a critic is depicted below (source: Foster, 2019, p. 122):\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"wgan_gp_critic_training.png\"></img>\n",
    "    <i>Computational Graph for one Discriminator Training Epoch. (Source: Foster, 2019, p.122)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below roughly follows the OOP-based framework set by keras see https://keras.io/examples/generative/wgan_gp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope(): \n",
    "\n",
    "    critic = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    ds_tiles = load_tiles()\n",
    "\n",
    "    class WGANGP(keras.Model):\n",
    "        def __init__(\n",
    "            self,\n",
    "            critic,\n",
    "            generator,\n",
    "            latent_dim,\n",
    "            tensorboard_callback,\n",
    "            critic_extra_steps=5,\n",
    "            gp_weight=10.0,\n",
    "            tiles=None\n",
    "        ):\n",
    "            super(WGANGP, self).__init__()\n",
    "            self.critic = critic\n",
    "            self.generator = generator\n",
    "            self.latent_dim = latent_dim\n",
    "            self.tensorboard_callback = tensorboard_callback\n",
    "            self.d_steps = critic_extra_steps\n",
    "            self.gp_weight = gp_weight\n",
    "            self.tiles=tiles\n",
    "            \n",
    "\n",
    "        def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "            super(WGANGP, self).compile()\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.g_optimizer = g_optimizer\n",
    "            self.d_loss_fn = d_loss_fn\n",
    "            self.g_loss_fn = g_loss_fn\n",
    "\n",
    "        def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "            \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "            This loss is calculated on an interpolated image\n",
    "            and added to the discriminator loss.\n",
    "            \"\"\"\n",
    "            # Get the interpolated image\n",
    "            alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "            diff = fake_images - real_images\n",
    "            interpolated = real_images + alpha * diff\n",
    "\n",
    "            with tf.GradientTape() as gp_tape:\n",
    "                gp_tape.watch(interpolated)\n",
    "                # 1. Get the discriminator output for this interpolated image.\n",
    "                pred = self.critic({'Discriminator_Input':interpolated,'Tiles_Input':ds_tiles}, training=True)\n",
    "\n",
    "            # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "            grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "            # 3. Calculate the norm of the gradients.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "            gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "            return gp\n",
    "\n",
    "        def train_step(self, real_images):\n",
    "            #checking whether we handed a tuple of (numpy) data to .fit().\n",
    "            #if not, the data must be a tf.data.Dataset generator that yields batches of datasets (data, labels)\n",
    "            if isinstance(real_images, tuple):\n",
    "                real_images = real_images[0]\n",
    "\n",
    "            # Get the batch size\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "            # For each batch, we are going to perform the\n",
    "            # following steps as laid out in the original paper:\n",
    "            # 1. Train the generator and get the generator loss\n",
    "            # 2. Train the discriminator and get the discriminator loss\n",
    "            # 3. Calculate the gradient penalty\n",
    "            # 4. Multiply this gradient penalty with a constant weight factor = self.discriminator_extra_steps = 5 (default value)\n",
    "            # 5. Add the gradient penalty to the discriminator loss\n",
    "            # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "            # Train the discriminator first. The original paper recommends training\n",
    "            # the discriminator for `x` more steps (typically 5) as compared to generator\n",
    "            \n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.critic)\n",
    "            \n",
    "            for i in range(self.d_steps):\n",
    "                # Get the latent vector\n",
    "                random_latent_vectors = tf.random.normal(\n",
    "                    shape=(batch_size, self.latent_dim)\n",
    "                )\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Generate fake images from the latent vector\n",
    "                    fake_images = self.generator({'Generator_Input':random_latent_vectors,'Tiles_Input':ds_tiles}, training=True)\n",
    "                    # Get the logits for the fake images\n",
    "                    fake_logits = self.critic({'Discriminator_Input':fake_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "                    # Get the logits for the real images\n",
    "                    real_logits = self.critic({'Discriminator_Input':real_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "\n",
    "                    # Calculate the discriminator loss using the fake and real image logits\n",
    "                    d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                    # Calculate the gradient penalty\n",
    "                    gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                    # Add the gradient penalty to the original discriminator loss\n",
    "                    d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "                # Get the gradients w.r.t the discriminator loss\n",
    "                d_gradient = tape.gradient(d_loss, self.critic.trainable_variables)\n",
    "                # Update the weights of the discriminator using the discriminator optimizer\n",
    "                self.d_optimizer.apply_gradients(\n",
    "                    zip(d_gradient, self.critic.trainable_variables)\n",
    "                )\n",
    "\n",
    "            # Train the generator\n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.generator)\n",
    "            \n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images using the generator\n",
    "                generated_images = self.generator({'Generator_Input':random_latent_vectors,'Tiles_Input':ds_tiles}, training=True)\n",
    "                # Get the discriminator logits for fake images\n",
    "                gen_img_logits = self.critic({'Discriminator_Input':generated_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "                # Calculate the generator loss\n",
    "                g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "            # Get the gradients w.r.t the generator loss\n",
    "            gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "            # Update the weights of the generator using the generator optimizer\n",
    "            self.g_optimizer.apply_gradients(\n",
    "                zip(gen_gradient, self.generator.trainable_variables)\n",
    "            )\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "        \n",
    "    class GANMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, num_img=5, latent_dim=128):\n",
    "            self.num_img = num_img\n",
    "            self.latent_dim = latent_dim\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None): #on_epoch_end(self, epoch, logs=None):\n",
    "            '''\n",
    "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "            generated_images = self.model.generator(random_latent_vectors)\n",
    "            #generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "            for i in range(self.num_img):\n",
    "                img = generated_images[i].numpy()\n",
    "                img = keras.preprocessing.image.array_to_img(img)\n",
    "                img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "            '''\n",
    "            \n",
    "            # Sample generator output for num_img images\n",
    "            noise = np.random.normal(0, 1, (self.num_img, self.latent_dim))\n",
    "            gen_imgs = generator.predict({'Generator_Input':noise,'Tiles_Input':ds_tiles[0:5]})\n",
    "            gen_imgs = gen_imgs.astype('uint8')\n",
    "\n",
    "            #!!!NOT NECESSARY ANYMORE AS IMPLEMENTED AS PART OF THE MODEL!!!\n",
    "            #gen_imgs = 0.5 * (gen_imgs + 1)  #scale back to [0:1]\n",
    "            gen_imgs = gen_imgs.reshape((self.num_img, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "            # save n example images\n",
    "            for i in range(self.num_img):\n",
    "                plt.figure(figsize=(5, 5))\n",
    "                plt.imshow(gen_imgs[i])\n",
    "                #plt.title(f'Example Generator Output')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # adjust path based on whether execution is local or on linux VM\n",
    "                if pathlib.Path(f'{out_img_dir}/{model_name}').exists():\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    #mkdir\n",
    "                    os.mkdir(f'{out_img_dir}/{model_name}')\n",
    "                    #save\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                    \n",
    "            # save corresponding model\n",
    "            now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "            \n",
    "            if pathlib.Path(f'{out_model_dir}/{model_name}').exists():\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5')        \n",
    "            else:\n",
    "                #make dir\n",
    "                os.mkdir(f'{out_model_dir}/{model_name}')\n",
    "                #write\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5') \n",
    "        \n",
    "        \n",
    "    # Instantiate the optimizer for both networks\n",
    "    # (learning_rate=0.0002, beta_1=0.5 are recommended) as per Radford et al. 2016 pp. 3-4\n",
    "    generator_optimizer = Adam(\n",
    "        learning_rate=GEN_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "    critic_optimizer = Adam(\n",
    "        learning_rate=CRIT_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def critic_loss(real_img, fake_img):\n",
    "        real_loss = tf.reduce_mean(real_img)\n",
    "        fake_loss = tf.reduce_mean(fake_img)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(fake_img):\n",
    "        return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "    # Instantiate the custome `GANMonitor` Keras callback.\n",
    "    cbk = GANMonitor(num_img=5, latent_dim=LATENT_DIM)\n",
    "    \n",
    "    # Instantiate the tensorboard tf.keras callback\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "    tb_cbk = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = f'{tboard_dir}/{model_name}_{now}', \n",
    "        write_graph = False, \n",
    "        write_images = True,\n",
    "        histogram_freq = 1) \n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGANGP(\n",
    "        critic=critic,\n",
    "        generator=generator,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        tensorboard_callback=tb_cbk,\n",
    "        critic_extra_steps=CRITIC_FACTOR,\n",
    "        gp_weight=GRADIENT_PENALTY_WEIGHT,\n",
    "        tiles=ds_tiles       \n",
    "        \n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "        d_optimizer=critic_optimizer,\n",
    "        g_optimizer=generator_optimizer,\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=critic_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 2/94 [..............................] - ETA: 2:59 - d_loss: 9.3475 - g_loss: -1.1281WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1894s vs `on_train_batch_end` time: 3.7137s). Check your callbacks.\n",
      "94/94 [==============================] - 21s 226ms/step - d_loss: -1125.9937 - g_loss: 6515.2757\n",
      "Epoch 2/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -17742.4763 - g_loss: 161006.8273\n",
      "Epoch 3/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: -25526.8557 - g_loss: 300900.8382\n",
      "Epoch 4/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -27995.2023 - g_loss: 401481.5306\n",
      "Epoch 5/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -30707.0999 - g_loss: 528282.7977\n",
      "Epoch 6/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -30580.4365 - g_loss: 579982.3539\n",
      "Epoch 7/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -30451.8160 - g_loss: 624151.4112\n",
      "Epoch 8/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -31010.1953 - g_loss: 767635.3566\n",
      "Epoch 9/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -28498.3198 - g_loss: 848118.0020\n",
      "Epoch 10/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -28938.6773 - g_loss: 1004259.1401\n",
      "Epoch 11/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: -31003.1378 - g_loss: 1000120.3217\n",
      "Epoch 12/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -27269.7515 - g_loss: 717534.6796\n",
      "Epoch 13/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -27419.1978 - g_loss: 630367.6401\n",
      "Epoch 14/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -27762.0184 - g_loss: 678199.2408\n",
      "Epoch 15/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -26016.4427 - g_loss: 650260.5276\n",
      "Epoch 16/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -25881.8417 - g_loss: 674257.2072\n",
      "Epoch 17/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -23401.3697 - g_loss: 563680.9546\n",
      "Epoch 18/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -24438.4688 - g_loss: 585423.4559\n",
      "Epoch 19/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -23466.6496 - g_loss: 598857.9855\n",
      "Epoch 20/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -6989.2725 - g_loss: 581066.8855\n",
      "Epoch 21/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -5707.7287 - g_loss: 629000.8711\n",
      "Epoch 22/2000\n",
      "94/94 [==============================] - 16s 175ms/step - d_loss: -226.6561 - g_loss: 447987.0934\n",
      "Epoch 23/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -1500.0912 - g_loss: 390445.8707\n",
      "Epoch 24/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -920.7561 - g_loss: 253955.5832\n",
      "Epoch 25/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -438.0357 - g_loss: 219133.1095\n",
      "Epoch 26/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -444.9231 - g_loss: 292884.2666\n",
      "Epoch 27/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -488.1734 - g_loss: 288850.2944\n",
      "Epoch 28/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -310.3805 - g_loss: 264876.0757\n",
      "Epoch 29/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -318.9717 - g_loss: 299024.2965\n",
      "Epoch 30/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 38.1177 - g_loss: 376945.0783\n",
      "Epoch 31/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: -63.7144 - g_loss: 389227.2059\n",
      "Epoch 32/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: -542.4199 - g_loss: 398140.1693\n",
      "Epoch 33/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -12.4065 - g_loss: 401203.2559\n",
      "Epoch 34/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -237.7673 - g_loss: 213033.9910\n",
      "Epoch 35/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -260.2284 - g_loss: 162243.1960\n",
      "Epoch 36/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 40.4417 - g_loss: 178737.4451\n",
      "Epoch 37/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -465.3772 - g_loss: 373054.7493\n",
      "Epoch 38/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -290.9902 - g_loss: 516590.3319\n",
      "Epoch 39/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -656.8442 - g_loss: 438395.5658\n",
      "Epoch 40/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -921.0425 - g_loss: 378031.8322\n",
      "Epoch 41/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 297.6394 - g_loss: 552824.4668\n",
      "Epoch 42/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 131.0071 - g_loss: 633165.6862\n",
      "Epoch 43/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -581.4873 - g_loss: 306792.5120\n",
      "Epoch 44/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 295.3910 - g_loss: 148412.9411\n",
      "Epoch 45/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -82.4943 - g_loss: 140309.8153\n",
      "Epoch 46/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -980.3978 - g_loss: 285120.2877\n",
      "Epoch 47/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 478.3967 - g_loss: 627050.0240\n",
      "Epoch 48/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 760.1755 - g_loss: 751326.8724\n",
      "Epoch 49/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -466.0115 - g_loss: 762798.1645\n",
      "Epoch 50/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 473.4300 - g_loss: 907512.0296\n",
      "Epoch 51/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -2569.1893 - g_loss: 1126545.5382\n",
      "Epoch 52/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1371.0493 - g_loss: 1442194.5105\n",
      "Epoch 53/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -482.2419 - g_loss: 1380749.2697\n",
      "Epoch 54/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -4821.6249 - g_loss: 963384.83953s - d_loss: -7578.36\n",
      "Epoch 55/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 1168.8459 - g_loss: 1011352.0717\n",
      "Epoch 56/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 889.2802 - g_loss: 810779.7270\n",
      "Epoch 57/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -7302.0896 - g_loss: 864261.4421\n",
      "Epoch 58/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 807.7291 - g_loss: 1333572.7191\n",
      "Epoch 59/2000\n",
      "94/94 [==============================] - 16s 175ms/step - d_loss: 2163.7262 - g_loss: 1736998.0605\n",
      "Epoch 60/2000\n",
      "94/94 [==============================] - 16s 174ms/step - d_loss: 2955.4150 - g_loss: 1709351.3211\n",
      "Epoch 61/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -6138.9589 - g_loss: 1459609.4000\n",
      "Epoch 62/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3388.0719 - g_loss: 1676217.8895\n",
      "Epoch 63/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -910.8541 - g_loss: 1671099.2750\n",
      "Epoch 64/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -4961.4365 - g_loss: 2052911.8711\n",
      "Epoch 65/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -2541.9556 - g_loss: 1816083.0645\n",
      "Epoch 66/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2629.1663 - g_loss: 1742221.3289\n",
      "Epoch 67/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -465.6154 - g_loss: 1478385.5066\n",
      "Epoch 68/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3336.1683 - g_loss: 1335942.4500\n",
      "Epoch 69/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 903.1041 - g_loss: 1738639.5000\n",
      "Epoch 70/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -8220.0941 - g_loss: 1897915.6539\n",
      "Epoch 71/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 4950.1085 - g_loss: 2325603.2789\n",
      "Epoch 72/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -2189.1960 - g_loss: 2710565.6632\n",
      "Epoch 73/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -5322.6067 - g_loss: 2337341.1671\n",
      "Epoch 74/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -8440.0201 - g_loss: 1651002.4342\n",
      "Epoch 75/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 4505.6218 - g_loss: 1172385.9480\n",
      "Epoch 76/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -5298.7238 - g_loss: 1279940.9789\n",
      "Epoch 77/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 5013.6422 - g_loss: 1950071.2500\n",
      "Epoch 78/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 1759.7669 - g_loss: 2945512.0053\n",
      "Epoch 79/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 1186.0078 - g_loss: 3460848.9632\n",
      "Epoch 80/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 3169.3118 - g_loss: 3526776.6132\n",
      "Epoch 81/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -3306.2432 - g_loss: 3402560.6184\n",
      "Epoch 82/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -6241.3591 - g_loss: 3139787.5816\n",
      "Epoch 83/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -7303.6928 - g_loss: 2239583.2763\n",
      "Epoch 84/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 7967.2373 - g_loss: 2066417.9342\n",
      "Epoch 85/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -4320.6103 - g_loss: 3180562.0211\n",
      "Epoch 86/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -5138.3742 - g_loss: 3394158.6711\n",
      "Epoch 87/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 4181.4489 - g_loss: 3377727.2395\n",
      "Epoch 88/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -5267.4094 - g_loss: 3521925.6421\n",
      "Epoch 89/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 1971.0612 - g_loss: 3557841.6184\n",
      "Epoch 90/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: -7835.5036 - g_loss: 3617400.1605\n",
      "Epoch 91/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 1546.2671 - g_loss: 3722540.6421\n",
      "Epoch 92/2000\n",
      "94/94 [==============================] - 17s 177ms/step - d_loss: 8049.6981 - g_loss: 4177378.4684\n",
      "Epoch 93/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5280.9364 - g_loss: 4315693.1632\n",
      "Epoch 94/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -7653.6323 - g_loss: 4299957.0263\n",
      "Epoch 95/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 383.4119 - g_loss: 4083857.7289\n",
      "Epoch 96/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 433.6375 - g_loss: 4215379.5184\n",
      "Epoch 97/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 9023.4708 - g_loss: 4220001.4105\n",
      "Epoch 98/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 1796.8979 - g_loss: 4320566.7500\n",
      "Epoch 99/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 1660.3015 - g_loss: 4484698.1053\n",
      "Epoch 100/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -3726.6848 - g_loss: 4644412.8947\n",
      "Epoch 101/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -122.6866 - g_loss: 4433552.6395\n",
      "Epoch 102/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -3905.5915 - g_loss: 4375648.0526\n",
      "Epoch 103/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -6641.8245 - g_loss: 4280354.4263\n",
      "Epoch 104/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -2166.4798 - g_loss: 4563600.5895\n",
      "Epoch 105/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -10305.0638 - g_loss: 4852826.0158\n",
      "Epoch 106/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1682.6908 - g_loss: 4456930.9816\n",
      "Epoch 107/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 1708.7560 - g_loss: 4302607.5711\n",
      "Epoch 108/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 8712.4931 - g_loss: 4378415.3289\n",
      "Epoch 109/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -9060.5748 - g_loss: 4223136.2026\n",
      "Epoch 110/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -17509.7037 - g_loss: 4010805.3342\n",
      "Epoch 111/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -11211.6703 - g_loss: 3878708.6000\n",
      "Epoch 112/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 23325.4224 - g_loss: 3504648.4711\n",
      "Epoch 113/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 10734.4187 - g_loss: 2820860.7132\n",
      "Epoch 114/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 3168.9687 - g_loss: 2578239.1105\n",
      "Epoch 115/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 1403.1311 - g_loss: 2204398.2697\n",
      "Epoch 116/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -13031.4001 - g_loss: 2052220.1579\n",
      "Epoch 117/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -3941.4431 - g_loss: 1683079.7947\n",
      "Epoch 118/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -5575.4535 - g_loss: 2196038.9447\n",
      "Epoch 119/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 2459.7692 - g_loss: 2623191.3289\n",
      "Epoch 120/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 14126.1364 - g_loss: 3252843.3526\n",
      "Epoch 121/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -19903.4178 - g_loss: 3684434.8000\n",
      "Epoch 122/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -5659.6058 - g_loss: 4035821.1237\n",
      "Epoch 123/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 14652.1509 - g_loss: 3545503.0158\n",
      "Epoch 124/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 468.6575 - g_loss: 3200952.4395\n",
      "Epoch 125/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -4860.3642 - g_loss: 2910042.8737\n",
      "Epoch 126/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 2489.1014 - g_loss: 3002878.1079\n",
      "Epoch 127/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 14714.2742 - g_loss: 2128401.5868\n",
      "Epoch 128/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3752.6915 - g_loss: 1477615.5039\n",
      "Epoch 129/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 5136.3088 - g_loss: 1578836.8053\n",
      "Epoch 130/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -1892.7139 - g_loss: 1559206.3566\n",
      "Epoch 131/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 612.6358 - g_loss: 1362103.3829\n",
      "Epoch 132/2000\n",
      "94/94 [==============================] - 17s 178ms/step - d_loss: 2366.3933 - g_loss: 1521919.1316\n",
      "Epoch 133/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -6935.6064 - g_loss: 1760618.9197\n",
      "Epoch 134/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 2228.0093 - g_loss: 1865794.1421\n",
      "Epoch 135/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 8590.2149 - g_loss: 2121246.0855\n",
      "Epoch 136/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -9110.0802 - g_loss: 1353186.7809\n",
      "Epoch 137/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5475.3890 - g_loss: 559801.5735\n",
      "Epoch 138/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 11760.3118 - g_loss: 299913.9319\n",
      "Epoch 139/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -18948.3903 - g_loss: 518934.7534\n",
      "Epoch 140/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -5377.6309 - g_loss: -10691.6556\n",
      "Epoch 141/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 16673.8538 - g_loss: -939482.7954\n",
      "Epoch 142/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -12417.2943 - g_loss: -69432.4763\n",
      "Epoch 143/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -6237.0900 - g_loss: -208107.9376\n",
      "Epoch 144/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 6453.4960 - g_loss: -1171518.2433\n",
      "Epoch 145/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 15404.1549 - g_loss: -1894535.0118\n",
      "Epoch 146/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 438.4080 - g_loss: -1753206.2079\n",
      "Epoch 147/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 4059.2739 - g_loss: -1336535.0151\n",
      "Epoch 148/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -45890.8586 - g_loss: -1750786.3480\n",
      "Epoch 149/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 5149.0658 - g_loss: -2046448.3355\n",
      "Epoch 150/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 25458.9909 - g_loss: -936155.4654\n",
      "Epoch 151/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -11611.4070 - g_loss: -55775.8296\n",
      "Epoch 152/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -5106.7266 - g_loss: 24792.0815\n",
      "Epoch 153/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -42325.5099 - g_loss: 620760.6923\n",
      "Epoch 154/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 18965.6980 - g_loss: 553533.0250\n",
      "Epoch 155/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -7238.3869 - g_loss: 1446094.8553\n",
      "Epoch 156/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -3767.7652 - g_loss: 2041139.1487\n",
      "Epoch 157/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 125.0021 - g_loss: 2524117.2855\n",
      "Epoch 158/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 5955.6585 - g_loss: 2558491.2276\n",
      "Epoch 159/2000\n",
      "94/94 [==============================] - 15s 163ms/step - d_loss: 5989.0127 - g_loss: 1533520.2039\n",
      "Epoch 160/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 972.0617 - g_loss: 687030.2130\n",
      "Epoch 161/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 15281.1227 - g_loss: 446312.5106\n",
      "Epoch 162/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 1133.2364 - g_loss: 234795.1132\n",
      "Epoch 163/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 14263.9970 - g_loss: -210771.0160\n",
      "Epoch 164/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 5832.1242 - g_loss: -1136748.0441\n",
      "Epoch 165/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 23865.4653 - g_loss: -475853.6920\n",
      "Epoch 166/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 26639.6603 - g_loss: -256816.5174\n",
      "Epoch 167/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -15575.6823 - g_loss: -446389.7287\n",
      "Epoch 168/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -1303.1674 - g_loss: -573054.5433\n",
      "Epoch 169/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3589.7901 - g_loss: -285000.5664\n",
      "Epoch 170/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 17372.5647 - g_loss: 378598.6183\n",
      "Epoch 171/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 10176.6577 - g_loss: 457844.0898\n",
      "Epoch 172/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -11026.6780 - g_loss: 566005.8042\n",
      "Epoch 173/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 9201.2968 - g_loss: 358847.2020\n",
      "Epoch 174/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -31614.8122 - g_loss: 110896.3281\n",
      "Epoch 175/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -10940.9915 - g_loss: -383755.4422\n",
      "Epoch 176/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -49811.9831 - g_loss: -100168.1186\n",
      "Epoch 177/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -21007.0236 - g_loss: -557463.0421\n",
      "Epoch 178/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -8176.5017 - g_loss: -439391.2294\n",
      "Epoch 179/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -877.3098 - g_loss: -513999.6637\n",
      "Epoch 180/2000\n",
      "94/94 [==============================] - 17s 181ms/step - d_loss: 588.8314 - g_loss: -1103340.5842\n",
      "Epoch 181/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 18825.0359 - g_loss: -1687573.3487\n",
      "Epoch 182/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 6965.4217 - g_loss: -1491027.2039\n",
      "Epoch 183/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 12603.0033 - g_loss: -1320594.4046\n",
      "Epoch 184/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -8370.3202 - g_loss: -850853.3518\n",
      "Epoch 185/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 1289.3617 - g_loss: -1299759.8605\n",
      "Epoch 186/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 6635.5079 - g_loss: -3106321.5342\n",
      "Epoch 187/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -10018.8610 - g_loss: -3615757.4526\n",
      "Epoch 188/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -20181.0925 - g_loss: -4557547.3421\n",
      "Epoch 189/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -78650.7369 - g_loss: -5864160.3053\n",
      "Epoch 190/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 47196.5449 - g_loss: -5854154.3632\n",
      "Epoch 191/2000\n",
      "94/94 [==============================] - 16s 174ms/step - d_loss: -16633.6396 - g_loss: -4822082.5658\n",
      "Epoch 192/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -20024.4657 - g_loss: -3317135.5684\n",
      "Epoch 193/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -45036.4695 - g_loss: -2243885.9816\n",
      "Epoch 194/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 8640.2601 - g_loss: -1956127.7763\n",
      "Epoch 195/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 10857.8417 - g_loss: -2552448.6921\n",
      "Epoch 196/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 11370.3177 - g_loss: -4012969.8763\n",
      "Epoch 197/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -917.0780 - g_loss: -4075940.7711\n",
      "Epoch 198/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 20618.0852 - g_loss: -4370417.4158\n",
      "Epoch 199/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -26027.1482 - g_loss: -5106102.9000\n",
      "Epoch 200/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -53748.1278 - g_loss: -5074599.2000\n",
      "Epoch 201/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -40978.5868 - g_loss: -5693192.0737\n",
      "Epoch 202/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 7838.8176 - g_loss: -6150588.7947\n",
      "Epoch 203/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 1635.7609 - g_loss: -7307024.5632\n",
      "Epoch 204/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 17687.6444 - g_loss: -7033021.5684\n",
      "Epoch 205/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 68191.5280 - g_loss: -5237121.8789\n",
      "Epoch 206/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -7652.1379 - g_loss: -5901985.3421\n",
      "Epoch 207/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3938.6656 - g_loss: -6403903.2947\n",
      "Epoch 208/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -7158.5896 - g_loss: -5769933.2053\n",
      "Epoch 209/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 30140.1183 - g_loss: -5540632.3947\n",
      "Epoch 210/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -16226.0859 - g_loss: -5016215.0579\n",
      "Epoch 211/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 13593.9058 - g_loss: -4960754.0421\n",
      "Epoch 212/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -16456.4498 - g_loss: -4739862.5974\n",
      "Epoch 213/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -5841.7952 - g_loss: -3944954.8237\n",
      "Epoch 214/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -42202.8382 - g_loss: -2766435.2316\n",
      "Epoch 215/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -11926.2334 - g_loss: -3566974.4526\n",
      "Epoch 216/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -4723.6543 - g_loss: -3447084.7895\n",
      "Epoch 217/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 34496.5534 - g_loss: -2657173.4895\n",
      "Epoch 218/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 1910.5159 - g_loss: -3560714.0974\n",
      "Epoch 219/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 30456.1675 - g_loss: -4651122.2026\n",
      "Epoch 220/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 49302.5283 - g_loss: -4894432.6605\n",
      "Epoch 221/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 54988.7022 - g_loss: -5549644.6368\n",
      "Epoch 222/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 14727.7983 - g_loss: -4803062.5763\n",
      "Epoch 223/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 244.6572 - g_loss: -4218450.6789\n",
      "Epoch 224/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 24615.1636 - g_loss: -4662634.6447\n",
      "Epoch 225/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 19175.7070 - g_loss: -5739567.1789\n",
      "Epoch 226/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 82404.8833 - g_loss: -6564827.67371s - d_loss: 91015.8404 - g_loss: -\n",
      "Epoch 227/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 1044.6154 - g_loss: -5351504.5526\n",
      "Epoch 228/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 2194.1382 - g_loss: -5052613.5947\n",
      "Epoch 229/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -107096.6026 - g_loss: -5030599.1921\n",
      "Epoch 230/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 19806.6175 - g_loss: -5317644.6474\n",
      "Epoch 231/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -13580.5299 - g_loss: -5115730.7737\n",
      "Epoch 232/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 33284.5302 - g_loss: -5199897.0895\n",
      "Epoch 233/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -62570.2121 - g_loss: -5209603.2421\n",
      "Epoch 234/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 41852.9051 - g_loss: -4892807.5605\n",
      "Epoch 235/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 16614.2801 - g_loss: -5283631.7421\n",
      "Epoch 236/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -6867.0253 - g_loss: -6210452.6684\n",
      "Epoch 237/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -29939.7681 - g_loss: -6578981.8474\n",
      "Epoch 238/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -4831.0431 - g_loss: -8476104.9895\n",
      "Epoch 239/2000\n",
      "94/94 [==============================] - 18s 189ms/step - d_loss: -12955.1845 - g_loss: -9888269.2526\n",
      "Epoch 240/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 654.4828 - g_loss: -9888155.4000\n",
      "Epoch 241/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 5870.7317 - g_loss: -9951382.2316\n",
      "Epoch 242/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 3117.6160 - g_loss: -10204137.1789\n",
      "Epoch 243/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 30079.6920 - g_loss: -10286809.6842\n",
      "Epoch 244/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 58343.8743 - g_loss: -11178066.0842\n",
      "Epoch 245/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -60588.8954 - g_loss: -10727958.0000\n",
      "Epoch 246/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -10635.7635 - g_loss: -10853733.6842\n",
      "Epoch 247/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -31500.7405 - g_loss: -10718636.8947\n",
      "Epoch 248/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 36091.9070 - g_loss: -10181463.9789\n",
      "Epoch 249/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 171.3243 - g_loss: -10329470.1368\n",
      "Epoch 250/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 23296.6703 - g_loss: -10014213.2632\n",
      "Epoch 251/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -48996.5284 - g_loss: -10264945.9053\n",
      "Epoch 252/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -39561.7576 - g_loss: -9872218.6211\n",
      "Epoch 253/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -70949.7281 - g_loss: -9502719.4737\n",
      "Epoch 254/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 2059.5790 - g_loss: -8658942.9632\n",
      "Epoch 255/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -57099.3210 - g_loss: -8348451.1684\n",
      "Epoch 256/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -4497.1806 - g_loss: -9177223.0474\n",
      "Epoch 257/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -14584.3629 - g_loss: -10703436.0316\n",
      "Epoch 258/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 27428.3918 - g_loss: -8878905.3895\n",
      "Epoch 259/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 15540.9891 - g_loss: -8759493.1421\n",
      "Epoch 260/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -89140.0959 - g_loss: -7748943.2316\n",
      "Epoch 261/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 4453.6126 - g_loss: -7268347.6684\n",
      "Epoch 262/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -67162.2846 - g_loss: -7302763.2632\n",
      "Epoch 263/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 16985.4857 - g_loss: -7026316.1526\n",
      "Epoch 264/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -40704.2416 - g_loss: -6965571.9895\n",
      "Epoch 265/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 58829.6373 - g_loss: -7455928.3000\n",
      "Epoch 266/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -18380.2682 - g_loss: -7473600.5737\n",
      "Epoch 267/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 7753.0562 - g_loss: -7500918.9842\n",
      "Epoch 268/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -22091.4351 - g_loss: -7490577.2789\n",
      "Epoch 269/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -3992.4361 - g_loss: -8387992.1053\n",
      "Epoch 270/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 45563.0498 - g_loss: -8410441.8579\n",
      "Epoch 271/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -16243.7295 - g_loss: -8157749.6263\n",
      "Epoch 272/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -15476.9359 - g_loss: -8579813.0053\n",
      "Epoch 273/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 3622.4924 - g_loss: -8914088.3789\n",
      "Epoch 274/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -34485.6387 - g_loss: -9811306.1368\n",
      "Epoch 275/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -15372.3672 - g_loss: -10001546.6105\n",
      "Epoch 276/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -12073.6739 - g_loss: -10369012.3684\n",
      "Epoch 277/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 23864.9280 - g_loss: -10553462.2947\n",
      "Epoch 278/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 30668.6109 - g_loss: -11142966.3263\n",
      "Epoch 279/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 30071.1751 - g_loss: -10430909.4947\n",
      "Epoch 280/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -65403.5386 - g_loss: -10138358.8105\n",
      "Epoch 281/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 21980.1784 - g_loss: -10968389.8105s - d_loss: 20410.3319 - g_loss: -1091838\n",
      "Epoch 282/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -38782.4222 - g_loss: -13198676.8737\n",
      "Epoch 283/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 54568.2047 - g_loss: -12895873.2000\n",
      "Epoch 284/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 15712.2118 - g_loss: -13107567.8000\n",
      "Epoch 285/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 16199.6620 - g_loss: -12072465.2632\n",
      "Epoch 286/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 53351.9398 - g_loss: -11352398.0737\n",
      "Epoch 287/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 56602.2627 - g_loss: -11873648.3474\n",
      "Epoch 288/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 52343.7491 - g_loss: -12546328.3579\n",
      "Epoch 289/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 27955.2507 - g_loss: -12673894.2421\n",
      "Epoch 290/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -33775.0187 - g_loss: -12474440.2842\n",
      "Epoch 291/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 37851.7792 - g_loss: -12539402.5579\n",
      "Epoch 292/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 54390.9668 - g_loss: -12854969.4737\n",
      "Epoch 293/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 70727.3316 - g_loss: -13158791.6737\n",
      "Epoch 294/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 109976.8301 - g_loss: -14037125.4526\n",
      "Epoch 295/2000\n",
      "94/94 [==============================] - 15s 162ms/step - d_loss: -26237.7707 - g_loss: -13575954.8632\n",
      "Epoch 296/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 630.8344 - g_loss: -13552465.9789\n",
      "Epoch 297/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 36100.5899 - g_loss: -13189959.9263\n",
      "Epoch 298/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 116908.9935 - g_loss: -14456588.0632\n",
      "Epoch 299/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -35857.8105 - g_loss: -15236809.1684\n",
      "Epoch 300/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 5189.3684 - g_loss: -15162602.1789\n",
      "Epoch 301/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -126389.2310 - g_loss: -17148342.6421\n",
      "Epoch 302/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 44694.3615 - g_loss: -16624731.3474\n",
      "Epoch 303/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -4511.6066 - g_loss: -16370369.7368\n",
      "Epoch 304/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 18751.0387 - g_loss: -16862855.2421\n",
      "Epoch 305/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 27849.2857 - g_loss: -19639323.9158\n",
      "Epoch 306/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 157401.0948 - g_loss: -19929751.8947\n",
      "Epoch 307/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 23304.7238 - g_loss: -19422616.5053\n",
      "Epoch 308/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 55827.4409 - g_loss: -19234185.9789\n",
      "Epoch 309/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 643.3177 - g_loss: -18474100.3789\n",
      "Epoch 310/2000\n",
      "94/94 [==============================] - 18s 193ms/step - d_loss: 113033.5576 - g_loss: -19069543.5158\n",
      "Epoch 311/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -9268.9037 - g_loss: -19686138.5895\n",
      "Epoch 312/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 12316.6360 - g_loss: -20196416.0000\n",
      "Epoch 313/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -79798.1765 - g_loss: -21233230.9684\n",
      "Epoch 314/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 55354.3262 - g_loss: -24312155.5158\n",
      "Epoch 315/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -44067.7916 - g_loss: -23895384.2947\n",
      "Epoch 316/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -49808.4224 - g_loss: -26392407.1158\n",
      "Epoch 317/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 51553.6501 - g_loss: -26511965.9789\n",
      "Epoch 318/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 61604.1626 - g_loss: -25416447.7684\n",
      "Epoch 319/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -75474.5026 - g_loss: -25682545.3263\n",
      "Epoch 320/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 65696.7627 - g_loss: -24021210.4000\n",
      "Epoch 321/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -38814.0524 - g_loss: -23602706.5053\n",
      "Epoch 322/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 71329.8848 - g_loss: -23618666.0000\n",
      "Epoch 323/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 75024.7801 - g_loss: -23524514.6316\n",
      "Epoch 324/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -23677.8700 - g_loss: -24404723.9368\n",
      "Epoch 325/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -62026.9063 - g_loss: -24560076.6526\n",
      "Epoch 326/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -60263.8166 - g_loss: -24979985.4105\n",
      "Epoch 327/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 14463.4387 - g_loss: -23445582.3368\n",
      "Epoch 328/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -41757.4602 - g_loss: -23080573.4526\n",
      "Epoch 329/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 59189.4089 - g_loss: -22342628.5684\n",
      "Epoch 330/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -29359.7664 - g_loss: -23341427.3053\n",
      "Epoch 331/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -233696.8934 - g_loss: -22633449.2632\n",
      "Epoch 332/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 46425.5499 - g_loss: -21558251.9789\n",
      "Epoch 333/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2477.2826 - g_loss: -21338964.5895\n",
      "Epoch 334/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -79036.1830 - g_loss: -20616792.1263\n",
      "Epoch 335/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -153121.4988 - g_loss: -21077111.1368\n",
      "Epoch 336/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 114941.8289 - g_loss: -21694100.3158\n",
      "Epoch 337/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 58394.1104 - g_loss: -20800765.2632\n",
      "Epoch 338/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 96289.8344 - g_loss: -19491869.4632\n",
      "Epoch 339/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 124266.2264 - g_loss: -16241419.4000\n",
      "Epoch 340/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -69541.3688 - g_loss: -14217727.9053\n",
      "Epoch 341/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 35670.5531 - g_loss: -15239382.8316\n",
      "Epoch 342/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 10265.9289 - g_loss: -15142162.0737\n",
      "Epoch 343/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -30346.0639 - g_loss: -15438482.1895\n",
      "Epoch 344/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 45093.0879 - g_loss: -13073759.1684\n",
      "Epoch 345/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 21733.8274 - g_loss: -13881826.3368\n",
      "Epoch 346/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -147851.5030 - g_loss: -11767397.9368\n",
      "Epoch 347/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -85854.2478 - g_loss: -12264517.1579\n",
      "Epoch 348/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 42396.9571 - g_loss: -12701661.7053\n",
      "Epoch 349/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -12607.5125 - g_loss: -13776551.9895\n",
      "Epoch 350/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -134467.4814 - g_loss: -14943605.3053\n",
      "Epoch 351/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 31230.0141 - g_loss: -14819752.0842\n",
      "Epoch 352/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -60981.5498 - g_loss: -13759930.5368\n",
      "Epoch 353/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 28687.2737 - g_loss: -13822807.1158\n",
      "Epoch 354/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -124001.0467 - g_loss: -13491606.5579\n",
      "Epoch 355/2000\n",
      "94/94 [==============================] - 16s 174ms/step - d_loss: 131508.0803 - g_loss: -13665082.4105\n",
      "Epoch 356/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 79301.0992 - g_loss: -12455869.5263\n",
      "Epoch 357/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 78910.8835 - g_loss: -12417739.6947\n",
      "Epoch 358/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -3220.5114 - g_loss: -11611819.1053\n",
      "Epoch 359/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 45257.5108 - g_loss: -12497990.9789\n",
      "Epoch 360/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 109331.3470 - g_loss: -13367894.4316\n",
      "Epoch 361/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 57666.8921 - g_loss: -13025384.3579\n",
      "Epoch 362/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -50859.5066 - g_loss: -14735064.5579\n",
      "Epoch 363/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 56522.9186 - g_loss: -13502514.5684\n",
      "Epoch 364/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: 137895.2575 - g_loss: -13214441.7474\n",
      "Epoch 365/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 27319.8561 - g_loss: -13609306.7474\n",
      "Epoch 366/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -66147.8493 - g_loss: -15044027.5263\n",
      "Epoch 367/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -66642.3285 - g_loss: -14889439.9579\n",
      "Epoch 368/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 23633.2730 - g_loss: -14383803.1684\n",
      "Epoch 369/2000\n",
      "94/94 [==============================] - 16s 174ms/step - d_loss: -75555.8909 - g_loss: -12881946.5895\n",
      "Epoch 370/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 104833.8444 - g_loss: -13220894.0632\n",
      "Epoch 371/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -13298.6788 - g_loss: -13247313.0316\n",
      "Epoch 372/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -82892.1624 - g_loss: -13383531.8421\n",
      "Epoch 373/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 50702.6391 - g_loss: -13011399.6526\n",
      "Epoch 374/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -44904.3985 - g_loss: -11210858.9579\n",
      "Epoch 375/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 43905.2347 - g_loss: -12066934.0000\n",
      "Epoch 376/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 80994.3334 - g_loss: -11643758.1579\n",
      "Epoch 377/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 34539.9399 - g_loss: -11037445.2737\n",
      "Epoch 378/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 80109.5379 - g_loss: -9924216.6526\n",
      "Epoch 379/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 7112.3836 - g_loss: -9921005.0105\n",
      "Epoch 380/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 17244.5060 - g_loss: -10043101.1895\n",
      "Epoch 381/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 8104.8443 - g_loss: -8751657.7947\n",
      "Epoch 382/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 150275.2496 - g_loss: -8045958.3211\n",
      "Epoch 383/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -156121.2068 - g_loss: -7869390.0263\n",
      "Epoch 384/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -27625.2218 - g_loss: -9059483.2000\n",
      "Epoch 385/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -10630.5669 - g_loss: -10244221.4842\n",
      "Epoch 386/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -41700.3784 - g_loss: -10528792.3789\n",
      "Epoch 387/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 93787.3963 - g_loss: -8651809.8632\n",
      "Epoch 388/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 70657.1741 - g_loss: -6511283.1421\n",
      "Epoch 389/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -11918.3067 - g_loss: -7573868.7579\n",
      "Epoch 390/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -107371.6999 - g_loss: -7434974.1211\n",
      "Epoch 391/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -152274.4537 - g_loss: -7176960.5737\n",
      "Epoch 392/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 111094.4437 - g_loss: -8342914.2053\n",
      "Epoch 393/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 29500.4455 - g_loss: -9255991.9105\n",
      "Epoch 394/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 66660.5901 - g_loss: -8908471.4842\n",
      "Epoch 395/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 170943.3105 - g_loss: -7324113.6316\n",
      "Epoch 396/2000\n",
      "94/94 [==============================] - 19s 197ms/step - d_loss: 36095.6575 - g_loss: -6471038.8579\n",
      "Epoch 397/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 49031.2470 - g_loss: -4387531.6974\n",
      "Epoch 398/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 77522.0081 - g_loss: -3981714.9658\n",
      "Epoch 399/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 57157.4049 - g_loss: -3810408.4895\n",
      "Epoch 400/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 33784.6611 - g_loss: -4092148.6763\n",
      "Epoch 401/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 71540.8820 - g_loss: -4693041.2211\n",
      "Epoch 402/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -16541.1026 - g_loss: -5435772.8474\n",
      "Epoch 403/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -51085.7183 - g_loss: -6770316.6632\n",
      "Epoch 404/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -27783.6183 - g_loss: -6795734.0947\n",
      "Epoch 405/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -38313.0675 - g_loss: -7225600.5000\n",
      "Epoch 406/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -2058.1500 - g_loss: -7691120.0842\n",
      "Epoch 407/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 7115.6815 - g_loss: -7748866.1789\n",
      "Epoch 408/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 8346.9131 - g_loss: -6715076.6368\n",
      "Epoch 409/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 8342.9090 - g_loss: -7205883.6895\n",
      "Epoch 410/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 34691.8559 - g_loss: -6754139.8263\n",
      "Epoch 411/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -160753.3792 - g_loss: -7515154.2842\n",
      "Epoch 412/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 28496.0140 - g_loss: -7233899.7316\n",
      "Epoch 413/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -84173.6224 - g_loss: -6937268.7842\n",
      "Epoch 414/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -130212.0260 - g_loss: -8037338.7158\n",
      "Epoch 415/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 58686.2372 - g_loss: -8171447.8105\n",
      "Epoch 416/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 9977.2394 - g_loss: -8357426.0211\n",
      "Epoch 417/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -4946.8529 - g_loss: -9600721.5947\n",
      "Epoch 418/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -46624.9053 - g_loss: -9500323.0526\n",
      "Epoch 419/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -45286.9039 - g_loss: -9194416.2737\n",
      "Epoch 420/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 61778.2275 - g_loss: -8397957.0105\n",
      "Epoch 421/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -23905.3069 - g_loss: -7826475.9789\n",
      "Epoch 422/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -38811.6282 - g_loss: -7496806.5526\n",
      "Epoch 423/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 51031.8464 - g_loss: -5968299.6789\n",
      "Epoch 424/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -23112.9615 - g_loss: -5855814.5684\n",
      "Epoch 425/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 51770.3703 - g_loss: -6294765.8000\n",
      "Epoch 426/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -80984.4022 - g_loss: -7523017.5316\n",
      "Epoch 427/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 8541.6092 - g_loss: -7745258.6368\n",
      "Epoch 428/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -30049.1407 - g_loss: -8080118.0105\n",
      "Epoch 429/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -18872.9041 - g_loss: -8161895.4789\n",
      "Epoch 430/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: -14168.8438 - g_loss: -8320231.1421\n",
      "Epoch 431/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -70064.8723 - g_loss: -7752735.2105\n",
      "Epoch 432/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -34586.0949 - g_loss: -7442288.2474\n",
      "Epoch 433/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -30460.1111 - g_loss: -6308062.5895\n",
      "Epoch 434/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -46586.5809 - g_loss: -5504699.0868\n",
      "Epoch 435/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 54551.7222 - g_loss: -5031305.4316\n",
      "Epoch 436/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 25925.9586 - g_loss: -4791537.7053\n",
      "Epoch 437/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -28748.6948 - g_loss: -3762511.7079\n",
      "Epoch 438/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -52494.5151 - g_loss: -3195499.1605\n",
      "Epoch 439/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: -61453.3003 - g_loss: -3032937.3592\n",
      "Epoch 440/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 157475.1163 - g_loss: -1824501.0273\n",
      "Epoch 441/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -14885.5506 - g_loss: -1688166.0748\n",
      "Epoch 442/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 27164.1504 - g_loss: -718344.0884\n",
      "Epoch 443/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -22314.2853 - g_loss: -839195.4093\n",
      "Epoch 444/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 18431.8390 - g_loss: -916975.7873\n",
      "Epoch 445/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -30914.8073 - g_loss: -115619.9019\n",
      "Epoch 446/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 26794.5772 - g_loss: -116263.0486\n",
      "Epoch 447/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -61527.2208 - g_loss: -669771.7262\n",
      "Epoch 448/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: -127418.4229 - g_loss: -499132.3975\n",
      "Epoch 449/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -40374.9023 - g_loss: 572766.1935\n",
      "Epoch 450/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -58563.8532 - g_loss: 1893975.8118\n",
      "Epoch 451/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -30145.6180 - g_loss: 1903946.3737\n",
      "Epoch 452/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -31640.2240 - g_loss: 3005585.6184\n",
      "Epoch 453/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 32177.1994 - g_loss: 3275499.8395\n",
      "Epoch 454/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -78957.5971 - g_loss: 2624974.8316\n",
      "Epoch 455/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 35156.2295 - g_loss: 3246558.5329\n",
      "Epoch 456/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 69977.9120 - g_loss: 3001182.2289\n",
      "Epoch 457/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 40906.2674 - g_loss: 1599545.1355\n",
      "Epoch 458/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -101450.0730 - g_loss: 3192628.3947\n",
      "Epoch 459/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 3939.6109 - g_loss: 4228975.5000\n",
      "Epoch 460/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 101793.2138 - g_loss: 3995213.9684\n",
      "Epoch 461/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 1459.6146 - g_loss: 4111184.4526\n",
      "Epoch 462/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -36128.2907 - g_loss: 5109171.3789\n",
      "Epoch 463/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 50255.5429 - g_loss: 5933396.7421\n",
      "Epoch 464/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 44253.5048 - g_loss: 6520708.1368\n",
      "Epoch 465/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -42106.4105 - g_loss: 5594297.2263\n",
      "Epoch 466/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 7270.5013 - g_loss: 6343632.2842\n",
      "Epoch 467/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 66730.8862 - g_loss: 6249744.0000\n",
      "Epoch 468/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 20033.8776 - g_loss: 6597758.3421\n",
      "Epoch 469/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 16576.9339 - g_loss: 6112649.5526\n",
      "Epoch 470/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -4217.3152 - g_loss: 5502270.8105\n",
      "Epoch 471/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -9998.2448 - g_loss: 4881496.4316\n",
      "Epoch 472/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -3198.2810 - g_loss: 4797837.4737\n",
      "Epoch 473/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 32086.6876 - g_loss: 4930798.9158\n",
      "Epoch 474/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -26226.2032 - g_loss: 4088936.2553\n",
      "Epoch 475/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 19979.8948 - g_loss: 3236917.1368\n",
      "Epoch 476/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 8270.2818 - g_loss: 2953775.4158\n",
      "Epoch 477/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -29416.5985 - g_loss: 3059501.8474\n",
      "Epoch 478/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -8356.4812 - g_loss: 3096526.4500\n",
      "Epoch 479/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 53764.7593 - g_loss: 2508533.3829\n",
      "Epoch 480/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 30364.5432 - g_loss: 2177744.9447\n",
      "Epoch 481/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -27395.8067 - g_loss: 1314999.5401\n",
      "Epoch 482/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -15771.9957 - g_loss: 1373576.5998\n",
      "Epoch 483/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -28795.8932 - g_loss: 1240186.1579\n",
      "Epoch 484/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -25752.0099 - g_loss: 1272989.6862\n",
      "Epoch 485/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 22774.2235 - g_loss: 1642446.4816\n",
      "Epoch 486/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -28090.6648 - g_loss: 1597912.3526\n",
      "Epoch 487/2000\n",
      "94/94 [==============================] - 15s 163ms/step - d_loss: 17168.7216 - g_loss: 1024298.4708\n",
      "Epoch 488/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 56982.7952 - g_loss: 1450703.4480\n",
      "Epoch 489/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 49660.4998 - g_loss: 1393616.3888\n",
      "Epoch 490/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 1510.1807 - g_loss: 523135.1873\n",
      "Epoch 491/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 18052.9109 - g_loss: 246597.8545\n",
      "Epoch 492/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 1841.2987 - g_loss: 907932.0657\n",
      "Epoch 493/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 68761.4657 - g_loss: 1044663.4008\n",
      "Epoch 494/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: 114193.1152 - g_loss: 1426342.7641\n",
      "Epoch 495/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 47443.1139 - g_loss: 2430668.6408\n",
      "Epoch 496/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 7472.6010 - g_loss: 2549748.1829\n",
      "Epoch 497/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -15537.4053 - g_loss: 2731893.1842\n",
      "Epoch 498/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -71523.3540 - g_loss: 3173273.6737\n",
      "Epoch 499/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -35433.9262 - g_loss: 3011423.7895\n",
      "Epoch 500/2000\n",
      "94/94 [==============================] - 19s 202ms/step - d_loss: -41501.2665 - g_loss: 2584350.9539\n",
      "Epoch 501/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -19395.4176 - g_loss: 2203594.5118\n",
      "Epoch 502/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -53167.1020 - g_loss: 2612398.7684\n",
      "Epoch 503/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -2595.8961 - g_loss: 3025366.2224\n",
      "Epoch 504/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -33889.1458 - g_loss: 4471488.2579\n",
      "Epoch 505/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -14274.1890 - g_loss: 4870188.9132\n",
      "Epoch 506/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -6528.3955 - g_loss: 3382257.7408\n",
      "Epoch 507/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -54248.0300 - g_loss: 3002671.2684\n",
      "Epoch 508/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 92319.9220 - g_loss: 2951824.6303\n",
      "Epoch 509/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 14247.4309 - g_loss: 2885647.7382\n",
      "Epoch 510/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 61024.0773 - g_loss: 3282929.1447\n",
      "Epoch 511/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -103950.3352 - g_loss: 2055553.6934\n",
      "Epoch 512/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 61939.7343 - g_loss: 1829406.1447\n",
      "Epoch 513/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 9541.8832 - g_loss: 2536916.6500\n",
      "Epoch 514/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -7779.1092 - g_loss: 2469996.8276\n",
      "Epoch 515/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -24127.9745 - g_loss: 2168655.8250\n",
      "Epoch 516/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -8181.7006 - g_loss: 1206857.3576\n",
      "Epoch 517/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 52991.2697 - g_loss: 1221870.9770\n",
      "Epoch 518/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 67876.0722 - g_loss: 259427.2598\n",
      "Epoch 519/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -18094.1973 - g_loss: 1092470.5789\n",
      "Epoch 520/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -51055.7533 - g_loss: 1302087.7576\n",
      "Epoch 521/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -5921.0071 - g_loss: 562188.4899\n",
      "Epoch 522/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -22118.4754 - g_loss: -396000.9748\n",
      "Epoch 523/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 43962.6498 - g_loss: -598340.0982\n",
      "Epoch 524/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 21571.3957 - g_loss: -619097.7692\n",
      "Epoch 525/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 9218.5069 - g_loss: -318008.0120\n",
      "Epoch 526/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -10832.5853 - g_loss: -560787.4440\n",
      "Epoch 527/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 7147.3468 - g_loss: -999980.4931\n",
      "Epoch 528/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -9523.6579 - g_loss: -1208963.9743\n",
      "Epoch 529/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 57230.4314 - g_loss: -1219256.9184\n",
      "Epoch 530/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -12690.9256 - g_loss: -854232.3510\n",
      "Epoch 531/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 40175.7739 - g_loss: -610093.1123\n",
      "Epoch 532/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 18763.3835 - g_loss: -542862.4939\n",
      "Epoch 533/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5835.2693 - g_loss: -23262.3852\n",
      "Epoch 534/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 38041.9057 - g_loss: 347218.9598\n",
      "Epoch 535/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -12322.0030 - g_loss: 544088.1813\n",
      "Epoch 536/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 20185.2138 - g_loss: 125351.8378\n",
      "Epoch 537/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -8116.8812 - g_loss: 314275.9039\n",
      "Epoch 538/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 1680.0061 - g_loss: 361483.9778\n",
      "Epoch 539/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -2244.4455 - g_loss: -389246.8536\n",
      "Epoch 540/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 38560.8062 - g_loss: -302917.0247\n",
      "Epoch 541/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -19902.0667 - g_loss: -295816.7838\n",
      "Epoch 542/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 20519.8342 - g_loss: -101994.9018\n",
      "Epoch 543/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -4085.1946 - g_loss: 375211.7098\n",
      "Epoch 544/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -25061.9661 - g_loss: 1099987.3313\n",
      "Epoch 545/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 8906.4131 - g_loss: 1505664.0355\n",
      "Epoch 546/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 25824.6113 - g_loss: 1834037.7145\n",
      "Epoch 547/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -14725.7520 - g_loss: 1476547.4553\n",
      "Epoch 548/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 85.1758 - g_loss: 1336297.6461\n",
      "Epoch 549/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -12117.1200 - g_loss: 1417624.4276\n",
      "Epoch 550/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 13344.0167 - g_loss: 1390864.3329\n",
      "Epoch 551/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 116.6422 - g_loss: 1354599.1224\n",
      "Epoch 552/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 10428.8410 - g_loss: 1650081.9461\n",
      "Epoch 553/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2859.5375 - g_loss: 1520539.8184\n",
      "Epoch 554/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: 2603.4952 - g_loss: 1181026.3072\n",
      "Epoch 555/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -23930.7058 - g_loss: 1291412.7303\n",
      "Epoch 556/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -1713.3475 - g_loss: 1457848.6566\n",
      "Epoch 557/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 5719.4734 - g_loss: 1738624.8316\n",
      "Epoch 558/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 12508.6433 - g_loss: 1783643.6250\n",
      "Epoch 559/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -4896.2934 - g_loss: 2025982.8737\n",
      "Epoch 560/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 3325.1148 - g_loss: 1871339.0645\n",
      "Epoch 561/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -11302.5603 - g_loss: 1675542.0303\n",
      "Epoch 562/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 1952.6181 - g_loss: 1493453.4750\n",
      "Epoch 563/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -10953.5695 - g_loss: 1502444.4947\n",
      "Epoch 564/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -8172.1824 - g_loss: 1564367.1592\n",
      "Epoch 565/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -7251.5146 - g_loss: 1779334.8908\n",
      "Epoch 566/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -7757.2231 - g_loss: 1313580.8217\n",
      "Epoch 567/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 8041.2519 - g_loss: 1393465.5724\n",
      "Epoch 568/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -4487.1437 - g_loss: 1404339.2750\n",
      "Epoch 569/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -8066.9783 - g_loss: 1482455.4250\n",
      "Epoch 570/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 9316.9681 - g_loss: 1629741.5158\n",
      "Epoch 571/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 6135.9290 - g_loss: 1612580.1605\n",
      "Epoch 572/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3957.5156 - g_loss: 1745089.4000\n",
      "Epoch 573/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 762.2564 - g_loss: 1898949.2592\n",
      "Epoch 574/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -4095.5128 - g_loss: 2191464.8776\n",
      "Epoch 575/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 9660.1365 - g_loss: 2398568.7895\n",
      "Epoch 576/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 1914.1263 - g_loss: 2216481.8803\n",
      "Epoch 577/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -5884.9645 - g_loss: 2350259.4013\n",
      "Epoch 578/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -615.5487 - g_loss: 2108980.7763\n",
      "Epoch 579/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -837.2914 - g_loss: 2048104.0776\n",
      "Epoch 580/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -6247.8449 - g_loss: 1991102.9039\n",
      "Epoch 581/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 6396.0251 - g_loss: 2130937.0868\n",
      "Epoch 582/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2657.6875 - g_loss: 2001215.2803\n",
      "Epoch 583/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 326.6969 - g_loss: 2170518.9645\n",
      "Epoch 584/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 6727.3330 - g_loss: 2026167.2092\n",
      "Epoch 585/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 1247.3825 - g_loss: 1715638.2895\n",
      "Epoch 586/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 2463.3748 - g_loss: 1520724.8105\n",
      "Epoch 587/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 5670.5205 - g_loss: 1719676.7579\n",
      "Epoch 588/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1584.0496 - g_loss: 1871260.2566\n",
      "Epoch 589/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 6067.8385 - g_loss: 1812664.0276\n",
      "Epoch 590/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -13074.3113 - g_loss: 2068747.4908\n",
      "Epoch 591/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 6863.0713 - g_loss: 2064967.6316\n",
      "Epoch 592/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -10462.7630 - g_loss: 2004869.4342\n",
      "Epoch 593/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 10526.1086 - g_loss: 2005091.3816\n",
      "Epoch 594/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 13619.2487 - g_loss: 2405552.1921\n",
      "Epoch 595/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -1918.7754 - g_loss: 2478105.0237\n",
      "Epoch 596/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -9735.3565 - g_loss: 2740909.4526\n",
      "Epoch 597/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -1492.2975 - g_loss: 2487598.6053\n",
      "Epoch 598/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -17139.2127 - g_loss: 2665874.3368\n",
      "Epoch 599/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 5846.5538 - g_loss: 2435353.9145\n",
      "Epoch 600/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 17860.0046 - g_loss: 2187770.5132\n",
      "Epoch 601/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 7448.7659 - g_loss: 2043403.6724\n",
      "Epoch 602/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 6636.8065 - g_loss: 1896931.1368\n",
      "Epoch 603/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 4770.3775 - g_loss: 1829327.1697\n",
      "Epoch 604/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 2224.0704 - g_loss: 2101730.5197\n",
      "Epoch 605/2000\n",
      "94/94 [==============================] - 15s 163ms/step - d_loss: 2191.6519 - g_loss: 2182646.7276\n",
      "Epoch 606/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 617.2095 - g_loss: 1994731.3342\n",
      "Epoch 607/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 950.2076 - g_loss: 2015888.2132\n",
      "Epoch 608/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 8971.1604 - g_loss: 1836744.2263\n",
      "Epoch 609/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -4529.4656 - g_loss: 2161872.8803\n",
      "Epoch 610/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -2252.1476 - g_loss: 2320061.0382\n",
      "Epoch 611/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -14817.1842 - g_loss: 2639711.6816\n",
      "Epoch 612/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5450.3235 - g_loss: 2455818.8237\n",
      "Epoch 613/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -16956.4271 - g_loss: 2855423.6895\n",
      "Epoch 614/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -3995.6815 - g_loss: 3240731.5316\n",
      "Epoch 615/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 5141.7003 - g_loss: 3271236.5316\n",
      "Epoch 616/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -23202.1097 - g_loss: 3949123.0868\n",
      "Epoch 617/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 7466.2796 - g_loss: 3810811.9553\n",
      "Epoch 618/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 23012.2383 - g_loss: 3257853.1974\n",
      "Epoch 619/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 32978.7078 - g_loss: 3395817.3737\n",
      "Epoch 620/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -2200.4189 - g_loss: 3467833.2000\n",
      "Epoch 621/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 10985.7015 - g_loss: 4007320.5421\n",
      "Epoch 622/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -36439.3497 - g_loss: 4136001.3263\n",
      "Epoch 623/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -8880.4122 - g_loss: 4401879.5184\n",
      "Epoch 624/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 10955.6818 - g_loss: 4386641.1763\n",
      "Epoch 625/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -8451.2318 - g_loss: 4891954.2105\n",
      "Epoch 626/2000\n",
      "94/94 [==============================] - 20s 209ms/step - d_loss: 10990.7823 - g_loss: 5324069.9263\n",
      "Epoch 627/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -260.2352 - g_loss: 5958308.5579\n",
      "Epoch 628/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -4831.3573 - g_loss: 5999309.9737\n",
      "Epoch 629/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -1379.6592 - g_loss: 5681567.7684\n",
      "Epoch 630/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 10683.1218 - g_loss: 5511842.6947\n",
      "Epoch 631/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 19268.2056 - g_loss: 5048985.8684\n",
      "Epoch 632/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -28899.9748 - g_loss: 4741708.5105\n",
      "Epoch 633/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -30472.7949 - g_loss: 4558112.0553\n",
      "Epoch 634/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 2395.7965 - g_loss: 4690155.9842\n",
      "Epoch 635/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -16929.4201 - g_loss: 4982929.0316\n",
      "Epoch 636/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -8513.3182 - g_loss: 4965325.3895\n",
      "Epoch 637/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 8272.2652 - g_loss: 5068578.8632\n",
      "Epoch 638/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 13781.2293 - g_loss: 4858851.8368\n",
      "Epoch 639/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 18719.4131 - g_loss: 4812690.6500\n",
      "Epoch 640/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 49095.3875 - g_loss: 4874622.4895\n",
      "Epoch 641/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -23404.3498 - g_loss: 5615005.37376s - \n",
      "Epoch 642/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 44125.4286 - g_loss: 5623297.2421\n",
      "Epoch 643/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 25638.4904 - g_loss: 5279275.6368\n",
      "Epoch 644/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 20397.6362 - g_loss: 5336355.8684\n",
      "Epoch 645/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -44596.3647 - g_loss: 5918747.2316\n",
      "Epoch 646/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 47624.0719 - g_loss: 5940808.4053\n",
      "Epoch 647/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -33429.1658 - g_loss: 6373838.3474\n",
      "Epoch 648/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -48980.3833 - g_loss: 7422582.8579\n",
      "Epoch 649/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 24075.1432 - g_loss: 8423834.7895\n",
      "Epoch 650/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -50282.2083 - g_loss: 8717993.7105\n",
      "Epoch 651/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 12005.4957 - g_loss: 8943684.9474\n",
      "Epoch 652/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 24637.0397 - g_loss: 8249805.7053\n",
      "Epoch 653/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -18411.4155 - g_loss: 7798248.5684\n",
      "Epoch 654/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 27872.3061 - g_loss: 7964894.2842\n",
      "Epoch 655/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -12612.5686 - g_loss: 7326312.3737\n",
      "Epoch 656/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -43644.9793 - g_loss: 7764836.1632\n",
      "Epoch 657/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -16004.3984 - g_loss: 7387023.7474\n",
      "Epoch 658/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 23421.7977 - g_loss: 7536080.7421\n",
      "Epoch 659/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 69087.5465 - g_loss: 7161494.2316\n",
      "Epoch 660/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 750.0064 - g_loss: 7471513.1895\n",
      "Epoch 661/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 601.3647 - g_loss: 7475651.4947\n",
      "Epoch 662/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 122239.0532 - g_loss: 6477389.1474\n",
      "Epoch 663/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -128264.8918 - g_loss: 5843618.8947\n",
      "Epoch 664/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 56023.8952 - g_loss: 6694804.6579\n",
      "Epoch 665/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 72863.4407 - g_loss: 7333626.0368\n",
      "Epoch 666/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -78761.8041 - g_loss: 8737523.2000\n",
      "Epoch 667/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 118861.5218 - g_loss: 8438990.0158\n",
      "Epoch 668/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 22571.1627 - g_loss: 8886226.6895\n",
      "Epoch 669/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -84256.2392 - g_loss: 8626026.5000\n",
      "Epoch 670/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -74084.9862 - g_loss: 9498054.6474\n",
      "Epoch 671/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 22025.6079 - g_loss: 9597149.3579\n",
      "Epoch 672/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 58882.1580 - g_loss: 10154493.9895\n",
      "Epoch 673/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -1379.3346 - g_loss: 10207473.8737\n",
      "Epoch 674/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 61816.3548 - g_loss: 10415673.0000\n",
      "Epoch 675/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 7765.4651 - g_loss: 10209982.4526\n",
      "Epoch 676/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 27237.2460 - g_loss: 9797213.2000\n",
      "Epoch 677/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 63789.8223 - g_loss: 9685360.0632\n",
      "Epoch 678/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 18610.5626 - g_loss: 9570391.7684\n",
      "Epoch 679/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 3102.2800 - g_loss: 9075553.4895\n",
      "Epoch 680/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: -3214.5065 - g_loss: 9189668.2053\n",
      "Epoch 681/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -19685.5060 - g_loss: 9788818.2421\n",
      "Epoch 682/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 35195.0452 - g_loss: 10294856.7263\n",
      "Epoch 683/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -7224.3728 - g_loss: 10571696.2000\n",
      "Epoch 684/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 1671.5624 - g_loss: 9885149.1158\n",
      "Epoch 685/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2078.5324 - g_loss: 9177056.8632\n",
      "Epoch 686/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 49859.3552 - g_loss: 9165980.0053\n",
      "Epoch 687/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 63824.0675 - g_loss: 8767579.5000\n",
      "Epoch 688/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -4174.7382 - g_loss: 9290940.4526\n",
      "Epoch 689/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -107828.9008 - g_loss: 9735396.0263\n",
      "Epoch 690/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 62323.6155 - g_loss: 10473264.6737\n",
      "Epoch 691/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -17265.2754 - g_loss: 9885944.5895\n",
      "Epoch 692/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -5581.2193 - g_loss: 9116134.1421\n",
      "Epoch 693/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -13089.1409 - g_loss: 9492536.8895\n",
      "Epoch 694/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 29696.3441 - g_loss: 9543465.6737\n",
      "Epoch 695/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 11572.6623 - g_loss: 8484305.4158\n",
      "Epoch 696/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 27083.0940 - g_loss: 8437514.6421\n",
      "Epoch 697/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -674.5521 - g_loss: 8344685.2211\n",
      "Epoch 698/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -13142.2351 - g_loss: 9188768.0053\n",
      "Epoch 699/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -106936.9835 - g_loss: 11109475.7053\n",
      "Epoch 700/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -63640.6780 - g_loss: 13070455.6737\n",
      "Epoch 701/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 63751.0917 - g_loss: 12739019.2316\n",
      "Epoch 702/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 66342.2115 - g_loss: 11194041.8947\n",
      "Epoch 703/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -83608.0892 - g_loss: 10774806.1895\n",
      "Epoch 704/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 62255.6979 - g_loss: 10274998.5053\n",
      "Epoch 705/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 143485.0520 - g_loss: 9138784.8632\n",
      "Epoch 706/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 118468.9106 - g_loss: 8842563.1895\n",
      "Epoch 707/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 34220.1440 - g_loss: 8554468.9105\n",
      "Epoch 708/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -34122.1563 - g_loss: 9358704.2632\n",
      "Epoch 709/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 43423.9479 - g_loss: 9841639.4789\n",
      "Epoch 710/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -47979.2586 - g_loss: 8585160.1947\n",
      "Epoch 711/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 15930.4671 - g_loss: 9061725.0842\n",
      "Epoch 712/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 56691.2343 - g_loss: 10450995.4737\n",
      "Epoch 713/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -26244.5569 - g_loss: 11192463.2632\n",
      "Epoch 714/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 73124.0270 - g_loss: 12393392.2105\n",
      "Epoch 715/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -21274.4932 - g_loss: 14362823.9789\n",
      "Epoch 716/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 45673.3230 - g_loss: 14804712.1895\n",
      "Epoch 717/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -5932.4060 - g_loss: 16014156.7684\n",
      "Epoch 718/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 53167.0582 - g_loss: 14764391.6526\n",
      "Epoch 719/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 6959.5638 - g_loss: 13950105.9263\n",
      "Epoch 720/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -31214.6421 - g_loss: 13570695.6842\n",
      "Epoch 721/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -69494.7407 - g_loss: 13642673.2737\n",
      "Epoch 722/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -37391.4960 - g_loss: 13903456.2632s - d_loss: -60172.6959 - g_loss: 13896\n",
      "Epoch 723/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -65346.6776 - g_loss: 14501674.9684\n",
      "Epoch 724/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -181747.6481 - g_loss: 14847044.8947\n",
      "Epoch 725/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 7191.5147 - g_loss: 16157804.8000\n",
      "Epoch 726/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 38576.5683 - g_loss: 15732516.7158\n",
      "Epoch 727/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -67304.3250 - g_loss: 15430716.0105\n",
      "Epoch 728/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -67529.3014 - g_loss: 13264937.7053\n",
      "Epoch 729/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -68100.2309 - g_loss: 12250770.6421\n",
      "Epoch 730/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -2916.6555 - g_loss: 13256369.7053\n",
      "Epoch 731/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -22124.9957 - g_loss: 13385176.7158\n",
      "Epoch 732/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -180920.2610 - g_loss: 13507327.7789\n",
      "Epoch 733/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -109505.1169 - g_loss: 14904030.1789\n",
      "Epoch 734/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 31517.2226 - g_loss: 14338977.3158\n",
      "Epoch 735/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -41648.4357 - g_loss: 14094699.3368\n",
      "Epoch 736/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -84790.1025 - g_loss: 13835935.7895\n",
      "Epoch 737/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 129697.8353 - g_loss: 13328745.5789\n",
      "Epoch 738/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -20382.1943 - g_loss: 13130399.5368\n",
      "Epoch 739/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 64880.9105 - g_loss: 12207712.3789\n",
      "Epoch 740/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -26652.9445 - g_loss: 12450904.9789\n",
      "Epoch 741/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 64286.0189 - g_loss: 12636644.9789\n",
      "Epoch 742/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 59372.9572 - g_loss: 12917321.5684\n",
      "Epoch 743/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -40637.1618 - g_loss: 12908948.1263\n",
      "Epoch 744/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -64807.7700 - g_loss: 13435495.3368\n",
      "Epoch 745/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 69608.7462 - g_loss: 12635275.3158\n",
      "Epoch 746/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 59066.6368 - g_loss: 13317504.1158\n",
      "Epoch 747/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -6350.7288 - g_loss: 14543950.0000\n",
      "Epoch 748/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -55190.4298 - g_loss: 14022779.5684\n",
      "Epoch 749/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 31379.1884 - g_loss: 13816815.7684\n",
      "Epoch 750/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 58824.7436 - g_loss: 13247916.6316\n",
      "Epoch 751/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -51166.5347 - g_loss: 12302037.5158\n",
      "Epoch 752/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -26443.5454 - g_loss: 12211340.6000\n",
      "Epoch 753/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 20243.9473 - g_loss: 10929869.0526\n",
      "Epoch 754/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 45661.8081 - g_loss: 9794439.6105\n",
      "Epoch 755/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -41146.5887 - g_loss: 9577266.1789\n",
      "Epoch 756/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 13206.8823 - g_loss: 9602406.9789\n",
      "Epoch 757/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 18859.5656 - g_loss: 9039997.0737\n",
      "Epoch 758/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -47207.2083 - g_loss: 8838084.7579\n",
      "Epoch 759/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 23918.2378 - g_loss: 9808481.1526\n",
      "Epoch 760/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 21992.7985 - g_loss: 10187485.3684\n",
      "Epoch 761/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 10226.0778 - g_loss: 9409782.9105\n",
      "Epoch 762/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -55145.4168 - g_loss: 9086181.6737\n",
      "Epoch 763/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 29643.8888 - g_loss: 9202477.0263 10s - d_loss:\n",
      "Epoch 764/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 29623.3332 - g_loss: 9000571.5842\n",
      "Epoch 765/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 7695.0454 - g_loss: 8432640.6105\n",
      "Epoch 766/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 71004.4979 - g_loss: 8041744.4737\n",
      "Epoch 767/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -43328.4208 - g_loss: 7620698.4158\n",
      "Epoch 768/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -35175.3095 - g_loss: 7072397.6947\n",
      "Epoch 769/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 17693.5230 - g_loss: 7319073.6684\n",
      "Epoch 770/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 147.9803 - g_loss: 7423719.0474\n",
      "Epoch 771/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 30622.4765 - g_loss: 6797534.2211\n",
      "Epoch 772/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 8325.1251 - g_loss: 7079105.3789\n",
      "Epoch 773/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 34558.9273 - g_loss: 6731156.6789\n",
      "Epoch 774/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -35848.8213 - g_loss: 7466230.4421\n",
      "Epoch 775/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 22332.7572 - g_loss: 8101274.5526\n",
      "Epoch 776/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -54887.6968 - g_loss: 7672769.8474\n",
      "Epoch 777/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 42030.4504 - g_loss: 7551505.0263\n",
      "Epoch 778/2000\n",
      "94/94 [==============================] - 21s 221ms/step - d_loss: -77446.9877 - g_loss: 7369045.5579\n",
      "Epoch 779/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 36263.4363 - g_loss: 7100611.4316\n",
      "Epoch 780/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 42919.3852 - g_loss: 6724579.7895\n",
      "Epoch 781/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -84384.2010 - g_loss: 7734529.1632\n",
      "Epoch 782/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -101010.1920 - g_loss: 8318851.2211\n",
      "Epoch 783/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -25160.4315 - g_loss: 7562153.4263\n",
      "Epoch 784/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 65483.5881 - g_loss: 6794140.5158\n",
      "Epoch 785/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -116913.6091 - g_loss: 6850160.1947\n",
      "Epoch 786/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 36764.4315 - g_loss: 6472881.9105\n",
      "Epoch 787/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -32241.3973 - g_loss: 5925551.9947\n",
      "Epoch 788/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -24158.6141 - g_loss: 6647304.2000\n",
      "Epoch 789/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 25774.5381 - g_loss: 7077304.3632\n",
      "Epoch 790/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -9964.0749 - g_loss: 7114270.9474\n",
      "Epoch 791/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 12120.8137 - g_loss: 7146197.0474\n",
      "Epoch 792/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -71003.2394 - g_loss: 6873460.0263\n",
      "Epoch 793/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -23320.3116 - g_loss: 7182515.5737\n",
      "Epoch 794/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 39405.2061 - g_loss: 7672090.1316\n",
      "Epoch 795/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 72437.5728 - g_loss: 8404937.2211\n",
      "Epoch 796/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -56688.4325 - g_loss: 7830317.2895\n",
      "Epoch 797/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -19473.6824 - g_loss: 8323218.3737\n",
      "Epoch 798/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 41083.6705 - g_loss: 8309719.1579\n",
      "Epoch 799/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 48588.6391 - g_loss: 7339427.8842\n",
      "Epoch 800/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 71568.3835 - g_loss: 6047282.0632\n",
      "Epoch 801/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -15155.7086 - g_loss: 5790242.0474\n",
      "Epoch 802/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 32864.2755 - g_loss: 5673164.9158\n",
      "Epoch 803/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 26508.5903 - g_loss: 5992835.5000\n",
      "Epoch 804/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -7286.3630 - g_loss: 6452252.6789\n",
      "Epoch 805/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -31291.5111 - g_loss: 6772645.2211\n",
      "Epoch 806/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 4922.4181 - g_loss: 6999381.6579\n",
      "Epoch 807/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -10777.3281 - g_loss: 7034780.5895\n",
      "Epoch 808/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -49409.1790 - g_loss: 7293045.3316\n",
      "Epoch 809/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 36071.8308 - g_loss: 7176322.3211\n",
      "Epoch 810/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 15593.6925 - g_loss: 6779406.7316\n",
      "Epoch 811/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -3682.6103 - g_loss: 6300608.0105\n",
      "Epoch 812/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -8476.0724 - g_loss: 6441722.5947\n",
      "Epoch 813/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 7985.5326 - g_loss: 6898350.9947\n",
      "Epoch 814/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -57585.8968 - g_loss: 7691727.6105\n",
      "Epoch 815/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -21223.9304 - g_loss: 8281139.4263\n",
      "Epoch 816/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 2235.7937 - g_loss: 8937417.0632\n",
      "Epoch 817/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 33096.1203 - g_loss: 8716142.5842\n",
      "Epoch 818/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 6225.3331 - g_loss: 7932620.8737\n",
      "Epoch 819/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -7303.7168 - g_loss: 7897559.7368\n",
      "Epoch 820/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 18668.5737 - g_loss: 6881775.5895\n",
      "Epoch 821/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 38788.4410 - g_loss: 6113020.5000\n",
      "Epoch 822/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 42100.8882 - g_loss: 6315287.7211\n",
      "Epoch 823/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -33886.6115 - g_loss: 6393351.8737\n",
      "Epoch 824/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -11383.3310 - g_loss: 6133981.4895\n",
      "Epoch 825/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -30178.6264 - g_loss: 5804688.0842\n",
      "Epoch 826/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -17907.9216 - g_loss: 5543580.2947\n",
      "Epoch 827/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 40991.9135 - g_loss: 5853884.2895\n",
      "Epoch 828/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -22143.1956 - g_loss: 5408534.3579\n",
      "Epoch 829/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 10299.0168 - g_loss: 4802480.6684\n",
      "Epoch 830/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -3207.1338 - g_loss: 4837994.2368\n",
      "Epoch 831/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -16365.6661 - g_loss: 5073608.0421\n",
      "Epoch 832/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 28116.0001 - g_loss: 5017961.4000\n",
      "Epoch 833/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -11621.1906 - g_loss: 4808923.1342\n",
      "Epoch 834/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 19223.6328 - g_loss: 4860480.4947\n",
      "Epoch 835/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -54552.7985 - g_loss: 5449926.3000\n",
      "Epoch 836/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -16289.4326 - g_loss: 5214609.6895\n",
      "Epoch 837/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 18483.8493 - g_loss: 4935339.4921\n",
      "Epoch 838/2000\n",
      "94/94 [==============================] - 16s 174ms/step - d_loss: 14008.9527 - g_loss: 4591412.6237\n",
      "Epoch 839/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -15576.5995 - g_loss: 4662947.0316\n",
      "Epoch 840/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -2292.9182 - g_loss: 4172389.1658\n",
      "Epoch 841/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 15478.9496 - g_loss: 3864122.1947\n",
      "Epoch 842/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 20629.0335 - g_loss: 3615426.5711\n",
      "Epoch 843/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 20258.1845 - g_loss: 3330474.1211\n",
      "Epoch 844/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -1508.4660 - g_loss: 3049072.2974\n",
      "Epoch 845/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 216.7983 - g_loss: 2577870.1974\n",
      "Epoch 846/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 15436.5229 - g_loss: 2023908.8697\n",
      "Epoch 847/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -10719.7817 - g_loss: 1611770.8184\n",
      "Epoch 848/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 2374.5247 - g_loss: 1545713.5566\n",
      "Epoch 849/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -380.5125 - g_loss: 1149077.8645\n",
      "Epoch 850/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 567.2388 - g_loss: 1125999.4349\n",
      "Epoch 851/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: -4883.0523 - g_loss: 1214776.7197\n",
      "Epoch 852/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 5808.2676 - g_loss: 1086383.9414\n",
      "Epoch 853/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 2357.6756 - g_loss: 720221.4286\n",
      "Epoch 854/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -4300.5577 - g_loss: 703559.7059\n",
      "Epoch 855/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -4624.3408 - g_loss: 995294.8329\n",
      "Epoch 856/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -11906.7117 - g_loss: 1190679.8118\n",
      "Epoch 857/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 12475.1093 - g_loss: 1376254.3697\n",
      "Epoch 858/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 6259.4164 - g_loss: 1569634.6447\n",
      "Epoch 859/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 12934.6716 - g_loss: 1593218.0500\n",
      "Epoch 860/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -4157.0364 - g_loss: 1821635.5908\n",
      "Epoch 861/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: 13299.4453 - g_loss: 1912352.3000\n",
      "Epoch 862/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 13855.1246 - g_loss: 2312213.5158\n",
      "Epoch 863/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1724.8857 - g_loss: 2336723.5829\n",
      "Epoch 864/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -3164.9332 - g_loss: 2244831.1842\n",
      "Epoch 865/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -1355.6221 - g_loss: 2232039.1592\n",
      "Epoch 866/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -56.7238 - g_loss: 1651390.2158\n",
      "Epoch 867/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 285.9027 - g_loss: 1007355.6309\n",
      "Epoch 868/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -464.0063 - g_loss: 1079943.2447\n",
      "Epoch 869/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -2643.1083 - g_loss: 1145675.0520\n",
      "Epoch 870/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 1140.3887 - g_loss: 756925.5217\n",
      "Epoch 871/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 9535.3943 - g_loss: 206289.6385\n",
      "Epoch 872/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 5402.3705 - g_loss: 402170.7192\n",
      "Epoch 873/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -8243.4777 - g_loss: 335608.4379\n",
      "Epoch 874/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -3014.8856 - g_loss: 408507.8747\n",
      "Epoch 875/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 1593.2194 - g_loss: -460119.9600\n",
      "Epoch 876/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -536.3877 - g_loss: -244797.6812\n",
      "Epoch 877/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -6861.6650 - g_loss: -991754.9868\n",
      "Epoch 878/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -4915.3932 - g_loss: -1010690.2289\n",
      "Epoch 879/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 5337.5593 - g_loss: -892933.7842\n",
      "Epoch 880/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 2600.1141 - g_loss: -268035.8364\n",
      "Epoch 881/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 9371.1780 - g_loss: -1035580.3253\n",
      "Epoch 882/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 1720.7416 - g_loss: -1328017.0849\n",
      "Epoch 883/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 2572.3974 - g_loss: -1022501.5730\n",
      "Epoch 884/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -622.5363 - g_loss: -1056018.7967\n",
      "Epoch 885/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 5552.8843 - g_loss: -1140612.2099\n",
      "Epoch 886/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 7987.2533 - g_loss: -1124955.6947\n",
      "Epoch 887/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 4279.7215 - g_loss: -964353.8063\n",
      "Epoch 888/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 4586.1079 - g_loss: -420742.9971\n",
      "Epoch 889/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -1866.8194 - g_loss: -1246815.9829\n",
      "Epoch 890/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -3614.1120 - g_loss: -1437330.3158\n",
      "Epoch 891/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 1621.3307 - g_loss: -1497535.2184\n",
      "Epoch 892/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2383.7643 - g_loss: -1413401.1842\n",
      "Epoch 893/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -1604.9827 - g_loss: -1250459.2888\n",
      "Epoch 894/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -1470.3624 - g_loss: -1595494.3658\n",
      "Epoch 895/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3319.5223 - g_loss: -1627770.0737\n",
      "Epoch 896/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -1657.4378 - g_loss: -1239101.9375\n",
      "Epoch 897/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5964.7178 - g_loss: -1223153.6783\n",
      "Epoch 898/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -8461.6669 - g_loss: -1289990.1908\n",
      "Epoch 899/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 3571.9735 - g_loss: -1126174.8474\n",
      "Epoch 900/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 1223.1382 - g_loss: -1003521.6612\n",
      "Epoch 901/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -53.1103 - g_loss: -549707.1365\n",
      "Epoch 902/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -2328.3598 - g_loss: -463616.6895\n",
      "Epoch 903/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -342.5376 - g_loss: -375841.2701\n",
      "Epoch 904/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1113.9129 - g_loss: -355144.0194\n",
      "Epoch 905/2000\n",
      "94/94 [==============================] - 15s 163ms/step - d_loss: -1357.0746 - g_loss: -582254.4724\n",
      "Epoch 906/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -222.8139 - g_loss: -731496.5184\n",
      "Epoch 907/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -594.8260 - g_loss: -825957.7507\n",
      "Epoch 908/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -6146.0658 - g_loss: -925897.9118\n",
      "Epoch 909/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -4241.3859 - g_loss: -816289.3243\n",
      "Epoch 910/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -2887.0255 - g_loss: -710706.9934\n",
      "Epoch 911/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -255.0630 - g_loss: -782053.5151\n",
      "Epoch 912/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 148.4741 - g_loss: -859767.1743\n",
      "Epoch 913/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -3273.3027 - g_loss: -952690.6612\n",
      "Epoch 914/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -1122.9837 - g_loss: -818258.8480\n",
      "Epoch 915/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 2354.4932 - g_loss: -649460.3026\n",
      "Epoch 916/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 2053.3880 - g_loss: -496478.4161\n",
      "Epoch 917/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -2091.1918 - g_loss: -448750.4734\n",
      "Epoch 918/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -6166.0720 - g_loss: -491335.2711\n",
      "Epoch 919/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 4425.4616 - g_loss: -326507.3518\n",
      "Epoch 920/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -242.3418 - g_loss: -334122.2253\n",
      "Epoch 921/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 2026.0601 - g_loss: -297250.7280\n",
      "Epoch 922/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: -386.6659 - g_loss: -305085.1691\n",
      "Epoch 923/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -645.1502 - g_loss: -278710.1414\n",
      "Epoch 924/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -1521.2413 - g_loss: -225828.9859\n",
      "Epoch 925/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -1794.7719 - g_loss: -184981.9810\n",
      "Epoch 926/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 1133.0755 - g_loss: -68101.1646\n",
      "Epoch 927/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 1300.5876 - g_loss: -61656.1611\n",
      "Epoch 928/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 2069.2094 - g_loss: -203590.5192\n",
      "Epoch 929/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -729.9323 - g_loss: -162339.1269\n",
      "Epoch 930/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -443.9544 - g_loss: -249563.9227\n",
      "Epoch 931/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -120.7803 - g_loss: -582568.0832\n",
      "Epoch 932/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -729.9140 - g_loss: -468076.8497\n",
      "Epoch 933/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 765.7540 - g_loss: -488732.0734\n",
      "Epoch 934/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -1277.0974 - g_loss: -474253.7819\n",
      "Epoch 935/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 3704.8408 - g_loss: -584492.4069\n",
      "Epoch 936/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2119.6160 - g_loss: -780816.8862\n",
      "Epoch 937/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -1669.1757 - g_loss: -711412.7454\n",
      "Epoch 938/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 3807.6981 - g_loss: -775654.8691\n",
      "Epoch 939/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 6312.4972 - g_loss: -738087.1395\n",
      "Epoch 940/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -391.3646 - g_loss: -839543.5921\n",
      "Epoch 941/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 330.4864 - g_loss: -666722.6553\n",
      "Epoch 942/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 2322.6345 - g_loss: -732331.1079\n",
      "Epoch 943/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 4758.7441 - g_loss: -711332.3441\n",
      "Epoch 944/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 4725.8276 - g_loss: -635867.5599\n",
      "Epoch 945/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1169.5787 - g_loss: -513482.0855\n",
      "Epoch 946/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -11838.3991 - g_loss: -534053.1882\n",
      "Epoch 947/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 3014.2310 - g_loss: -545136.1658\n",
      "Epoch 948/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 3111.6560 - g_loss: -369006.7829\n",
      "Epoch 949/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2215.7470 - g_loss: -246212.1964\n",
      "Epoch 950/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1126.6579 - g_loss: -182222.6214\n",
      "Epoch 951/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -1978.8258 - g_loss: -200611.8469\n",
      "Epoch 952/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -1939.5042 - g_loss: -293695.7809\n",
      "Epoch 953/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 3940.5757 - g_loss: -471398.2661\n",
      "Epoch 954/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -1295.1915 - g_loss: -341980.0095\n",
      "Epoch 955/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -99.8243 - g_loss: -223438.5184\n",
      "Epoch 956/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -641.2056 - g_loss: -163689.3865\n",
      "Epoch 957/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 302.2763 - g_loss: -68630.5794\n",
      "Epoch 958/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 423.3342 - g_loss: 4719.8635\n",
      "Epoch 959/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: -2100.1398 - g_loss: 414802.3576\n",
      "Epoch 960/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 826.5368 - g_loss: 315080.5035\n",
      "Epoch 961/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -1952.6694 - g_loss: 470070.3878\n",
      "Epoch 962/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 3854.2668 - g_loss: 186809.1649\n",
      "Epoch 963/2000\n",
      "94/94 [==============================] - 21s 224ms/step - d_loss: 9686.7067 - g_loss: 179114.0113\n",
      "Epoch 964/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 336.4386 - g_loss: 41561.6889\n",
      "Epoch 965/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -5218.9736 - g_loss: 158130.7511\n",
      "Epoch 966/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 886.0090 - g_loss: 47259.8480\n",
      "Epoch 967/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 2564.7352 - g_loss: 172094.8986\n",
      "Epoch 968/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -869.4651 - g_loss: 292213.7892\n",
      "Epoch 969/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 4087.1805 - g_loss: 495506.9237\n",
      "Epoch 970/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -6804.5367 - g_loss: 286738.4092\n",
      "Epoch 971/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -9255.1869 - g_loss: -23365.4676\n",
      "Epoch 972/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -3392.0549 - g_loss: -170276.2618\n",
      "Epoch 973/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 1172.2314 - g_loss: -329564.3365\n",
      "Epoch 974/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1154.6548 - g_loss: -395541.2980\n",
      "Epoch 975/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -700.6139 - g_loss: -411491.5970\n",
      "Epoch 976/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 3871.7636 - g_loss: -467008.4507\n",
      "Epoch 977/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5035.0127 - g_loss: -623403.3010\n",
      "Epoch 978/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 5475.7081 - g_loss: -650161.5145\n",
      "Epoch 979/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 1171.2448 - g_loss: -539500.3780\n",
      "Epoch 980/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -2115.8569 - g_loss: -718827.0375\n",
      "Epoch 981/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -4506.0858 - g_loss: -940962.0079\n",
      "Epoch 982/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -8133.7369 - g_loss: -999632.6342\n",
      "Epoch 983/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -2082.2067 - g_loss: -824856.6447\n",
      "Epoch 984/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -4885.8904 - g_loss: -1124523.4454\n",
      "Epoch 985/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -4133.3702 - g_loss: -1298442.4816\n",
      "Epoch 986/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 8252.2744 - g_loss: -1377291.4553\n",
      "Epoch 987/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -13229.7606 - g_loss: -1525143.5316\n",
      "Epoch 988/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -12393.0576 - g_loss: -1771983.0987\n",
      "Epoch 989/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -18565.2467 - g_loss: -1800741.0303\n",
      "Epoch 990/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 2147.7533 - g_loss: -1641622.8408\n",
      "Epoch 991/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 11232.2660 - g_loss: -1672093.0224\n",
      "Epoch 992/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -6211.6819 - g_loss: -1685792.3092\n",
      "Epoch 993/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 5098.8675 - g_loss: -1964576.1658\n",
      "Epoch 994/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 3519.6748 - g_loss: -1707250.8382\n",
      "Epoch 995/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 8307.2521 - g_loss: -1882329.2947\n",
      "Epoch 996/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 23222.0638 - g_loss: -2279037.1250\n",
      "Epoch 997/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -10628.4626 - g_loss: -2318946.4974\n",
      "Epoch 998/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -1971.7536 - g_loss: -2087589.7316\n",
      "Epoch 999/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 21774.5698 - g_loss: -2356914.5895\n",
      "Epoch 1000/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 27323.4742 - g_loss: -2562547.6132\n",
      "Epoch 1001/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 10814.6663 - g_loss: -2741609.8395\n",
      "Epoch 1002/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -8556.8185 - g_loss: -2697161.7553\n",
      "Epoch 1003/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 18664.0754 - g_loss: -2789006.6842\n",
      "Epoch 1004/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -19058.6648 - g_loss: -2610586.4368\n",
      "Epoch 1005/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 5588.0904 - g_loss: -2575712.2816\n",
      "Epoch 1006/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 10981.1095 - g_loss: -2450760.8079\n",
      "Epoch 1007/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 21928.8172 - g_loss: -2271463.2276\n",
      "Epoch 1008/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 4527.7725 - g_loss: -2267184.1737\n",
      "Epoch 1009/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -5132.2073 - g_loss: -2178328.6763\n",
      "Epoch 1010/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -3766.5981 - g_loss: -1941761.7908\n",
      "Epoch 1011/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5788.5309 - g_loss: -2059112.4882\n",
      "Epoch 1012/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -7462.5187 - g_loss: -2196577.9092\n",
      "Epoch 1013/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 13023.6384 - g_loss: -2359674.4934\n",
      "Epoch 1014/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 3375.7414 - g_loss: -2338068.1066\n",
      "Epoch 1015/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 7368.9685 - g_loss: -2616936.1289\n",
      "Epoch 1016/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 7777.4067 - g_loss: -2567118.5684\n",
      "Epoch 1017/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -7079.7385 - g_loss: -2709178.4500\n",
      "Epoch 1018/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 7909.9720 - g_loss: -2595447.5263\n",
      "Epoch 1019/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 21103.9078 - g_loss: -3033539.4447\n",
      "Epoch 1020/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -2514.5600 - g_loss: -2774347.2079\n",
      "Epoch 1021/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 1072.3371 - g_loss: -3104812.1711\n",
      "Epoch 1022/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 2452.4388 - g_loss: -3640670.6526\n",
      "Epoch 1023/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 1789.7304 - g_loss: -3523248.4237\n",
      "Epoch 1024/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -130.1346 - g_loss: -3407248.3184\n",
      "Epoch 1025/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -2075.9251 - g_loss: -3405397.4316\n",
      "Epoch 1026/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 12780.8202 - g_loss: -3246772.7500\n",
      "Epoch 1027/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 5743.9273 - g_loss: -3254102.7895\n",
      "Epoch 1028/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 29451.6975 - g_loss: -3173464.9605\n",
      "Epoch 1029/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -1017.8242 - g_loss: -3313234.8263\n",
      "Epoch 1030/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -26414.4240 - g_loss: -3036072.9105\n",
      "Epoch 1031/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 19009.7204 - g_loss: -2660595.1000\n",
      "Epoch 1032/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 2213.8108 - g_loss: -2283033.6158\n",
      "Epoch 1033/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -19430.8951 - g_loss: -2518157.9158\n",
      "Epoch 1034/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5220.6084 - g_loss: -2366873.8382\n",
      "Epoch 1035/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 12963.3423 - g_loss: -1650838.0750\n",
      "Epoch 1036/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 9206.2181 - g_loss: -1265491.6684\n",
      "Epoch 1037/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 23.4015 - g_loss: -1364310.4618\n",
      "Epoch 1038/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2843.7651 - g_loss: -1181681.4967\n",
      "Epoch 1039/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -12011.4992 - g_loss: -885897.9145\n",
      "Epoch 1040/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 627.0206 - g_loss: -605533.9470\n",
      "Epoch 1041/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 10442.1995 - g_loss: -448439.5337\n",
      "Epoch 1042/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -4343.5845 - g_loss: -404114.2704\n",
      "Epoch 1043/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -3464.0393 - g_loss: -402435.1987\n",
      "Epoch 1044/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 2056.4612 - g_loss: -246304.0882\n",
      "Epoch 1045/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -8209.3646 - g_loss: -197595.8707\n",
      "Epoch 1046/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 1565.5884 - g_loss: -294897.2557\n",
      "Epoch 1047/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -5285.1720 - g_loss: -377298.8822\n",
      "Epoch 1048/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -2890.3427 - g_loss: -21198.5405\n",
      "Epoch 1049/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -1233.9887 - g_loss: 424695.4798\n",
      "Epoch 1050/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 24983.7252 - g_loss: 749459.7717\n",
      "Epoch 1051/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -4436.0676 - g_loss: 400461.3990\n",
      "Epoch 1052/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 9620.0151 - g_loss: -32229.6819\n",
      "Epoch 1053/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 5078.0169 - g_loss: -283532.9972\n",
      "Epoch 1054/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -4908.8342 - g_loss: -104525.2749\n",
      "Epoch 1055/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -2403.9909 - g_loss: -111821.3775\n",
      "Epoch 1056/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 14382.7323 - g_loss: -74791.9419\n",
      "Epoch 1057/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -655.5201 - g_loss: -146803.9449\n",
      "Epoch 1058/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 6117.6810 - g_loss: -476357.6980\n",
      "Epoch 1059/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -205.1971 - g_loss: -665824.1086\n",
      "Epoch 1060/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 10042.2202 - g_loss: -722105.0763\n",
      "Epoch 1061/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -2246.1542 - g_loss: -878832.8151\n",
      "Epoch 1062/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 4306.0720 - g_loss: -757019.9059\n",
      "Epoch 1063/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1114.9633 - g_loss: -643337.2793\n",
      "Epoch 1064/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -2200.6102 - g_loss: -513655.4086\n",
      "Epoch 1065/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -1657.4966 - g_loss: -537219.2049\n",
      "Epoch 1066/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -5302.6948 - g_loss: -611522.2276\n",
      "Epoch 1067/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -1236.6049 - g_loss: -625683.4490\n",
      "Epoch 1068/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -4787.7126 - g_loss: -566161.9671\n",
      "Epoch 1069/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -9051.7131 - g_loss: -541422.0543\n",
      "Epoch 1070/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -8473.7821 - g_loss: -362591.9232\n",
      "Epoch 1071/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -2185.9558 - g_loss: -534132.5566\n",
      "Epoch 1072/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 794.2628 - g_loss: -707778.4158\n",
      "Epoch 1073/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -7392.1910 - g_loss: -670048.4921\n",
      "Epoch 1074/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 8521.7921 - g_loss: -728233.0967\n",
      "Epoch 1075/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -610.3821 - g_loss: -680208.8599\n",
      "Epoch 1076/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -3244.8146 - g_loss: -1024516.0691\n",
      "Epoch 1077/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 6402.4271 - g_loss: -1249933.9961\n",
      "Epoch 1078/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 7322.4992 - g_loss: -1048188.5586\n",
      "Epoch 1079/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -724.5655 - g_loss: -1260080.2724\n",
      "Epoch 1080/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -4426.2650 - g_loss: -1313212.7500\n",
      "Epoch 1081/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 4105.7121 - g_loss: -1065124.8632\n",
      "Epoch 1082/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 3970.2740 - g_loss: -821996.5105\n",
      "Epoch 1083/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 14930.4653 - g_loss: -826627.9395\n",
      "Epoch 1084/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -8024.5788 - g_loss: -672169.4428\n",
      "Epoch 1085/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -3080.7503 - g_loss: -507951.7789\n",
      "Epoch 1086/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -4008.6740 - g_loss: -286193.8231\n",
      "Epoch 1087/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -8180.4325 - g_loss: -170584.0738\n",
      "Epoch 1088/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -911.6579 - g_loss: -292810.8802\n",
      "Epoch 1089/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 1536.2785 - g_loss: -813757.7507\n",
      "Epoch 1090/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 4253.7431 - g_loss: -889124.7961\n",
      "Epoch 1091/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 109.2843 - g_loss: -1222463.5770\n",
      "Epoch 1092/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 227.7903 - g_loss: -1584015.8579\n",
      "Epoch 1093/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -155.1744 - g_loss: -1438999.4171\n",
      "Epoch 1094/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -4412.3456 - g_loss: -1328496.7053\n",
      "Epoch 1095/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 10066.5046 - g_loss: -1384262.7658\n",
      "Epoch 1096/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -21600.9698 - g_loss: -1715341.3118\n",
      "Epoch 1097/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 14274.1375 - g_loss: -1909276.1658\n",
      "Epoch 1098/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -6214.0453 - g_loss: -2137594.6487\n",
      "Epoch 1099/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 21327.0195 - g_loss: -2184235.3118\n",
      "Epoch 1100/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 6561.1052 - g_loss: -1847301.3816\n",
      "Epoch 1101/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -516.2508 - g_loss: -1543287.9711\n",
      "Epoch 1102/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -16657.7075 - g_loss: -1717264.8566\n",
      "Epoch 1103/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -788.6581 - g_loss: -1699095.8895\n",
      "Epoch 1104/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -343.4697 - g_loss: -1681091.4158\n",
      "Epoch 1105/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -7269.2975 - g_loss: -1527444.7316\n",
      "Epoch 1106/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1004.7795 - g_loss: -1371145.5882\n",
      "Epoch 1107/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -1152.4914 - g_loss: -1484116.0618\n",
      "Epoch 1108/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 14092.5738 - g_loss: -1490478.4066\n",
      "Epoch 1109/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -15790.4299 - g_loss: -1403539.0697\n",
      "Epoch 1110/2000\n",
      "94/94 [==============================] - 15s 163ms/step - d_loss: -1723.6674 - g_loss: -1337106.3526\n",
      "Epoch 1111/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3490.4234 - g_loss: -1560716.4250\n",
      "Epoch 1112/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 1858.0342 - g_loss: -1412073.2342\n",
      "Epoch 1113/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 4460.9297 - g_loss: -1063048.1178\n",
      "Epoch 1114/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 11403.0187 - g_loss: -635132.5579\n",
      "Epoch 1115/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 1635.5440 - g_loss: -516030.8890\n",
      "Epoch 1116/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3501.0549 - g_loss: -649868.4757\n",
      "Epoch 1117/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -1122.0142 - g_loss: -439596.0563\n",
      "Epoch 1118/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 3812.9115 - g_loss: -337034.3575\n",
      "Epoch 1119/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: -2275.8970 - g_loss: -289888.7701\n",
      "Epoch 1120/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 5400.2159 - g_loss: -482424.9049\n",
      "Epoch 1121/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -12224.1491 - g_loss: -598215.7207\n",
      "Epoch 1122/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -596.4424 - g_loss: -1051736.4434\n",
      "Epoch 1123/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -12187.0211 - g_loss: -1018734.7691\n",
      "Epoch 1124/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -16893.7652 - g_loss: -941318.2079\n",
      "Epoch 1125/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -7681.0943 - g_loss: -1041908.2822\n",
      "Epoch 1126/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 17963.3299 - g_loss: -915031.8599\n",
      "Epoch 1127/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 7974.9435 - g_loss: -858665.7586\n",
      "Epoch 1128/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -6111.8371 - g_loss: -601901.8331\n",
      "Epoch 1129/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 3036.8504 - g_loss: -493500.9681\n",
      "Epoch 1130/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 18404.8305 - g_loss: -264166.8542\n",
      "Epoch 1131/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -3887.4485 - g_loss: 811.5278\n",
      "Epoch 1132/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -13836.9776 - g_loss: 494773.8561\n",
      "Epoch 1133/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 14273.3524 - g_loss: 741975.1632\n",
      "Epoch 1134/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 6716.6134 - g_loss: 674619.1882\n",
      "Epoch 1135/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -17186.4376 - g_loss: 934415.7046\n",
      "Epoch 1136/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1154.6736 - g_loss: 1136161.8362\n",
      "Epoch 1137/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 20921.1544 - g_loss: 962621.4421\n",
      "Epoch 1138/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 10418.3072 - g_loss: 762795.6263\n",
      "Epoch 1139/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 6846.0620 - g_loss: 475070.7203\n",
      "Epoch 1140/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 2801.4894 - g_loss: -104486.3384\n",
      "Epoch 1141/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 17096.0791 - g_loss: -282019.9301\n",
      "Epoch 1142/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -1849.0512 - g_loss: 49201.6473\n",
      "Epoch 1143/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -6124.0670 - g_loss: 383252.9003\n",
      "Epoch 1144/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 9448.5156 - g_loss: 151492.8407\n",
      "Epoch 1145/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 7809.6287 - g_loss: 586343.9556\n",
      "Epoch 1146/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 2221.8492 - g_loss: 309366.3542\n",
      "Epoch 1147/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -14137.2276 - g_loss: 543075.2301\n",
      "Epoch 1148/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -201.5076 - g_loss: 819305.5579\n",
      "Epoch 1149/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 11406.3680 - g_loss: 1131130.7329\n",
      "Epoch 1150/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 11240.8666 - g_loss: 846253.4046\n",
      "Epoch 1151/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -10137.4538 - g_loss: 1188666.6316\n",
      "Epoch 1152/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3796.2507 - g_loss: 1027395.1224\n",
      "Epoch 1153/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -15970.1303 - g_loss: 1381324.7809\n",
      "Epoch 1154/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -18846.0464 - g_loss: 1949114.66842s - d_loss: -13960.6000 - g\n",
      "Epoch 1155/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -12643.4729 - g_loss: 1848497.7737\n",
      "Epoch 1156/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 4151.3239 - g_loss: 1760025.2763\n",
      "Epoch 1157/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -22562.9093 - g_loss: 1921138.1250\n",
      "Epoch 1158/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 5420.3754 - g_loss: 1980398.1526\n",
      "Epoch 1159/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -11131.2716 - g_loss: 2179257.6855\n",
      "Epoch 1160/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 40315.0146 - g_loss: 1794050.9961\n",
      "Epoch 1161/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -17678.2095 - g_loss: 1728767.7711\n",
      "Epoch 1162/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 1224.4245 - g_loss: 1635555.5711\n",
      "Epoch 1163/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -1669.2988 - g_loss: 1949297.7316\n",
      "Epoch 1164/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 6903.5902 - g_loss: 1212639.3408\n",
      "Epoch 1165/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 20407.2818 - g_loss: 986693.1829\n",
      "Epoch 1166/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -361.0293 - g_loss: 1026819.7691\n",
      "Epoch 1167/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 10624.5352 - g_loss: 1036433.0507\n",
      "Epoch 1168/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 12211.7066 - g_loss: 977114.4039\n",
      "Epoch 1169/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -30183.7055 - g_loss: 1335629.8526\n",
      "Epoch 1170/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 9050.7472 - g_loss: 1639511.4697\n",
      "Epoch 1171/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -7805.4994 - g_loss: 1457981.2158\n",
      "Epoch 1172/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 13622.2937 - g_loss: 1240271.4447\n",
      "Epoch 1173/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 8582.4926 - g_loss: 1486919.9092\n",
      "Epoch 1174/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 15823.6558 - g_loss: 1287207.3263\n",
      "Epoch 1175/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -5942.4812 - g_loss: 1187317.5895\n",
      "Epoch 1176/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -11303.8023 - g_loss: 954332.3618\n",
      "Epoch 1177/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 2152.5477 - g_loss: 1157458.1046\n",
      "Epoch 1178/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -1471.6333 - g_loss: 1087270.2257\n",
      "Epoch 1179/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 640.8438 - g_loss: 1370277.1079\n",
      "Epoch 1180/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -23700.1198 - g_loss: 1767469.6224\n",
      "Epoch 1181/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 6674.5708 - g_loss: 1661722.8395\n",
      "Epoch 1182/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -26131.4332 - g_loss: 1633898.8618\n",
      "Epoch 1183/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 11908.1023 - g_loss: 1507095.9184\n",
      "Epoch 1184/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -5849.4787 - g_loss: 1880858.7842\n",
      "Epoch 1185/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -45311.6656 - g_loss: 2041467.1171\n",
      "Epoch 1186/2000\n",
      "94/94 [==============================] - 22s 236ms/step - d_loss: 10153.5086 - g_loss: 1827269.3421\n",
      "Epoch 1187/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 8462.3397 - g_loss: 1701904.1763\n",
      "Epoch 1188/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 7513.0081 - g_loss: 1878691.2737\n",
      "Epoch 1189/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -15186.0857 - g_loss: 2003937.6447\n",
      "Epoch 1190/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -10685.5578 - g_loss: 2023998.8224\n",
      "Epoch 1191/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 19526.0259 - g_loss: 1783109.1211\n",
      "Epoch 1192/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -22224.4968 - g_loss: 1742147.6895\n",
      "Epoch 1193/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 13324.0931 - g_loss: 1528554.0592\n",
      "Epoch 1194/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -460.9388 - g_loss: 1339900.1645\n",
      "Epoch 1195/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 29844.1583 - g_loss: 1002677.8829\n",
      "Epoch 1196/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2404.6225 - g_loss: 857931.8526\n",
      "Epoch 1197/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 16113.8745 - g_loss: 734472.0076\n",
      "Epoch 1198/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -10424.9171 - g_loss: 419080.7274\n",
      "Epoch 1199/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 21229.5247 - g_loss: 461098.0299\n",
      "Epoch 1200/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 36381.0028 - g_loss: 393262.1530\n",
      "Epoch 1201/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5714.8923 - g_loss: 484216.5745\n",
      "Epoch 1202/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -16665.1077 - g_loss: 814972.6740\n",
      "Epoch 1203/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -33994.7493 - g_loss: 997083.5783\n",
      "Epoch 1204/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -14477.3405 - g_loss: 938522.3980\n",
      "Epoch 1205/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -6666.1008 - g_loss: 943169.1829\n",
      "Epoch 1206/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: -18973.5213 - g_loss: 870261.9388\n",
      "Epoch 1207/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 20042.5098 - g_loss: 896911.8171\n",
      "Epoch 1208/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -22944.9992 - g_loss: 1059832.1796\n",
      "Epoch 1209/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 23469.4845 - g_loss: 1295956.5829\n",
      "Epoch 1210/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -17710.9049 - g_loss: 1337753.1967\n",
      "Epoch 1211/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 8175.5291 - g_loss: 1873864.2632\n",
      "Epoch 1212/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -7476.3749 - g_loss: 1731061.3579\n",
      "Epoch 1213/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 23473.2076 - g_loss: 1178797.0467\n",
      "Epoch 1214/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 15480.2513 - g_loss: 1166561.9704\n",
      "Epoch 1215/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -184.8927 - g_loss: 1182833.9329\n",
      "Epoch 1216/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: 16375.6495 - g_loss: 1045260.3526\n",
      "Epoch 1217/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 12271.6693 - g_loss: 1419252.5987\n",
      "Epoch 1218/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 13015.6810 - g_loss: 1595417.5539\n",
      "Epoch 1219/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 38114.8225 - g_loss: 1484827.5500\n",
      "Epoch 1220/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -18357.3312 - g_loss: 1522222.5645\n",
      "Epoch 1221/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 12102.0635 - g_loss: 1760858.5171\n",
      "Epoch 1222/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 7615.9158 - g_loss: 1946464.9855\n",
      "Epoch 1223/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -11964.5453 - g_loss: 2221923.4724\n",
      "Epoch 1224/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -54603.1609 - g_loss: 2668343.0763\n",
      "Epoch 1225/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 42562.2262 - g_loss: 2993697.8921\n",
      "Epoch 1226/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 26169.5360 - g_loss: 2771650.1447\n",
      "Epoch 1227/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -61020.8599 - g_loss: 2698063.49873s - d_loss: -55137.3244 - \n",
      "Epoch 1228/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: -23223.8769 - g_loss: 2567703.7868\n",
      "Epoch 1229/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -41044.5398 - g_loss: 2965991.0474\n",
      "Epoch 1230/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -47862.1638 - g_loss: 3248899.8921\n",
      "Epoch 1231/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 17984.7236 - g_loss: 3714204.8184\n",
      "Epoch 1232/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 27205.3779 - g_loss: 3399566.6526\n",
      "Epoch 1233/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 12138.7363 - g_loss: 3636994.3632\n",
      "Epoch 1234/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 56326.9621 - g_loss: 4243172.0079\n",
      "Epoch 1235/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 45091.8409 - g_loss: 5098853.9289\n",
      "Epoch 1236/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: -90370.7171 - g_loss: 5662466.5632\n",
      "Epoch 1237/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 9535.2647 - g_loss: 6011798.8684\n",
      "Epoch 1238/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 78299.8928 - g_loss: 6076917.7421\n",
      "Epoch 1239/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 60074.2393 - g_loss: 5807542.5421\n",
      "Epoch 1240/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 3560.5482 - g_loss: 5293905.0737\n",
      "Epoch 1241/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 19616.0840 - g_loss: 4841950.7789\n",
      "Epoch 1242/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -16915.0118 - g_loss: 4760077.6842\n",
      "Epoch 1243/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -9148.6598 - g_loss: 4043982.6105\n",
      "Epoch 1244/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -8391.2447 - g_loss: 3915222.0895\n",
      "Epoch 1245/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 33584.5884 - g_loss: 3263941.8105\n",
      "Epoch 1246/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 12153.1426 - g_loss: 2973667.1500\n",
      "Epoch 1247/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -15478.6302 - g_loss: 3115337.4342\n",
      "Epoch 1248/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -33251.7418 - g_loss: 4000668.4711\n",
      "Epoch 1249/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -28181.1811 - g_loss: 4407429.7395\n",
      "Epoch 1250/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -17478.7869 - g_loss: 4844841.4684\n",
      "Epoch 1251/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 15345.6086 - g_loss: 5567074.1895\n",
      "Epoch 1252/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 47199.6564 - g_loss: 5684727.1263\n",
      "Epoch 1253/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 493.8143 - g_loss: 5971012.0316\n",
      "Epoch 1254/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -37255.9271 - g_loss: 5505918.3684\n",
      "Epoch 1255/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 27203.3678 - g_loss: 5319137.1737\n",
      "Epoch 1256/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 16396.1267 - g_loss: 5084375.0211\n",
      "Epoch 1257/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -9489.8167 - g_loss: 3957740.7184\n",
      "Epoch 1258/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 38262.0933 - g_loss: 3433190.4184\n",
      "Epoch 1259/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 55885.8807 - g_loss: 2847709.1000\n",
      "Epoch 1260/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -6689.4279 - g_loss: 2332274.3303\n",
      "Epoch 1261/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 10871.5084 - g_loss: 2139407.2461\n",
      "Epoch 1262/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -1552.9413 - g_loss: 2100171.3842\n",
      "Epoch 1263/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -24593.1735 - g_loss: 2190432.8211\n",
      "Epoch 1264/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 45980.2201 - g_loss: 1716382.6697\n",
      "Epoch 1265/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -26600.3525 - g_loss: 1881555.6829\n",
      "Epoch 1266/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -40326.2976 - g_loss: 1685398.5355\n",
      "Epoch 1267/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 32126.5393 - g_loss: 1904723.2039\n",
      "Epoch 1268/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -16341.0424 - g_loss: 1850081.3066\n",
      "Epoch 1269/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -10669.0389 - g_loss: 1759322.1645\n",
      "Epoch 1270/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 13629.0269 - g_loss: 2160447.4658\n",
      "Epoch 1271/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -11555.3462 - g_loss: 2737594.0303\n",
      "Epoch 1272/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 20824.0917 - g_loss: 2716627.6105\n",
      "Epoch 1273/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 35900.2706 - g_loss: 2456021.5487\n",
      "Epoch 1274/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -62244.6083 - g_loss: 2435271.7105\n",
      "Epoch 1275/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -57517.5038 - g_loss: 2891523.4316\n",
      "Epoch 1276/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -29050.6479 - g_loss: 2760172.7092\n",
      "Epoch 1277/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -61429.5555 - g_loss: 2301371.2105\n",
      "Epoch 1278/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -4658.1141 - g_loss: 2688716.3500\n",
      "Epoch 1279/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -61220.7988 - g_loss: 3448589.2263\n",
      "Epoch 1280/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 18623.5757 - g_loss: 3967079.6132\n",
      "Epoch 1281/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -10951.0863 - g_loss: 4241234.1263\n",
      "Epoch 1282/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 8834.1904 - g_loss: 4235117.1079\n",
      "Epoch 1283/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -73330.3170 - g_loss: 4245269.0500\n",
      "Epoch 1284/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 66747.6780 - g_loss: 4667576.1184\n",
      "Epoch 1285/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -11888.6318 - g_loss: 4651631.6447\n",
      "Epoch 1286/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -20728.8118 - g_loss: 4530354.5763\n",
      "Epoch 1287/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -864.9792 - g_loss: 5000166.9789\n",
      "Epoch 1288/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 7323.8957 - g_loss: 4611389.9895\n",
      "Epoch 1289/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -36580.2893 - g_loss: 4742266.0789\n",
      "Epoch 1290/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -80107.3398 - g_loss: 5128553.5026\n",
      "Epoch 1291/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -87667.1647 - g_loss: 5058781.1868\n",
      "Epoch 1292/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -39674.7455 - g_loss: 5362431.0316\n",
      "Epoch 1293/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 9062.3685 - g_loss: 6593467.1737\n",
      "Epoch 1294/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -30725.3216 - g_loss: 7357947.5421\n",
      "Epoch 1295/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -35838.1640 - g_loss: 7271890.3842\n",
      "Epoch 1296/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -15214.8291 - g_loss: 7638604.7316\n",
      "Epoch 1297/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: -16787.0391 - g_loss: 6835187.5947\n",
      "Epoch 1298/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -43191.9651 - g_loss: 6679601.4842\n",
      "Epoch 1299/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -61987.9547 - g_loss: 6413878.8579\n",
      "Epoch 1300/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -21876.9835 - g_loss: 6779615.3789\n",
      "Epoch 1301/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: -11622.5397 - g_loss: 6429604.3579\n",
      "Epoch 1302/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -10619.7057 - g_loss: 5975228.1684\n",
      "Epoch 1303/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -47043.1612 - g_loss: 6119934.6789\n",
      "Epoch 1304/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -63559.7575 - g_loss: 6261186.6263\n",
      "Epoch 1305/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 17977.0880 - g_loss: 6635489.0263\n",
      "Epoch 1306/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -7801.9042 - g_loss: 7254785.4684\n",
      "Epoch 1307/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 7835.4283 - g_loss: 8260866.4158\n",
      "Epoch 1308/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -112659.9391 - g_loss: 9517369.8632\n",
      "Epoch 1309/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -29303.5603 - g_loss: 10547027.1789\n",
      "Epoch 1310/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 85603.3763 - g_loss: 9317718.3737\n",
      "Epoch 1311/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 89363.7978 - g_loss: 9198667.0474\n",
      "Epoch 1312/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -27089.8309 - g_loss: 10009588.9579\n",
      "Epoch 1313/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -40064.4512 - g_loss: 10593730.1474\n",
      "Epoch 1314/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -38623.9650 - g_loss: 10385971.9158\n",
      "Epoch 1315/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 8381.0117 - g_loss: 10812620.5158\n",
      "Epoch 1316/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 28101.9456 - g_loss: 9896895.1895\n",
      "Epoch 1317/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 31156.4485 - g_loss: 8355420.0000\n",
      "Epoch 1318/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 10751.2306 - g_loss: 9109192.3947\n",
      "Epoch 1319/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -126023.4656 - g_loss: 8815105.7895\n",
      "Epoch 1320/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 130586.6136 - g_loss: 7792726.1158\n",
      "Epoch 1321/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -5443.9033 - g_loss: 6817551.5316\n",
      "Epoch 1322/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -14732.7452 - g_loss: 6505353.6947\n",
      "Epoch 1323/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 38767.4752 - g_loss: 6595711.5263\n",
      "Epoch 1324/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 23608.4286 - g_loss: 6346195.9421\n",
      "Epoch 1325/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -8800.2806 - g_loss: 6724711.3263\n",
      "Epoch 1326/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 30668.0436 - g_loss: 7292956.2421\n",
      "Epoch 1327/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -64634.9868 - g_loss: 8715022.3842\n",
      "Epoch 1328/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 46194.9668 - g_loss: 8911833.6526\n",
      "Epoch 1329/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -7165.0617 - g_loss: 9094468.3474\n",
      "Epoch 1330/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 128922.7667 - g_loss: 9413631.2000\n",
      "Epoch 1331/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 45019.6532 - g_loss: 9569112.0842\n",
      "Epoch 1332/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -232045.5013 - g_loss: 8634942.4368\n",
      "Epoch 1333/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -29543.8600 - g_loss: 8179907.0211\n",
      "Epoch 1334/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -27088.6614 - g_loss: 8568779.6263\n",
      "Epoch 1335/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -51255.7486 - g_loss: 9298056.5158\n",
      "Epoch 1336/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -82590.2621 - g_loss: 9584157.1474\n",
      "Epoch 1337/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 6332.7615 - g_loss: 10212247.0421\n",
      "Epoch 1338/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 41223.0026 - g_loss: 10883173.3789\n",
      "Epoch 1339/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -113603.7109 - g_loss: 12475792.3789\n",
      "Epoch 1340/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 52175.1996 - g_loss: 14046965.1474\n",
      "Epoch 1341/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -44919.5450 - g_loss: 14148533.3684\n",
      "Epoch 1342/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 91188.5501 - g_loss: 13919683.2632\n",
      "Epoch 1343/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -11793.7467 - g_loss: 13679843.7474\n",
      "Epoch 1344/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 55348.8108 - g_loss: 13712062.8632\n",
      "Epoch 1345/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -17350.7505 - g_loss: 14937007.8316\n",
      "Epoch 1346/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 1404.0084 - g_loss: 13908890.2211\n",
      "Epoch 1347/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -32462.2274 - g_loss: 13060060.3053\n",
      "Epoch 1348/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 67259.3141 - g_loss: 13097676.0105\n",
      "Epoch 1349/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 33041.9678 - g_loss: 12989229.7053\n",
      "Epoch 1350/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -43211.4064 - g_loss: 14385200.6000\n",
      "Epoch 1351/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -26370.8777 - g_loss: 13284172.5684\n",
      "Epoch 1352/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -9167.4139 - g_loss: 12127115.9368\n",
      "Epoch 1353/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 76590.8758 - g_loss: 12168178.7368\n",
      "Epoch 1354/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -107106.7410 - g_loss: 12901368.1474\n",
      "Epoch 1355/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 97093.4570 - g_loss: 14602302.4842\n",
      "Epoch 1356/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -3985.4134 - g_loss: 13454843.3368\n",
      "Epoch 1357/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -173503.0147 - g_loss: 15099266.1053\n",
      "Epoch 1358/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -82856.8953 - g_loss: 15626942.1263\n",
      "Epoch 1359/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -18925.4532 - g_loss: 15886925.5053\n",
      "Epoch 1360/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 121269.2621 - g_loss: 15205436.4947\n",
      "Epoch 1361/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 101305.2417 - g_loss: 13770135.8000\n",
      "Epoch 1362/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 8662.0342 - g_loss: 13720570.6105\n",
      "Epoch 1363/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -62891.3473 - g_loss: 12412280.7789\n",
      "Epoch 1364/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -130730.3052 - g_loss: 13583892.5263\n",
      "Epoch 1365/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -73452.4664 - g_loss: 13150660.8421\n",
      "Epoch 1366/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 104202.6868 - g_loss: 12275688.4316\n",
      "Epoch 1367/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 22059.4955 - g_loss: 12234738.9789\n",
      "Epoch 1368/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 610.3241 - g_loss: 11170694.5895\n",
      "Epoch 1369/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 54501.2627 - g_loss: 11729176.1263\n",
      "Epoch 1370/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 19589.6292 - g_loss: 11247813.8421\n",
      "Epoch 1371/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -1681.1915 - g_loss: 11351483.7158\n",
      "Epoch 1372/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -46559.7162 - g_loss: 10809069.2421\n",
      "Epoch 1373/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 153783.1856 - g_loss: 10709303.2211\n",
      "Epoch 1374/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -25299.2698 - g_loss: 10483484.0421\n",
      "Epoch 1375/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -32827.0933 - g_loss: 9400180.9316\n",
      "Epoch 1376/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -64803.6506 - g_loss: 9406247.6684\n",
      "Epoch 1377/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -121633.5700 - g_loss: 10099517.6632\n",
      "Epoch 1378/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -112433.2016 - g_loss: 9101355.9737\n",
      "Epoch 1379/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -202900.5081 - g_loss: 8075043.2842\n",
      "Epoch 1380/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 39675.6984 - g_loss: 8190658.2053\n",
      "Epoch 1381/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -3971.7644 - g_loss: 8355723.1105\n",
      "Epoch 1382/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -25252.5864 - g_loss: 9069308.6632\n",
      "Epoch 1383/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 40566.5970 - g_loss: 8639954.2842\n",
      "Epoch 1384/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -55430.0973 - g_loss: 8221972.5421\n",
      "Epoch 1385/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 126399.9631 - g_loss: 7290863.0368\n",
      "Epoch 1386/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -32383.9016 - g_loss: 7407130.5211\n",
      "Epoch 1387/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 95005.5399 - g_loss: 7320485.8526\n",
      "Epoch 1388/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -47716.0843 - g_loss: 7128612.1684\n",
      "Epoch 1389/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 88074.0485 - g_loss: 6298188.5211\n",
      "Epoch 1390/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -4298.7926 - g_loss: 5721737.8579\n",
      "Epoch 1391/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -81864.3103 - g_loss: 6084438.2000\n",
      "Epoch 1392/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -21299.2867 - g_loss: 4752570.4368\n",
      "Epoch 1393/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -17596.9343 - g_loss: 3812274.4000\n",
      "Epoch 1394/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -60585.6381 - g_loss: 3447897.4395\n",
      "Epoch 1395/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 76641.6994 - g_loss: 3641779.7763\n",
      "Epoch 1396/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -39809.7755 - g_loss: 3204817.1303\n",
      "Epoch 1397/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -4731.6907 - g_loss: 2431545.9711\n",
      "Epoch 1398/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 37798.7035 - g_loss: 2899752.9395\n",
      "Epoch 1399/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 116952.1039 - g_loss: 2297545.2112\n",
      "Epoch 1400/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 71207.0430 - g_loss: 1664621.1250\n",
      "Epoch 1401/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -39422.2976 - g_loss: 1598339.3230\n",
      "Epoch 1402/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -52003.2325 - g_loss: 1711131.8276\n",
      "Epoch 1403/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -10715.5927 - g_loss: 1860426.0132\n",
      "Epoch 1404/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 60858.1322 - g_loss: 1807885.2658\n",
      "Epoch 1405/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -2781.7493 - g_loss: 2321463.4461\n",
      "Epoch 1406/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 78606.0596 - g_loss: 2645572.4895\n",
      "Epoch 1407/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 7298.2281 - g_loss: 2205678.1118\n",
      "Epoch 1408/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -91719.2188 - g_loss: 1785382.2829\n",
      "Epoch 1409/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 27143.4346 - g_loss: 1823164.1632\n",
      "Epoch 1410/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 132901.8342 - g_loss: 1885184.3658\n",
      "Epoch 1411/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -23286.4325 - g_loss: 1599284.6816\n",
      "Epoch 1412/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -58327.3123 - g_loss: 1133989.0431\n",
      "Epoch 1413/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -52693.1057 - g_loss: 582293.9032\n",
      "Epoch 1414/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -44498.6131 - g_loss: 275074.8811\n",
      "Epoch 1415/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 15550.3975 - g_loss: 170050.52082s - d_loss: -13485.677\n",
      "Epoch 1416/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 52165.6200 - g_loss: -152150.3422\n",
      "Epoch 1417/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 14911.6754 - g_loss: -332518.6817\n",
      "Epoch 1418/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -48349.8185 - g_loss: -382133.7396\n",
      "Epoch 1419/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 15590.4566 - g_loss: -630619.1410\n",
      "Epoch 1420/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -15075.8332 - g_loss: -902669.5819\n",
      "Epoch 1421/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -78618.9763 - g_loss: -1074382.7253\n",
      "Epoch 1422/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -40736.8291 - g_loss: -840383.0170\n",
      "Epoch 1423/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 31119.1164 - g_loss: -718649.8482\n",
      "Epoch 1424/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 27061.2479 - g_loss: -1083320.0112\n",
      "Epoch 1425/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 33294.4035 - g_loss: -956514.8834\n",
      "Epoch 1426/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 36327.9325 - g_loss: -1057750.8915\n",
      "Epoch 1427/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 58645.0520 - g_loss: -731704.8450\n",
      "Epoch 1428/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 54227.3422 - g_loss: -781144.9123\n",
      "Epoch 1429/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 104351.5205 - g_loss: -1016204.9520\n",
      "Epoch 1430/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 30659.7819 - g_loss: -792666.7165\n",
      "Epoch 1431/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -14724.5284 - g_loss: -699747.5525\n",
      "Epoch 1432/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -92484.2359 - g_loss: -413207.0907\n",
      "Epoch 1433/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -3905.8851 - g_loss: -239291.4859\n",
      "Epoch 1434/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -16903.5425 - g_loss: -106488.0206\n",
      "Epoch 1435/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -106297.9292 - g_loss: 155065.8706\n",
      "Epoch 1436/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 54576.0475 - g_loss: 103045.9728\n",
      "Epoch 1437/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 11450.4991 - g_loss: 284794.5399\n",
      "Epoch 1438/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 23244.3148 - g_loss: 516232.0304\n",
      "Epoch 1439/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 19032.3919 - g_loss: 432263.5828\n",
      "Epoch 1440/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -474.7076 - g_loss: 510917.6001\n",
      "Epoch 1441/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -40164.3506 - g_loss: 637482.9442\n",
      "Epoch 1442/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 4878.1762 - g_loss: 652208.3897\n",
      "Epoch 1443/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 25068.7700 - g_loss: 629935.8495\n",
      "Epoch 1444/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -4416.3131 - g_loss: -104298.1020\n",
      "Epoch 1445/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 13568.8366 - g_loss: 281790.9650\n",
      "Epoch 1446/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -65783.1614 - g_loss: 689024.5567\n",
      "Epoch 1447/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -3885.2470 - g_loss: 481615.9720\n",
      "Epoch 1448/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 50829.5554 - g_loss: 1033045.5763\n",
      "Epoch 1449/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -90898.7735 - g_loss: 1596633.3329\n",
      "Epoch 1450/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -91.2352 - g_loss: 2173139.6645\n",
      "Epoch 1451/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 72514.0247 - g_loss: 1861187.4895\n",
      "Epoch 1452/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 50501.1865 - g_loss: 1514268.0086\n",
      "Epoch 1453/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -7182.7460 - g_loss: 2090199.1329\n",
      "Epoch 1454/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -25864.5455 - g_loss: 2856371.0579\n",
      "Epoch 1455/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -1713.1272 - g_loss: 2618742.8566\n",
      "Epoch 1456/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 31616.0872 - g_loss: 2080746.4105\n",
      "Epoch 1457/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 61313.3549 - g_loss: 2286901.4276\n",
      "Epoch 1458/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 66674.6294 - g_loss: 2652705.5724\n",
      "Epoch 1459/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 10212.7320 - g_loss: 2417129.5921\n",
      "Epoch 1460/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -3910.3455 - g_loss: 2458889.3789\n",
      "Epoch 1461/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 7689.2257 - g_loss: 2509382.5671\n",
      "Epoch 1462/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -26959.6621 - g_loss: 2538784.1934\n",
      "Epoch 1463/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 33471.1748 - g_loss: 2920452.8368\n",
      "Epoch 1464/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 70871.2235 - g_loss: 2847759.7724\n",
      "Epoch 1465/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -39243.2748 - g_loss: 2545782.66322s - d_loss: -14282.1239 - g_los\n",
      "Epoch 1466/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 49477.9015 - g_loss: 2641299.5908\n",
      "Epoch 1467/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -7628.4611 - g_loss: 2775673.6789\n",
      "Epoch 1468/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 15932.6136 - g_loss: 2437835.3447\n",
      "Epoch 1469/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -43139.0552 - g_loss: 3428682.2500\n",
      "Epoch 1470/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 74251.8896 - g_loss: 3133855.8579\n",
      "Epoch 1471/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 62419.9826 - g_loss: 2816416.3684\n",
      "Epoch 1472/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -51851.1145 - g_loss: 2592525.2632\n",
      "Epoch 1473/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 34690.9939 - g_loss: 3105871.5974\n",
      "Epoch 1474/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 50144.6234 - g_loss: 3119879.9026\n",
      "Epoch 1475/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -58513.8203 - g_loss: 3190196.8158\n",
      "Epoch 1476/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 19107.6306 - g_loss: 3149694.0711\n",
      "Epoch 1477/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -35156.2792 - g_loss: 3221168.7211\n",
      "Epoch 1478/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -33095.4857 - g_loss: 3683108.6316\n",
      "Epoch 1479/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 17908.3306 - g_loss: 4147595.1658\n",
      "Epoch 1480/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -3117.1019 - g_loss: 4827819.5263\n",
      "Epoch 1481/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 130460.3107 - g_loss: 4789776.1342\n",
      "Epoch 1482/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 73224.2893 - g_loss: 4420247.6421\n",
      "Epoch 1483/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 11378.6057 - g_loss: 4411121.6000\n",
      "Epoch 1484/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 87582.3350 - g_loss: 4374203.4947\n",
      "Epoch 1485/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -8364.6953 - g_loss: 4128785.4684\n",
      "Epoch 1486/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -14296.1239 - g_loss: 4480345.4211\n",
      "Epoch 1487/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -18038.7513 - g_loss: 4371091.1026\n",
      "Epoch 1488/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 4491.5069 - g_loss: 4082672.4605\n",
      "Epoch 1489/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -32256.3333 - g_loss: 4121788.0474\n",
      "Epoch 1490/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -16463.8882 - g_loss: 4662648.6237\n",
      "Epoch 1491/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -41225.7168 - g_loss: 5023690.3053\n",
      "Epoch 1492/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 74833.4278 - g_loss: 4965621.9632\n",
      "Epoch 1493/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -19738.2410 - g_loss: 4904137.1053\n",
      "Epoch 1494/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -6922.6293 - g_loss: 4635990.1921\n",
      "Epoch 1495/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 22692.6127 - g_loss: 4974902.8737\n",
      "Epoch 1496/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -25334.8421 - g_loss: 4911988.8684\n",
      "Epoch 1497/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -69247.0414 - g_loss: 4524133.9921\n",
      "Epoch 1498/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 39716.3736 - g_loss: 4239000.4026\n",
      "Epoch 1499/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -28914.2607 - g_loss: 4112923.9763\n",
      "Epoch 1500/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -43843.3494 - g_loss: 4088772.0079\n",
      "Epoch 1501/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -51161.3169 - g_loss: 4412037.3895\n",
      "Epoch 1502/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -12491.4120 - g_loss: 4284709.1895\n",
      "Epoch 1503/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 18881.3053 - g_loss: 4202937.8026\n",
      "Epoch 1504/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -27385.0498 - g_loss: 4271422.4289\n",
      "Epoch 1505/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -15105.7785 - g_loss: 3645616.7316\n",
      "Epoch 1506/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -4425.6825 - g_loss: 2848763.2000\n",
      "Epoch 1507/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -40833.7225 - g_loss: 2115810.1500\n",
      "Epoch 1508/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -9403.3804 - g_loss: 1271702.7349\n",
      "Epoch 1509/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -8400.0254 - g_loss: 1550915.5559\n",
      "Epoch 1510/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 58476.2377 - g_loss: 1789638.2342\n",
      "Epoch 1511/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 14440.1698 - g_loss: 981801.9100\n",
      "Epoch 1512/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 26796.3700 - g_loss: 559865.8986\n",
      "Epoch 1513/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 1261.4032 - g_loss: 304896.3672\n",
      "Epoch 1514/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 40592.9993 - g_loss: 404093.1588\n",
      "Epoch 1515/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 5893.1985 - g_loss: -65804.6846\n",
      "Epoch 1516/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -67882.1747 - g_loss: -158263.4944\n",
      "Epoch 1517/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 19705.4393 - g_loss: -319750.6130\n",
      "Epoch 1518/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -37392.7106 - g_loss: -919690.4696\n",
      "Epoch 1519/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 19842.6899 - g_loss: -1320520.1355\n",
      "Epoch 1520/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 35013.7316 - g_loss: -982324.5033\n",
      "Epoch 1521/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -18055.9399 - g_loss: -1244071.6632\n",
      "Epoch 1522/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 16259.5317 - g_loss: -1565948.5368\n",
      "Epoch 1523/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 18160.7304 - g_loss: -1211928.4967\n",
      "Epoch 1524/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 2297.7912 - g_loss: -1395282.9724\n",
      "Epoch 1525/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 1328.9843 - g_loss: -1185227.3428\n",
      "Epoch 1526/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 35160.5310 - g_loss: -408566.6851\n",
      "Epoch 1527/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 13431.4757 - g_loss: 176611.9529\n",
      "Epoch 1528/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 28725.0696 - g_loss: 713264.1122\n",
      "Epoch 1529/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -29980.5808 - g_loss: 592351.4794\n",
      "Epoch 1530/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: -15921.8042 - g_loss: 472223.7629\n",
      "Epoch 1531/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -10030.6708 - g_loss: 635890.1972\n",
      "Epoch 1532/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 46865.3374 - g_loss: 694787.5082\n",
      "Epoch 1533/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 35736.4519 - g_loss: 1457029.6770\n",
      "Epoch 1534/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5535.9641 - g_loss: 2248447.4474\n",
      "Epoch 1535/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 53361.5933 - g_loss: 3335629.7421\n",
      "Epoch 1536/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 11987.7505 - g_loss: 3585657.6842\n",
      "Epoch 1537/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 18556.0100 - g_loss: 3588177.4395\n",
      "Epoch 1538/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -7791.2756 - g_loss: 3440280.5211\n",
      "Epoch 1539/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 58963.5474 - g_loss: 3230144.2289\n",
      "Epoch 1540/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 47184.3841 - g_loss: 2967214.1934\n",
      "Epoch 1541/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -43093.2954 - g_loss: 2125766.7171\n",
      "Epoch 1542/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -111900.9493 - g_loss: 3606437.0000\n",
      "Epoch 1543/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -57402.8074 - g_loss: 4189625.3289\n",
      "Epoch 1544/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 2728.3384 - g_loss: 4012665.6500\n",
      "Epoch 1545/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 8392.0492 - g_loss: 3358637.0368\n",
      "Epoch 1546/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 11320.7852 - g_loss: 3972842.6447\n",
      "Epoch 1547/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 13068.1317 - g_loss: 2926188.0763\n",
      "Epoch 1548/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 53830.4531 - g_loss: 2335641.7447\n",
      "Epoch 1549/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 25704.3388 - g_loss: 2384290.8816\n",
      "Epoch 1550/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -81141.8398 - g_loss: 3026720.7632\n",
      "Epoch 1551/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -22874.3978 - g_loss: 4056241.8553\n",
      "Epoch 1552/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 85820.9981 - g_loss: 4053852.6132\n",
      "Epoch 1553/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -69842.6990 - g_loss: 3673974.7368\n",
      "Epoch 1554/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 10754.7946 - g_loss: 3886569.9500\n",
      "Epoch 1555/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -24601.8818 - g_loss: 3762423.1368\n",
      "Epoch 1556/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -5165.7618 - g_loss: 4044046.7237\n",
      "Epoch 1557/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -30742.2774 - g_loss: 5087588.8053\n",
      "Epoch 1558/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 68178.3799 - g_loss: 5602902.9053\n",
      "Epoch 1559/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -54376.5720 - g_loss: 6212557.4316\n",
      "Epoch 1560/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -107108.9557 - g_loss: 5480712.6632\n",
      "Epoch 1561/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 34688.6736 - g_loss: 5186611.8421\n",
      "Epoch 1562/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 6330.8428 - g_loss: 4749807.4684\n",
      "Epoch 1563/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -61502.6455 - g_loss: 5103751.8289\n",
      "Epoch 1564/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -51540.4859 - g_loss: 4980488.5789\n",
      "Epoch 1565/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -3958.5225 - g_loss: 4956111.1053\n",
      "Epoch 1566/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -97542.5663 - g_loss: 4990695.6605\n",
      "Epoch 1567/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -67477.3434 - g_loss: 5092602.1553\n",
      "Epoch 1568/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 51893.9703 - g_loss: 4495742.9105\n",
      "Epoch 1569/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 90198.9675 - g_loss: 4252389.4605\n",
      "Epoch 1570/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 48809.7200 - g_loss: 4080693.6526\n",
      "Epoch 1571/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -17687.5807 - g_loss: 3577074.6526\n",
      "Epoch 1572/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -24208.3819 - g_loss: 3085993.3421\n",
      "Epoch 1573/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 136723.3793 - g_loss: 2238679.6026\n",
      "Epoch 1574/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -55781.6135 - g_loss: 1087006.5595\n",
      "Epoch 1575/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -71367.6615 - g_loss: 1256489.5724\n",
      "Epoch 1576/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 20554.6745 - g_loss: 1426943.1422\n",
      "Epoch 1577/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 61329.5472 - g_loss: 772142.0483\n",
      "Epoch 1578/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 6202.7650 - g_loss: 1026512.5586\n",
      "Epoch 1579/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -22817.7506 - g_loss: 2033565.8816\n",
      "Epoch 1580/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -35072.8413 - g_loss: 1454854.9289\n",
      "Epoch 1581/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 18445.0538 - g_loss: 2225137.4579\n",
      "Epoch 1582/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 2885.5593 - g_loss: 1651526.6862\n",
      "Epoch 1583/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 17156.9144 - g_loss: -19480.8133\n",
      "Epoch 1584/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -49193.4176 - g_loss: -132598.9188\n",
      "Epoch 1585/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -174.0495 - g_loss: -209624.1998\n",
      "Epoch 1586/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 16836.1596 - g_loss: 129296.7148\n",
      "Epoch 1587/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 21508.9034 - g_loss: 290373.7958\n",
      "Epoch 1588/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -7766.4630 - g_loss: 871796.2254\n",
      "Epoch 1589/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -24383.2444 - g_loss: 1736222.5020\n",
      "Epoch 1590/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 13364.1394 - g_loss: 759591.9693\n",
      "Epoch 1591/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -81667.3485 - g_loss: 1009158.8632\n",
      "Epoch 1592/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -67836.1609 - g_loss: 1675486.4118\n",
      "Epoch 1593/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 66355.7970 - g_loss: 1447524.0553\n",
      "Epoch 1594/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 869.3274 - g_loss: 2239447.8158\n",
      "Epoch 1595/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 82988.5570 - g_loss: 3623181.2553\n",
      "Epoch 1596/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 44279.5081 - g_loss: 3124419.2447\n",
      "Epoch 1597/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 31120.9077 - g_loss: 2740543.1237\n",
      "Epoch 1598/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -44617.2353 - g_loss: 3440637.5447\n",
      "Epoch 1599/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -73243.5067 - g_loss: 2960635.7329\n",
      "Epoch 1600/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -29407.9250 - g_loss: 1387061.5604\n",
      "Epoch 1601/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 1839.5801 - g_loss: 1882529.0664\n",
      "Epoch 1602/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 15647.3155 - g_loss: 2034763.4934\n",
      "Epoch 1603/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -5883.5022 - g_loss: 2312582.4434\n",
      "Epoch 1604/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 30133.5038 - g_loss: 3269939.5342\n",
      "Epoch 1605/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 1682.7911 - g_loss: 3133057.2566\n",
      "Epoch 1606/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 3948.8420 - g_loss: 2780881.5158\n",
      "Epoch 1607/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -58857.0521 - g_loss: 3260734.5395\n",
      "Epoch 1608/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -40248.9566 - g_loss: 3016117.8566\n",
      "Epoch 1609/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 8285.6079 - g_loss: 2778819.5987\n",
      "Epoch 1610/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 41969.3719 - g_loss: 2748448.7250\n",
      "Epoch 1611/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 97673.3293 - g_loss: 1922632.1230\n",
      "Epoch 1612/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -42503.6971 - g_loss: 2877158.0092\n",
      "Epoch 1613/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -117411.3317 - g_loss: 4351745.5316\n",
      "Epoch 1614/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -41331.2977 - g_loss: 5235505.2895\n",
      "Epoch 1615/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 82414.8347 - g_loss: 5186418.7789\n",
      "Epoch 1616/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -46074.0545 - g_loss: 6079855.5000\n",
      "Epoch 1617/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 54762.7002 - g_loss: 7378018.5421\n",
      "Epoch 1618/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -10147.4176 - g_loss: 7502770.8158\n",
      "Epoch 1619/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 9010.9822 - g_loss: 7919312.8105\n",
      "Epoch 1620/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -25216.1582 - g_loss: 8609887.5421\n",
      "Epoch 1621/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -97687.0234 - g_loss: 8277299.6211\n",
      "Epoch 1622/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -126168.1185 - g_loss: 8005620.0158\n",
      "Epoch 1623/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 39663.2407 - g_loss: 7188397.4789\n",
      "Epoch 1624/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 42228.9152 - g_loss: 6542443.1632\n",
      "Epoch 1625/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -43487.7631 - g_loss: 5405845.3421\n",
      "Epoch 1626/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -29791.5643 - g_loss: 4101059.89475s - d_l\n",
      "Epoch 1627/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -28368.2015 - g_loss: 4346240.9316\n",
      "Epoch 1628/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 92282.9226 - g_loss: 4694759.9658\n",
      "Epoch 1629/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -73545.7733 - g_loss: 5208958.8368\n",
      "Epoch 1630/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -32858.4532 - g_loss: 5235167.6237\n",
      "Epoch 1631/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 116877.8484 - g_loss: 3888592.8474\n",
      "Epoch 1632/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -73991.5095 - g_loss: 4236705.8553\n",
      "Epoch 1633/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -83402.9764 - g_loss: 5482528.5842\n",
      "Epoch 1634/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -143404.4313 - g_loss: 6554481.0053\n",
      "Epoch 1635/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -95244.1405 - g_loss: 7979675.9842\n",
      "Epoch 1636/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -69122.2009 - g_loss: 7837668.4684\n",
      "Epoch 1637/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 31011.6404 - g_loss: 7202575.6842\n",
      "Epoch 1638/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -136670.1990 - g_loss: 6372444.4526\n",
      "Epoch 1639/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -112319.4451 - g_loss: 7126577.4053\n",
      "Epoch 1640/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -68947.8816 - g_loss: 7527289.1474\n",
      "Epoch 1641/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 93329.2265 - g_loss: 7324540.1263s - d_loss: 89076.0816 - g_lo\n",
      "Epoch 1642/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 170651.4574 - g_loss: 7457856.7947\n",
      "Epoch 1643/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -86608.8451 - g_loss: 8066834.6684\n",
      "Epoch 1644/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -15444.6202 - g_loss: 8287134.1368\n",
      "Epoch 1645/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -110979.8807 - g_loss: 8675501.4000\n",
      "Epoch 1646/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -13564.2244 - g_loss: 8900633.8053\n",
      "Epoch 1647/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -10683.9441 - g_loss: 9067700.6105\n",
      "Epoch 1648/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 67420.7907 - g_loss: 8160750.8842\n",
      "Epoch 1649/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -199650.6978 - g_loss: 9195253.8526\n",
      "Epoch 1650/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -18473.7588 - g_loss: 9904565.6842\n",
      "Epoch 1651/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 8478.7840 - g_loss: 11059836.6737\n",
      "Epoch 1652/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -35966.4379 - g_loss: 10618088.1263\n",
      "Epoch 1653/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 57109.9112 - g_loss: 13186368.8211\n",
      "Epoch 1654/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -86407.2597 - g_loss: 13033428.1789\n",
      "Epoch 1655/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -131522.1480 - g_loss: 13385767.5368\n",
      "Epoch 1656/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 51989.9853 - g_loss: 12244764.7368\n",
      "Epoch 1657/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 98522.7388 - g_loss: 11562495.0947\n",
      "Epoch 1658/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -15823.6490 - g_loss: 10831157.0105\n",
      "Epoch 1659/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -108059.3821 - g_loss: 9382948.5316\n",
      "Epoch 1660/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -16087.0440 - g_loss: 8739337.4316\n",
      "Epoch 1661/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 94655.8614 - g_loss: 8955719.7316\n",
      "Epoch 1662/2000\n",
      "94/94 [==============================] - 15s 163ms/step - d_loss: 45427.0616 - g_loss: 10011286.7105\n",
      "Epoch 1663/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 212882.8666 - g_loss: 8599445.4737\n",
      "Epoch 1664/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 244456.6385 - g_loss: 6808143.2789\n",
      "Epoch 1665/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 109108.4415 - g_loss: 7095282.7842\n",
      "Epoch 1666/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 53407.6269 - g_loss: 7815427.0632\n",
      "Epoch 1667/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -486.0697 - g_loss: 7138880.7947\n",
      "Epoch 1668/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -89632.8607 - g_loss: 5624606.0158\n",
      "Epoch 1669/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -20338.3006 - g_loss: 5918570.6684\n",
      "Epoch 1670/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 69223.7259 - g_loss: 6600480.7421\n",
      "Epoch 1671/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 64671.3901 - g_loss: 6415920.1158\n",
      "Epoch 1672/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 18739.1827 - g_loss: 7078441.8632\n",
      "Epoch 1673/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -100419.1324 - g_loss: 7328377.5158\n",
      "Epoch 1674/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 170690.1623 - g_loss: 6776851.9316\n",
      "Epoch 1675/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 36284.6781 - g_loss: 7160181.5842\n",
      "Epoch 1676/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -52513.8577 - g_loss: 7138502.7000\n",
      "Epoch 1677/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -5318.3457 - g_loss: 6440764.3737\n",
      "Epoch 1678/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 17484.4108 - g_loss: 6045745.8158\n",
      "Epoch 1679/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 100506.2216 - g_loss: 4959422.5974\n",
      "Epoch 1680/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -24437.3490 - g_loss: 5701079.0368\n",
      "Epoch 1681/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 27695.8366 - g_loss: 7417367.3789\n",
      "Epoch 1682/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -110448.9859 - g_loss: 8021600.1842\n",
      "Epoch 1683/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 85487.9857 - g_loss: 7404631.9000 2s - d_loss: 1886\n",
      "Epoch 1684/2000\n",
      "94/94 [==============================] - 15s 163ms/step - d_loss: 12190.5827 - g_loss: 7858393.5842\n",
      "Epoch 1685/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 120921.3831 - g_loss: 7008299.9421\n",
      "Epoch 1686/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -163058.2653 - g_loss: 5310871.6632\n",
      "Epoch 1687/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -12503.3550 - g_loss: 6174822.1105\n",
      "Epoch 1688/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -28682.5278 - g_loss: 5816284.8053\n",
      "Epoch 1689/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -13004.8546 - g_loss: 5356362.3658\n",
      "Epoch 1690/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -19954.6351 - g_loss: 4781574.7500\n",
      "Epoch 1691/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -95493.9835 - g_loss: 5089748.1816\n",
      "Epoch 1692/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 6742.5767 - g_loss: 4644295.7079\n",
      "Epoch 1693/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -7105.1144 - g_loss: 4310004.8658\n",
      "Epoch 1694/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 89993.9565 - g_loss: 2854554.5118\n",
      "Epoch 1695/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -154136.7964 - g_loss: 2858202.4789\n",
      "Epoch 1696/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -62550.0334 - g_loss: 3062229.4158\n",
      "Epoch 1697/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -28894.3157 - g_loss: 2870205.4066\n",
      "Epoch 1698/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -53258.0443 - g_loss: 2850304.3895\n",
      "Epoch 1699/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -77477.8808 - g_loss: 4128276.4447\n",
      "Epoch 1700/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -40582.2125 - g_loss: 4301453.0816\n",
      "Epoch 1701/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -102389.9517 - g_loss: 4256579.5105\n",
      "Epoch 1702/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -38989.0189 - g_loss: 3539984.1211\n",
      "Epoch 1703/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -67522.5604 - g_loss: 4908598.5158\n",
      "Epoch 1704/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -23344.0457 - g_loss: 5646092.9000\n",
      "Epoch 1705/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 210336.6360 - g_loss: 5500779.6579\n",
      "Epoch 1706/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 19562.6926 - g_loss: 5581682.5000\n",
      "Epoch 1707/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -56366.0248 - g_loss: 5869050.8500\n",
      "Epoch 1708/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -39722.3982 - g_loss: 5650468.9789\n",
      "Epoch 1709/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -23595.0538 - g_loss: 5954514.6000\n",
      "Epoch 1710/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -45808.0484 - g_loss: 4730712.4316\n",
      "Epoch 1711/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -61115.8975 - g_loss: 3444977.7763\n",
      "Epoch 1712/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -197405.4495 - g_loss: 3509692.6395\n",
      "Epoch 1713/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 18601.0581 - g_loss: 3688091.0395\n",
      "Epoch 1714/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 41861.1250 - g_loss: 4171919.1263\n",
      "Epoch 1715/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 118066.6781 - g_loss: 3981617.8447\n",
      "Epoch 1716/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -28048.6229 - g_loss: 3582183.1974\n",
      "Epoch 1717/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -146288.2107 - g_loss: 2876290.0987\n",
      "Epoch 1718/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -9735.6378 - g_loss: 3043331.4382\n",
      "Epoch 1719/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -25910.8069 - g_loss: 2639683.7421\n",
      "Epoch 1720/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 77234.4815 - g_loss: 1362222.8541\n",
      "Epoch 1721/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -54876.3366 - g_loss: 982559.0557\n",
      "Epoch 1722/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -119967.0163 - g_loss: 601284.0127\n",
      "Epoch 1723/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -27723.5648 - g_loss: -39302.0996\n",
      "Epoch 1724/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 32636.8831 - g_loss: 491102.3628\n",
      "Epoch 1725/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 59240.7519 - g_loss: 663443.6981\n",
      "Epoch 1726/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -245.0247 - g_loss: 637079.1561\n",
      "Epoch 1727/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 156324.2650 - g_loss: 1085624.8410\n",
      "Epoch 1728/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -13812.6270 - g_loss: 1601873.7896\n",
      "Epoch 1729/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: 38386.4972 - g_loss: 1057959.9956\n",
      "Epoch 1730/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -15389.2512 - g_loss: 564741.2135\n",
      "Epoch 1731/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -46550.4987 - g_loss: 457983.2981\n",
      "Epoch 1732/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 47925.8556 - g_loss: 388665.5532\n",
      "Epoch 1733/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -17927.8766 - g_loss: -12051.7143\n",
      "Epoch 1734/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 26081.7332 - g_loss: -106811.7074\n",
      "Epoch 1735/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -8082.3329 - g_loss: 384775.2748\n",
      "Epoch 1736/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -85316.0909 - g_loss: 1643983.1211\n",
      "Epoch 1737/2000\n",
      "94/94 [==============================] - 16s 174ms/step - d_loss: -63337.4413 - g_loss: 1776150.1697\n",
      "Epoch 1738/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -258964.7773 - g_loss: 2018730.8839\n",
      "Epoch 1739/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 16593.2764 - g_loss: 3266930.8553\n",
      "Epoch 1740/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 58662.0209 - g_loss: 1821584.8622\n",
      "Epoch 1741/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -93179.8701 - g_loss: 1173872.3454\n",
      "Epoch 1742/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -146464.2846 - g_loss: 971929.7245\n",
      "Epoch 1743/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 19850.9945 - g_loss: 833421.2593\n",
      "Epoch 1744/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 108046.6562 - g_loss: -88412.6054\n",
      "Epoch 1745/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -337825.7709 - g_loss: 1779604.3942\n",
      "Epoch 1746/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 10538.2571 - g_loss: 2688132.0789\n",
      "Epoch 1747/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 67626.3075 - g_loss: 2204720.4143\n",
      "Epoch 1748/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -113574.0200 - g_loss: 2190378.2224\n",
      "Epoch 1749/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -117230.9123 - g_loss: 2079609.5484\n",
      "Epoch 1750/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 178536.6674 - g_loss: 2551207.65994s - d_lo\n",
      "Epoch 1751/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -107256.1952 - g_loss: 3033584.5211\n",
      "Epoch 1752/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -211957.3590 - g_loss: 3584696.4961\n",
      "Epoch 1753/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 42397.1747 - g_loss: 3147515.6520\n",
      "Epoch 1754/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 220671.6524 - g_loss: 2364711.0658\n",
      "Epoch 1755/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -239237.4244 - g_loss: 3600724.9921\n",
      "Epoch 1756/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -137905.0480 - g_loss: 3133590.6309\n",
      "Epoch 1757/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 141385.6252 - g_loss: 2629950.7934\n",
      "Epoch 1758/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -288024.9834 - g_loss: 2974571.3112\n",
      "Epoch 1759/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -66630.1320 - g_loss: 3033694.5664\n",
      "Epoch 1760/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -105038.9032 - g_loss: 356604.0512\n",
      "Epoch 1761/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -92185.0415 - g_loss: -1130658.3476\n",
      "Epoch 1762/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -47723.2697 - g_loss: -2301790.9158\n",
      "Epoch 1763/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 27914.7946 - g_loss: -2382401.8882\n",
      "Epoch 1764/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -8353.8028 - g_loss: -1574386.5832\n",
      "Epoch 1765/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 192767.0194 - g_loss: -1771026.5053\n",
      "Epoch 1766/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 76861.7519 - g_loss: -1842097.9250\n",
      "Epoch 1767/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -138858.9098 - g_loss: -2988021.8730\n",
      "Epoch 1768/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 128302.5966 - g_loss: -5180952.9763\n",
      "Epoch 1769/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -23790.8915 - g_loss: -6037887.3579\n",
      "Epoch 1770/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 128309.3935 - g_loss: -7289042.8789\n",
      "Epoch 1771/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: 51789.1522 - g_loss: -6737071.1789\n",
      "Epoch 1772/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 43520.1302 - g_loss: -6921015.3579\n",
      "Epoch 1773/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -74110.5572 - g_loss: -4937345.8842\n",
      "Epoch 1774/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 90302.9013 - g_loss: -5011481.2895\n",
      "Epoch 1775/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -103578.5592 - g_loss: -5203101.0447\n",
      "Epoch 1776/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -9236.2414 - g_loss: -3922174.8776\n",
      "Epoch 1777/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 77351.3895 - g_loss: -4252545.9303\n",
      "Epoch 1778/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -63710.7425 - g_loss: -4323144.0408\n",
      "Epoch 1779/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -95917.7517 - g_loss: -4464935.1717\n",
      "Epoch 1780/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -27774.2163 - g_loss: -5679680.7711\n",
      "Epoch 1781/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -71424.5153 - g_loss: -7605878.4474\n",
      "Epoch 1782/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -706.8052 - g_loss: -5835567.4132\n",
      "Epoch 1783/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -60258.2597 - g_loss: -6886444.7447\n",
      "Epoch 1784/2000\n",
      "94/94 [==============================] - 25s 266ms/step - d_loss: 95956.2699 - g_loss: -7644678.2105\n",
      "Epoch 1785/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 19741.8131 - g_loss: -6103652.4026\n",
      "Epoch 1786/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 232219.8065 - g_loss: -7170645.7368\n",
      "Epoch 1787/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 107497.8910 - g_loss: -7284666.1474\n",
      "Epoch 1788/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -63422.1360 - g_loss: -9141778.1684\n",
      "Epoch 1789/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 136853.4154 - g_loss: -9184799.7526\n",
      "Epoch 1790/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 76172.9464 - g_loss: -8558092.0579\n",
      "Epoch 1791/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -16278.1480 - g_loss: -7742572.4895\n",
      "Epoch 1792/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -64412.7813 - g_loss: -9169112.7579\n",
      "Epoch 1793/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 397624.3906 - g_loss: -9614955.6526\n",
      "Epoch 1794/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 26724.2973 - g_loss: -9579650.8368\n",
      "Epoch 1795/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 169670.9235 - g_loss: -10055747.2737\n",
      "Epoch 1796/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -90986.9001 - g_loss: -10044715.3526\n",
      "Epoch 1797/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 19809.4725 - g_loss: -10524148.3421\n",
      "Epoch 1798/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 293672.8657 - g_loss: -10550027.5684\n",
      "Epoch 1799/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 62323.2769 - g_loss: -11534936.8737\n",
      "Epoch 1800/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 118632.5338 - g_loss: -10303034.1421\n",
      "Epoch 1801/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -56472.8767 - g_loss: -10174490.6842\n",
      "Epoch 1802/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -101359.9251 - g_loss: -10913486.6211\n",
      "Epoch 1803/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -83295.4755 - g_loss: -11176332.6421\n",
      "Epoch 1804/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -229639.9199 - g_loss: -11902338.5368\n",
      "Epoch 1805/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -20584.1449 - g_loss: -12809448.9474\n",
      "Epoch 1806/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 4917.0348 - g_loss: -13404980.6737\n",
      "Epoch 1807/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 23523.8943 - g_loss: -13726693.2632\n",
      "Epoch 1808/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -48505.9114 - g_loss: -13790012.9263\n",
      "Epoch 1809/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -30264.9344 - g_loss: -13115784.8421\n",
      "Epoch 1810/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -158113.5420 - g_loss: -11421738.6316\n",
      "Epoch 1811/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 90303.9874 - g_loss: -8717920.1526\n",
      "Epoch 1812/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -5567.9863 - g_loss: -8795526.4947\n",
      "Epoch 1813/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -80482.5475 - g_loss: -8833742.0474\n",
      "Epoch 1814/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 100916.1773 - g_loss: -8652778.2211\n",
      "Epoch 1815/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 9803.4468 - g_loss: -6322324.2368\n",
      "Epoch 1816/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -5262.2368 - g_loss: -5175209.6711\n",
      "Epoch 1817/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -49224.2344 - g_loss: -4034921.5776\n",
      "Epoch 1818/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -1367.3078 - g_loss: -4447688.4842\n",
      "Epoch 1819/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -83272.5759 - g_loss: -4798365.3145\n",
      "Epoch 1820/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 52607.0144 - g_loss: -4823460.8816\n",
      "Epoch 1821/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -59255.6383 - g_loss: -4418153.7368\n",
      "Epoch 1822/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -31516.7380 - g_loss: -4663552.1447\n",
      "Epoch 1823/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -36551.7641 - g_loss: -4727351.4132\n",
      "Epoch 1824/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -72167.5180 - g_loss: -4476714.7171\n",
      "Epoch 1825/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 45512.3565 - g_loss: -5292872.8158\n",
      "Epoch 1826/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 41932.3883 - g_loss: -4135481.6605\n",
      "Epoch 1827/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -121668.4787 - g_loss: -3963446.8342\n",
      "Epoch 1828/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -3498.5647 - g_loss: -2299312.4905\n",
      "Epoch 1829/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 89106.2979 - g_loss: -1053331.5625\n",
      "Epoch 1830/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 71863.4989 - g_loss: -477047.1429\n",
      "Epoch 1831/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 87391.0146 - g_loss: -904714.6366s - d_loss: 51896.8471 - g_loss: -8\n",
      "Epoch 1832/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -90445.7694 - g_loss: 92378.2131\n",
      "Epoch 1833/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 116427.7919 - g_loss: -548786.3819\n",
      "Epoch 1834/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 22062.1914 - g_loss: -434755.6443\n",
      "Epoch 1835/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 159713.4765 - g_loss: 150937.2827\n",
      "Epoch 1836/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 213335.7326 - g_loss: -716047.3975\n",
      "Epoch 1837/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -11554.5788 - g_loss: 267495.5431\n",
      "Epoch 1838/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -22444.2506 - g_loss: 486426.3914\n",
      "Epoch 1839/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 77180.5419 - g_loss: 1055997.7206\n",
      "Epoch 1840/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -72690.0236 - g_loss: 1639864.6934\n",
      "Epoch 1841/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 46183.6347 - g_loss: 3053671.7260\n",
      "Epoch 1842/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 79506.6249 - g_loss: 1853627.2368\n",
      "Epoch 1843/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -27730.5680 - g_loss: 965331.0689\n",
      "Epoch 1844/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 50309.4561 - g_loss: 1212759.2128\n",
      "Epoch 1845/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 87960.9706 - g_loss: 3893054.1546\n",
      "Epoch 1846/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -192985.0126 - g_loss: 5891773.5053\n",
      "Epoch 1847/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 3740.2961 - g_loss: 5547630.6789\n",
      "Epoch 1848/2000\n",
      "94/94 [==============================] - 16s 165ms/step - d_loss: -106001.0161 - g_loss: 6153560.9895\n",
      "Epoch 1849/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -17159.3785 - g_loss: 6571807.6684\n",
      "Epoch 1850/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 103911.0183 - g_loss: 6253568.6789\n",
      "Epoch 1851/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 27058.8576 - g_loss: 5799376.4342\n",
      "Epoch 1852/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 6511.6014 - g_loss: 5043170.7447\n",
      "Epoch 1853/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 116453.7330 - g_loss: 6398454.5211\n",
      "Epoch 1854/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -165834.6246 - g_loss: 5595569.5026\n",
      "Epoch 1855/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 40538.2672 - g_loss: 6285046.1868\n",
      "Epoch 1856/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 149135.3675 - g_loss: 6859953.5737\n",
      "Epoch 1857/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -112568.1628 - g_loss: 8208418.4947\n",
      "Epoch 1858/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -91244.7506 - g_loss: 8750558.4000\n",
      "Epoch 1859/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 137472.9450 - g_loss: 9465387.9632\n",
      "Epoch 1860/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 55026.7053 - g_loss: 9593310.2789\n",
      "Epoch 1861/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -96149.6419 - g_loss: 8112744.2158\n",
      "Epoch 1862/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -20195.3649 - g_loss: 7924697.9526\n",
      "Epoch 1863/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -89975.0989 - g_loss: 9731989.0579\n",
      "Epoch 1864/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -3893.2238 - g_loss: 11570047.3579\n",
      "Epoch 1865/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -4167.4275 - g_loss: 13538928.1579\n",
      "Epoch 1866/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -123141.6414 - g_loss: 12075576.9895\n",
      "Epoch 1867/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 143198.7148 - g_loss: 11488845.7263\n",
      "Epoch 1868/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -867.3593 - g_loss: 11017863.7263\n",
      "Epoch 1869/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -83556.2561 - g_loss: 10521446.3895\n",
      "Epoch 1870/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -140608.9329 - g_loss: 9696236.2263\n",
      "Epoch 1871/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -50757.1109 - g_loss: 7931788.8316\n",
      "Epoch 1872/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 114239.9022 - g_loss: 8640350.9526\n",
      "Epoch 1873/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 83349.0782 - g_loss: 7933006.5579\n",
      "Epoch 1874/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 101480.9756 - g_loss: 7399105.3421\n",
      "Epoch 1875/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 20023.7870 - g_loss: 6456999.4184\n",
      "Epoch 1876/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -129834.8660 - g_loss: 6498337.9737\n",
      "Epoch 1877/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 277811.2344 - g_loss: 5175927.3145\n",
      "Epoch 1878/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 222265.1165 - g_loss: 3806609.5737\n",
      "Epoch 1879/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 217070.1250 - g_loss: 3747839.1546\n",
      "Epoch 1880/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 56528.9737 - g_loss: 5923323.2658\n",
      "Epoch 1881/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -144830.2745 - g_loss: 5728747.4579\n",
      "Epoch 1882/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -61955.0821 - g_loss: 6521436.4000\n",
      "Epoch 1883/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 37106.1819 - g_loss: 6009514.1079\n",
      "Epoch 1884/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 127630.5049 - g_loss: 7361771.6184\n",
      "Epoch 1885/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -87838.7749 - g_loss: 6099876.1658\n",
      "Epoch 1886/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 286369.8013 - g_loss: 6395627.8289\n",
      "Epoch 1887/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 100546.7924 - g_loss: 6904224.3842\n",
      "Epoch 1888/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 337996.1475 - g_loss: 7433883.6316\n",
      "Epoch 1889/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 246614.6319 - g_loss: 8618456.0947\n",
      "Epoch 1890/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -149315.8617 - g_loss: 9576443.2684\n",
      "Epoch 1891/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -105641.4387 - g_loss: 11253631.5895\n",
      "Epoch 1892/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -124921.7200 - g_loss: 9485162.6211\n",
      "Epoch 1893/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -25661.8380 - g_loss: 9625784.2737\n",
      "Epoch 1894/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 227657.2975 - g_loss: 10182417.5211\n",
      "Epoch 1895/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 110530.3054 - g_loss: 10631366.6684\n",
      "Epoch 1896/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -80862.1523 - g_loss: 11561107.1526\n",
      "Epoch 1897/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 9368.3185 - g_loss: 14358006.7579\n",
      "Epoch 1898/2000\n",
      "94/94 [==============================] - 16s 173ms/step - d_loss: 259452.9903 - g_loss: 14657821.7053\n",
      "Epoch 1899/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -248049.1239 - g_loss: 15904447.3053\n",
      "Epoch 1900/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 75533.0376 - g_loss: 16011986.1684\n",
      "Epoch 1901/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 71338.7827 - g_loss: 15447780.3368\n",
      "Epoch 1902/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -304353.6318 - g_loss: 15556677.1684\n",
      "Epoch 1903/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -162574.7803 - g_loss: 13358304.3368\n",
      "Epoch 1904/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -239831.3604 - g_loss: 11152764.0737\n",
      "Epoch 1905/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 20844.7753 - g_loss: 12877475.6000\n",
      "Epoch 1906/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 211183.4061 - g_loss: 12052468.8737\n",
      "Epoch 1907/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 75794.5588 - g_loss: 10379779.9211\n",
      "Epoch 1908/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -91241.9465 - g_loss: 9089677.6579\n",
      "Epoch 1909/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 341952.7441 - g_loss: 6422847.7842\n",
      "Epoch 1910/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -350192.8535 - g_loss: 4804465.7434\n",
      "Epoch 1911/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 34993.0128 - g_loss: 3912381.4211\n",
      "Epoch 1912/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -17840.4390 - g_loss: 1773718.2145\n",
      "Epoch 1913/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -244800.1508 - g_loss: 1883430.4493\n",
      "Epoch 1914/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -38740.9257 - g_loss: 2601341.7592\n",
      "Epoch 1915/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -190565.1708 - g_loss: 1699285.6526\n",
      "Epoch 1916/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -63582.3837 - g_loss: 1645348.7184\n",
      "Epoch 1917/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 275344.5824 - g_loss: 1498349.8911\n",
      "Epoch 1918/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 200539.1889 - g_loss: 416848.7640\n",
      "Epoch 1919/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -269905.3854 - g_loss: -2114149.2335\n",
      "Epoch 1920/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 71605.7445 - g_loss: -3049365.8941\n",
      "Epoch 1921/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 58186.7090 - g_loss: -3805430.4237\n",
      "Epoch 1922/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 5232.9671 - g_loss: -4084441.1342\n",
      "Epoch 1923/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 201964.1931 - g_loss: -5262427.3000\n",
      "Epoch 1924/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 45127.1971 - g_loss: -5914260.1658\n",
      "Epoch 1925/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -465897.7180 - g_loss: -4927115.4224\n",
      "Epoch 1926/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 13089.5739 - g_loss: -3807185.3803\n",
      "Epoch 1927/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 3856.5960 - g_loss: -3709231.7868\n",
      "Epoch 1928/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 272944.4933 - g_loss: -3407987.8533\n",
      "Epoch 1929/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 92273.6893 - g_loss: -2969951.2967\n",
      "Epoch 1930/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -110721.9109 - g_loss: -2745289.6201\n",
      "Epoch 1931/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 273286.0913 - g_loss: -3261862.6342\n",
      "Epoch 1932/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -70408.5986 - g_loss: -2982811.9105\n",
      "Epoch 1933/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -24859.5973 - g_loss: -2553165.9248s - d_loss: 18977.5194 - g_\n",
      "Epoch 1934/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -50552.5885 - g_loss: -1966345.2597\n",
      "Epoch 1935/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -76128.5711 - g_loss: -1539756.1459\n",
      "Epoch 1936/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -97508.5299 - g_loss: -1839721.8811\n",
      "Epoch 1937/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -238769.7311 - g_loss: -1884741.5653\n",
      "Epoch 1938/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -5045.8141 - g_loss: -2990162.1974\n",
      "Epoch 1939/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 128372.5852 - g_loss: -4055332.5039\n",
      "Epoch 1940/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -55948.0311 - g_loss: -3146771.4342\n",
      "Epoch 1941/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -9691.7211 - g_loss: -2983798.3039\n",
      "Epoch 1942/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -151765.9290 - g_loss: -2403260.7663\n",
      "Epoch 1943/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -122407.2003 - g_loss: -1696901.8263\n",
      "Epoch 1944/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -19509.2109 - g_loss: 516056.5635\n",
      "Epoch 1945/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 29717.4905 - g_loss: 592259.3021\n",
      "Epoch 1946/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: -37313.7859 - g_loss: 326745.1429\n",
      "Epoch 1947/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -577619.9617 - g_loss: 1307618.1740\n",
      "Epoch 1948/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 19621.3303 - g_loss: 1540141.0868\n",
      "Epoch 1949/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 144563.0664 - g_loss: 2192814.7832\n",
      "Epoch 1950/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -4729.9116 - g_loss: 741871.7329\n",
      "Epoch 1951/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -19597.5088 - g_loss: -1086744.3763\n",
      "Epoch 1952/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 88098.7618 - g_loss: -2325472.6375\n",
      "Epoch 1953/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 84255.5258 - g_loss: -3616626.5211\n",
      "Epoch 1954/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -39140.6015 - g_loss: -4127153.3132\n",
      "Epoch 1955/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -155797.9587 - g_loss: -1599133.8031\n",
      "Epoch 1956/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 3370.7917 - g_loss: -1341773.9051\n",
      "Epoch 1957/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -137293.3737 - g_loss: -495744.6303\n",
      "Epoch 1958/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 170656.0207 - g_loss: 164778.2414\n",
      "Epoch 1959/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 87981.0304 - g_loss: -564883.6719\n",
      "Epoch 1960/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 48938.9390 - g_loss: -1837227.2408\n",
      "Epoch 1961/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 67410.2802 - g_loss: -3385354.8013\n",
      "Epoch 1962/2000\n",
      "94/94 [==============================] - 15s 164ms/step - d_loss: 289473.6221 - g_loss: -3594601.8605\n",
      "Epoch 1963/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -134625.4220 - g_loss: -4137182.7707\n",
      "Epoch 1964/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 188311.4373 - g_loss: -2609371.1635\n",
      "Epoch 1965/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 161235.7613 - g_loss: -3651893.1289\n",
      "Epoch 1966/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -177845.3522 - g_loss: -5793745.1184\n",
      "Epoch 1967/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -23311.2215 - g_loss: -6936833.8763\n",
      "Epoch 1968/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -14191.4829 - g_loss: -7236733.7316\n",
      "Epoch 1969/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: -209804.6478 - g_loss: -7908215.9579\n",
      "Epoch 1970/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -10561.1586 - g_loss: -8674601.0579\n",
      "Epoch 1971/2000\n",
      "94/94 [==============================] - 15s 163ms/step - d_loss: -11266.7200 - g_loss: -10213635.0895\n",
      "Epoch 1972/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -8121.2950 - g_loss: -10509532.9000\n",
      "Epoch 1973/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 36611.7308 - g_loss: -8895797.9474\n",
      "Epoch 1974/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -12903.6786 - g_loss: -8248175.1211\n",
      "Epoch 1975/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -95927.5255 - g_loss: -8728644.0053\n",
      "Epoch 1976/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: -132187.2135 - g_loss: -9624410.6632\n",
      "Epoch 1977/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -14820.1136 - g_loss: -10356932.9316\n",
      "Epoch 1978/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 127099.9182 - g_loss: -9347419.8684\n",
      "Epoch 1979/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 156932.1725 - g_loss: -9122239.4474\n",
      "Epoch 1980/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: -89433.8873 - g_loss: -9870207.2474\n",
      "Epoch 1981/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: -97736.0871 - g_loss: -10945406.9263\n",
      "Epoch 1982/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 87968.5424 - g_loss: -10796877.6526\n",
      "Epoch 1983/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 402208.1717 - g_loss: -11413043.1053\n",
      "Epoch 1984/2000\n",
      "94/94 [==============================] - 16s 172ms/step - d_loss: 38238.9080 - g_loss: -10423530.5947\n",
      "Epoch 1985/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -47122.8402 - g_loss: -12126908.3579\n",
      "Epoch 1986/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -155531.4492 - g_loss: -12547535.7579\n",
      "Epoch 1987/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: 74908.0721 - g_loss: -12649682.9895\n",
      "Epoch 1988/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -371580.8536 - g_loss: -12027191.6632\n",
      "Epoch 1989/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -165318.3429 - g_loss: -10982515.0947\n",
      "Epoch 1990/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: 99058.7048 - g_loss: -10981564.1842\n",
      "Epoch 1991/2000\n",
      "94/94 [==============================] - 16s 170ms/step - d_loss: -225608.6372 - g_loss: -11086893.2211\n",
      "Epoch 1992/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 48814.3141 - g_loss: -11167030.1526\n",
      "Epoch 1993/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 37575.8065 - g_loss: -11346160.4000\n",
      "Epoch 1994/2000\n",
      "94/94 [==============================] - 16s 169ms/step - d_loss: 21027.5757 - g_loss: -9111411.2684\n",
      "Epoch 1995/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: -323171.3885 - g_loss: -9412167.4211\n",
      "Epoch 1996/2000\n",
      "94/94 [==============================] - 16s 167ms/step - d_loss: 174535.3854 - g_loss: -8672028.6000\n",
      "Epoch 1997/2000\n",
      "94/94 [==============================] - 16s 171ms/step - d_loss: 248900.1745 - g_loss: -9830179.7000\n",
      "Epoch 1998/2000\n",
      "94/94 [==============================] - 16s 166ms/step - d_loss: -308044.6034 - g_loss: -9466361.4105\n",
      "Epoch 1999/2000\n",
      "94/94 [==============================] - 15s 165ms/step - d_loss: 64930.8266 - g_loss: -10099378.7316\n",
      "Epoch 2000/2000\n",
      "94/94 [==============================] - 16s 168ms/step - d_loss: 113303.6463 - g_loss: -9096829.6895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f07f80a5978>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training the model.\n",
    "wgan.fit(train_ds, batch_size=BATCH_SIZE, epochs=2000, callbacks=[cbk, tb_cbk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Examples using learned Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 11.5, 11.5, -0.5)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load generator\n",
    "'''\n",
    "generator.compile(optimizer=Adam(lr=0.0008), # per Foster, 2017 RMSprop(lr=0.0008)\n",
    "                          loss=binary_crossentropy,\n",
    "                          metrics=['accuracy'])\n",
    "generator = tf.keras.models.load_model('/data/output/models/dwarfganWGANGPR02/generator-2021-04-04_025322.h5')\n",
    "'''\n",
    "# generate new example of learned representation in latent space\n",
    "try:\n",
    "    generator\n",
    "except NameError:\n",
    "    #get latest generator model save file\n",
    "    folder = pathlib.Path(f'{out_model_dir}/{model_name}')\n",
    "    saves = list(folder.glob('generator*'))\n",
    "    latest = max(saves, key=os.path.getctime)\n",
    "    #load latest generator save file\n",
    "    generator = tf.keras.models.load_model(latest)\n",
    "        \n",
    "noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "res = np.array(generator(noise, training=False)).astype('uint8')\n",
    "\n",
    "#Rescale\n",
    "res = res.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Visualize result\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(res)\n",
    "plt.title(f'Example Generator Output')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
