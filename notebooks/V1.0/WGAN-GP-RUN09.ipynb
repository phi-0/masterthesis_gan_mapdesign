{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DwarfGAN - Deep Learning based Map Design for Dwarf Fortress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "Number of GPUs found: 2\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from keras.layers import Add, Concatenate, Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D, LayerNormalization, Cropping2D\n",
    "from keras.layers.experimental import preprocessing\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.losses import binary_crossentropy, Loss\n",
    "from keras import metrics\n",
    "from functools import partial\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import random\n",
    "\n",
    "#!pip install boto3\n",
    "import boto3 as b3 \n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "NUM_GPUS = len(tf.config.list_physical_devices('GPU'))\n",
    "print(f'Number of GPUs found: {NUM_GPUS}')\n",
    "\n",
    "############### CONFIG ###################\n",
    "\n",
    "# model name\n",
    "model_name = 'dwarfganWGANGPR09Tiles'\n",
    "# folder path to input files (map images)\n",
    "fpath = r'/data2/input'\n",
    "# folder path to tensorboard output\n",
    "tboard_dir = '/data2/output/tensorboard'\n",
    "# folder path for saved model output\n",
    "out_model_dir = '/data2/output/models'\n",
    "# folder for images to be saved during training\n",
    "out_img_dir = '/data2/output/images'\n",
    "# use skip connections (additive/concatenate)?\n",
    "SKIP_ADD = False\n",
    "SKIP_CONCAT = False\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 1000 \n",
    "#BATCH_PER_EPOCH = 20\n",
    "# pre-processed (cropped) tiles are 12x12 pixels but will be cropped by the critic to 12x10\n",
    "IMAGE_SIZE = (12,12)\n",
    "BATCH_SIZE = 256\n",
    "CRITIC_FACTOR = 5 # number of times the critic is trained more often than the generator. Recommended = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "RELU_SLOPE_C = 0.2\n",
    "RELU_SLOPE_G = 0.2\n",
    "DROPOUT_C = 0.3\n",
    "MOMENTUM_G = 0.9\n",
    "CRIT_LR = 0.0003 # Adjusted learning rates according to two time-scale update rule (TTUR), see Heusel et al., 2017\n",
    "GEN_LR = 0.0001\n",
    "\n",
    "# NOTE: all extracted map PNGs have been saved on a separate virtual disk mapped to '/data' or '/data2' of the virtual machine in use\n",
    "data_dir = pathlib.Path(fpath + '/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Train / Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map images sourced from the DFMA come in a variety of dimensions. In order to create sample images with constant dimensions, as required by tensors, the 100k input samples were run through a python script to randomly crop 10 1024 x 1024 areas per picture. Of those cropped (sub-)images, only the ones which contain structures were retained. This was achieved by filtering out image crops which only contained two or less different colors. With that, the logic mainly filterd out crops which only contained black. This process resulted in 700'000+ (sub-)image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12068 cropped image samples available\n"
     ]
    }
   ],
   "source": [
    "# use pre-processed (cropped) 12 x 12 images\n",
    "data_dir = pathlib.Path(fpath + '/ascii_crops_12/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "print(f'There are {str(len(imgs))} cropped image samples available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random sample input image:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAIAAADZF8uwAAAAIklEQVR4nGNgYGBgSExk0NXFyyCsgoGBsIrERIZR66hnHQAXTjVBM+aL2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=12x12 at 0x7F6834FB92B0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show example sample image (cropped to 128x128)\n",
    "print('A random sample input image:')\n",
    "PIL.Image.open(imgs[random.randint(0,len(imgs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12068 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# creating keras datasets for training and validation - refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "dataset_train = tf.keras.preprocessing.image_dataset_from_directory(  fpath+'/ascii_crops_12',\n",
    "                                                                      image_size=IMAGE_SIZE, \n",
    "                                                                      batch_size=BATCH_SIZE, \n",
    "                                                                      #labels=[1.] * len(imgs), # setting all labels to 1.0 (for 'real') as float32\n",
    "                                                                      #label_mode=None, # yields float32 type labels\n",
    "                                                                      seed=875 #,\n",
    "                                                                   )\n",
    "\n",
    "#drop last batch that contains less samples (to match the constant input shape of the tile data)\n",
    "dataset_train = dataset_train.take(len(dataset_train)-1)\n",
    "# refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = dataset_train.cache().prefetch(buffer_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAINING = 12068 # = 20% of total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Random Sample from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check random images from prepared batches\n",
    "plt.figure(figsize=(5, 5))\n",
    "for images, labels in train_ds.take(1): # take one batch. Here batch_size = 128 examples per batch\n",
    "    for i in range(9): # show first 9 images of batch\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(f'img {i}')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATED Model Definition\n",
    "\n",
    "Generally the following changes have been implemented to the architectures:\n",
    "\n",
    "### RUN09\n",
    "\n",
    "- instead of generating full maps, this model focuses on learning to recreate single tiles which represent the different objects/elements in the game world\n",
    "- implemented secondary input for both critic and generator which introduces the single tiles cropped from the tileset files (12x10) as a 12x10x256 vector (256 different 12x10 tiles) \n",
    "- **NEXT RUN**: \n",
    "    - add a Dense layer or 1x1 CONV between Tiles input and concat layer to allow more flexibility for the model to learn which parts are considered\n",
    "    - increase batch size as much as possible\n",
    "\n",
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discriminator_model():\n",
    "\n",
    "    # DISCRIMINATOR\n",
    "    # set input variables to variable width + height. Will be cropped in preprocessing [CURRENTLY FIXED TO 256x256]\n",
    "    input_dim = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "\n",
    "    # Input\n",
    "    d_input = Input(shape=input_dim, name='Discriminator_Input')\n",
    "    \n",
    "    \n",
    "\n",
    "    # ---- REMOVED FOR 256x256 NETWORK ----------\n",
    "    # Keras-based preprocessing. Alternative: RandomCrop()\n",
    "    # use smart_resizing?\n",
    "    #x = tf.keras.preprocessing.image.smart_resize(d_input, (1024, 1024))\n",
    "    #x = preprocessing.Resizing(width=512, \n",
    "    #                           height=512, \n",
    "    #                           name='Preprocessing_Resize'\n",
    "    #                          )(d_input) # Resize to 512 x 512 images\n",
    "\n",
    "    #we crop the images y (1 pixel to the left and one to the right) dimension to 12x10 to match the tile dimensions. To stabilize the output we do NOT use random crop.\n",
    "    x = Cropping2D((0,1))(d_input)\n",
    "\n",
    "    x = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale'\n",
    "                               )(x) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    # START TILES INPUT\n",
    "    tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "    \n",
    "    tiles = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale_Tiles'\n",
    "                               )(tiles_input) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    \n",
    "    tiles = Conv2D(filters=256, kernel_size=(1,1), strides=1)(tiles)\n",
    "    \n",
    "    x = Concatenate()([x, tiles])\n",
    "    # END TILES INPUT\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 0\n",
    "    x = Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = (3,3), \n",
    "            strides = 1,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_0'\n",
    "    )(x)\n",
    "    \n",
    "    # Activation 0 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_0')(x)\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 1\n",
    "    x = Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_1'\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 1\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 1 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_1')(x)\n",
    "\n",
    "    # Dropout 1\n",
    "    x = Dropout(rate = DROPOUT_C)(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Conv2D Layer 2\n",
    "    x = Conv2D(\n",
    "            filters = 256,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            name = 'Discriminator_Conv2D_Layer_3',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 2\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 2 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_3')(x)\n",
    "\n",
    "\n",
    "    # OUTPUT\n",
    "    x = Flatten()(x)\n",
    "    #x = Dropout(DROPOUT_C)(x)\n",
    "    \n",
    "    d_output = Dense(1, \n",
    "                     #activation='sigmoid', \n",
    "                     kernel_initializer = RandomNormal(mean=0, stddev=0.02) # random initialization of weights with normal distribution around 0 with small SD\n",
    "                    )(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Discriminator Model intialization\n",
    "    discriminator = Model([d_input, tiles_input], d_output, name='Discriminator')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Discriminator_Input (InputLayer [(None, 12, 12, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Tiles_Input (InputLayer)        [(None, 12, 10, 256) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 12, 10, 3)    0           Discriminator_Input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_Rescale_Tiles (Re (None, 12, 10, 256)  0           Tiles_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_Rescale (Rescalin (None, 12, 10, 3)    0           cropping2d_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 12, 10, 256)  65792       Preprocessing_Rescale_Tiles[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 12, 10, 259)  0           Preprocessing_Rescale[0][0]      \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_0 (C (None, 12, 10, 64)   149248      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_0 (LeakyReLU)        (None, 12, 10, 64)   0           Discriminator_Conv2D_Layer_0[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_1 (C (None, 6, 5, 128)    73856       Activation_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_1 (LeakyReLU)        (None, 6, 5, 128)    0           Discriminator_Conv2D_Layer_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 6, 5, 128)    0           Activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_3 (C (None, 3, 3, 256)    295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Activation_3 (LeakyReLU)        (None, 3, 3, 256)    0           Discriminator_Conv2D_Layer_3[0][0\n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2304)         0           Activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            2305        flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 586,369\n",
      "Trainable params: 586,369\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = discriminator_model()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    # GENERATOR\n",
    "\n",
    "    # set input variable dimensions. Here we will start out with a vector of length 100 for each sample (sampled from a normal distribution, representing the learned latent space)\n",
    "    input_dim = (LATENT_DIM)\n",
    "\n",
    "    # Input\n",
    "    g_input = Input(shape=input_dim, name='Generator_Input')\n",
    "\n",
    "    # Dense Layer 1\n",
    "    x = Dense(np.prod([3,3,512]), kernel_initializer = RandomNormal(mean=0., stddev=0.02), \n",
    "              use_bias=False)(g_input) # use_bias=False see https://keras.io/examples/generative/wgan_gp/\n",
    "\n",
    "    # Batch Norm Layer 1\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    \n",
    "    # Activation Layer 1\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Reshape into 3D tensor\n",
    "    x = Reshape((3,3,512))(x)\n",
    "\n",
    "    # Upsampling Layer 1 + Conv2D Layer1\n",
    "    x = Conv2DTranspose(filters=512, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=256, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=(3,3), padding='same', strides=(1,1), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    # START TILES\n",
    "    tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "    tiles = preprocessing.Resizing(width=12, height=12)(tiles_input)\n",
    "    tiles = Conv2D(filters=256, kernel_size=(1,1), strides=1)(tiles)\n",
    "    \n",
    "    x = Concatenate()([x, tiles])\n",
    "    \n",
    "    # END TILES\n",
    "    # Batch Norm Layer 2\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "    \n",
    "    # reduce output dimensions via 1x1 convolution\n",
    "    x = Conv2D(filters=3, kernel_size=(1,1), padding='same', strides=(1,1),\n",
    "               kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "        \n",
    "    # tanh activation layer to scale values to [-1:1]\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    # Batch Norm Layer 7\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "\n",
    "    # Output - Rescale Values back to [0:255] since the discriminator will automatically rescale back down to [-1:1] as part of the pre-processing pipeline\n",
    "    g_output = (255 / 2) * (x + 1) \n",
    "\n",
    "\n",
    "    # Generator Model initialization\n",
    "    generator = Model([g_input, tiles_input], g_output, name='Generator')\n",
    "    \n",
    "    \n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Generator_Input (InputLayer)    [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4608)         589824      Generator_Input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4608)         18432       dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4608)         0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 3, 3, 512)    0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 6, 6, 512)    2359296     reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 6, 6, 512)    1024        conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 6, 6, 512)    0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 12, 12, 256)  1179648     leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 12, 12, 256)  512         conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Tiles_Input (InputLayer)        [(None, 12, 10, 256) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 12, 12, 256)  0           layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "resizing_1 (Resizing)           (None, 12, 12, 256)  0           Tiles_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 12, 12, 128)  294912      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 12, 12, 256)  65792       resizing_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 12, 12, 384)  0           conv2d_transpose_5[0][0]         \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 12, 12, 384)  1536        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 12, 12, 384)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 12, 12, 3)    1152        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 12, 12, 3)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_1 (TensorFlow [(None, 12, 12, 3)]  0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_1 (TensorFlowOp [(None, 12, 12, 3)]  0           tf_op_layer_AddV2_1[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 4,512,128\n",
      "Trainable params: 4,502,144\n",
      "Non-trainable params: 9,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen = generator_model()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tiles\n",
    "\n",
    "We load the 256 split 12x10 tiles into a (12,10,256) numpy array to feed to the network at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiles():\n",
    "    tiles = []\n",
    "\n",
    "    data_dir = pathlib.Path('/data2/input/tiles/800x600/')\n",
    "    imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "    # show example sample image (cropped to 128x128)\n",
    "    PIL.Image.open(imgs[random.randint(0,len(imgs))])\n",
    "\n",
    "    for img in imgs:\n",
    "        tiles.append(np.asarray(PIL.Image.open(img)).astype('uint8'))\n",
    "\n",
    "    #reshape\n",
    "    ds_tiles = np.array(tiles)\n",
    "    ds_tiles = ds_tiles.reshape((1,12,10,256))\n",
    "    ds_tiles = ds_tiles.repeat(repeats=BATCH_SIZE//NUM_GPUS, axis=0)\n",
    "\n",
    "    return ds_tiles\n",
    "\n",
    "\n",
    "    #repeate same 256 12x10 input images as many times as there are batches in the training dataset (N_TRAINING//BATCH_SIZE)\n",
    "    \n",
    "    #print(f'Created numpy array with shape: {ds_tiles.shape}')\n",
    "\n",
    "    #create tf dataset\n",
    "    #tiles_ds = tf.data.Dataset.from_tensor_slices(ds_tiles).batch(BATCH_SIZE)\n",
    "    #train_ds = np.array(train_ds)\n",
    "\n",
    "    #Combine\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices({'Discriminator_Input':train_ds,'Tiles_Input':tiles_ds})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP (Full) Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we compile the models, we need to implement a custom loss function which uses the Wasserstein distance and a gradient penalty term in order to ensure 1 Lipschitz constraints are followed. A WGAN with GP further involves a slightly more complicated training process which trains the critic (discriminator without sigmoid activation function) by feeding three different kinds of images:\n",
    "\n",
    "1. real images (i.e. available samples)\n",
    "2. 'fake' images (i.e. constructed by the generator)\n",
    "3. random interpolations between real and fake images (i.e. random samples interpolated from values between the fake and real images)\n",
    "\n",
    "The full training process of a critic is depicted below (source: Foster, 2019, p. 122):\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"wgan_gp_critic_training.png\"></img>\n",
    "    <i>Computational Graph for one Discriminator Training Epoch. (Source: Foster, 2019, p.122)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below roughly follows the OOP-based framework set by keras see https://keras.io/examples/generative/wgan_gp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope(): \n",
    "\n",
    "    critic = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    ds_tiles = load_tiles()\n",
    "\n",
    "    class WGANGP(keras.Model):\n",
    "        def __init__(\n",
    "            self,\n",
    "            critic,\n",
    "            generator,\n",
    "            latent_dim,\n",
    "            tensorboard_callback,\n",
    "            critic_extra_steps=5,\n",
    "            gp_weight=10.0,\n",
    "            tiles=None\n",
    "        ):\n",
    "            super(WGANGP, self).__init__()\n",
    "            self.critic = critic\n",
    "            self.generator = generator\n",
    "            self.latent_dim = latent_dim\n",
    "            self.tensorboard_callback = tensorboard_callback\n",
    "            self.d_steps = critic_extra_steps\n",
    "            self.gp_weight = gp_weight\n",
    "            self.tiles=tiles\n",
    "            \n",
    "\n",
    "        def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "            super(WGANGP, self).compile()\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.g_optimizer = g_optimizer\n",
    "            self.d_loss_fn = d_loss_fn\n",
    "            self.g_loss_fn = g_loss_fn\n",
    "\n",
    "        def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "            \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "            This loss is calculated on an interpolated image\n",
    "            and added to the discriminator loss.\n",
    "            \"\"\"\n",
    "            # Get the interpolated image\n",
    "            alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "            diff = fake_images - real_images\n",
    "            interpolated = real_images + alpha * diff\n",
    "\n",
    "            with tf.GradientTape() as gp_tape:\n",
    "                gp_tape.watch(interpolated)\n",
    "                # 1. Get the discriminator output for this interpolated image.\n",
    "                pred = self.critic({'Discriminator_Input':interpolated,'Tiles_Input':ds_tiles}, training=True)\n",
    "\n",
    "            # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "            grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "            # 3. Calculate the norm of the gradients.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "            gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "            return gp\n",
    "\n",
    "        def train_step(self, real_images):\n",
    "            #checking whether we handed a tuple of (numpy) data to .fit().\n",
    "            #if not, the data must be a tf.data.Dataset generator that yields batches of datasets (data, labels)\n",
    "            if isinstance(real_images, tuple):\n",
    "                real_images = real_images[0]\n",
    "\n",
    "            # Get the batch size\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "            # For each batch, we are going to perform the\n",
    "            # following steps as laid out in the original paper:\n",
    "            # 1. Train the generator and get the generator loss\n",
    "            # 2. Train the discriminator and get the discriminator loss\n",
    "            # 3. Calculate the gradient penalty\n",
    "            # 4. Multiply this gradient penalty with a constant weight factor = self.discriminator_extra_steps = 5 (default value)\n",
    "            # 5. Add the gradient penalty to the discriminator loss\n",
    "            # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "            # Train the discriminator first. The original paper recommends training\n",
    "            # the discriminator for `x` more steps (typically 5) as compared to generator\n",
    "            \n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.critic)\n",
    "            \n",
    "            for i in range(self.d_steps):\n",
    "                # Get the latent vector\n",
    "                random_latent_vectors = tf.random.normal(\n",
    "                    shape=(batch_size, self.latent_dim)\n",
    "                )\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Generate fake images from the latent vector\n",
    "                    fake_images = self.generator({'Generator_Input':random_latent_vectors,'Tiles_Input':ds_tiles}, training=True)\n",
    "                    # Get the logits for the fake images\n",
    "                    fake_logits = self.critic({'Discriminator_Input':fake_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "                    # Get the logits for the real images\n",
    "                    real_logits = self.critic({'Discriminator_Input':real_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "\n",
    "                    # Calculate the discriminator loss using the fake and real image logits\n",
    "                    d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                    # Calculate the gradient penalty\n",
    "                    gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                    # Add the gradient penalty to the original discriminator loss\n",
    "                    d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "                # Get the gradients w.r.t the discriminator loss\n",
    "                d_gradient = tape.gradient(d_loss, self.critic.trainable_variables)\n",
    "                # Update the weights of the discriminator using the discriminator optimizer\n",
    "                self.d_optimizer.apply_gradients(\n",
    "                    zip(d_gradient, self.critic.trainable_variables)\n",
    "                )\n",
    "\n",
    "            # Train the generator\n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.generator)\n",
    "            \n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images using the generator\n",
    "                generated_images = self.generator({'Generator_Input':random_latent_vectors,'Tiles_Input':ds_tiles}, training=True)\n",
    "                # Get the discriminator logits for fake images\n",
    "                gen_img_logits = self.critic({'Discriminator_Input':generated_images,'Tiles_Input':ds_tiles}, training=True)\n",
    "                # Calculate the generator loss\n",
    "                g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "            # Get the gradients w.r.t the generator loss\n",
    "            gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "            # Update the weights of the generator using the generator optimizer\n",
    "            self.g_optimizer.apply_gradients(\n",
    "                zip(gen_gradient, self.generator.trainable_variables)\n",
    "            )\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "        \n",
    "    class GANMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, num_img=5, latent_dim=128):\n",
    "            self.num_img = num_img\n",
    "            self.latent_dim = latent_dim\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None): #on_epoch_end(self, epoch, logs=None):\n",
    "            '''\n",
    "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "            generated_images = self.model.generator(random_latent_vectors)\n",
    "            #generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "            for i in range(self.num_img):\n",
    "                img = generated_images[i].numpy()\n",
    "                img = keras.preprocessing.image.array_to_img(img)\n",
    "                img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "            '''\n",
    "            \n",
    "            # Sample generator output for num_img images\n",
    "            noise = np.random.normal(0, 1, (self.num_img, self.latent_dim))\n",
    "            gen_imgs = generator.predict({'Generator_Input':noise,'Tiles_Input':ds_tiles[0:5]})\n",
    "            gen_imgs = gen_imgs.astype('uint8')\n",
    "\n",
    "            #!!!NOT NECESSARY ANYMORE AS IMPLEMENTED AS PART OF THE MODEL!!!\n",
    "            #gen_imgs = 0.5 * (gen_imgs + 1)  #scale back to [0:1]\n",
    "            gen_imgs = gen_imgs.reshape((self.num_img, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "            # save n example images\n",
    "            for i in range(self.num_img):\n",
    "                plt.figure(figsize=(5, 5))\n",
    "                plt.imshow(gen_imgs[i])\n",
    "                #plt.title(f'Example Generator Output')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # adjust path based on whether execution is local or on linux VM\n",
    "                if pathlib.Path(f'{out_img_dir}/{model_name}').exists():\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    #mkdir\n",
    "                    os.mkdir(f'{out_img_dir}/{model_name}')\n",
    "                    #save\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                    \n",
    "            # save corresponding model\n",
    "            now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "            \n",
    "            if pathlib.Path(f'{out_model_dir}/{model_name}').exists():\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5')        \n",
    "            else:\n",
    "                #make dir\n",
    "                os.mkdir(f'{out_model_dir}/{model_name}')\n",
    "                #write\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5') \n",
    "        \n",
    "        \n",
    "    # Instantiate the optimizer for both networks\n",
    "    # (learning_rate=0.0002, beta_1=0.5 are recommended) as per Radford et al. 2016 pp. 3-4\n",
    "    generator_optimizer = Adam(\n",
    "        learning_rate=GEN_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "    critic_optimizer = Adam(\n",
    "        learning_rate=CRIT_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def critic_loss(real_img, fake_img):\n",
    "        real_loss = tf.reduce_mean(real_img)\n",
    "        fake_loss = tf.reduce_mean(fake_img)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(fake_img):\n",
    "        return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "    # Instantiate the custome `GANMonitor` Keras callback.\n",
    "    cbk = GANMonitor(num_img=5, latent_dim=LATENT_DIM)\n",
    "    \n",
    "    # Instantiate the tensorboard tf.keras callback\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "    tb_cbk = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = f'{tboard_dir}/{model_name}_{now}', \n",
    "        write_graph = False, \n",
    "        write_images = True,\n",
    "        histogram_freq = 1) \n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGANGP(\n",
    "        critic=critic,\n",
    "        generator=generator,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        tensorboard_callback=tb_cbk,\n",
    "        critic_extra_steps=CRITIC_FACTOR,\n",
    "        gp_weight=GRADIENT_PENALTY_WEIGHT,\n",
    "        tiles=ds_tiles       \n",
    "        \n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "        d_optimizer=critic_optimizer,\n",
    "        g_optimizer=generator_optimizer,\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=critic_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 15 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      " 1/47 [..............................] - ETA: 0s - d_loss: 9.7321 - g_loss: 0.1011WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "47/47 [==============================] - ETA: 0s - d_loss: -641.8432 - g_loss: 41.7497INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "47/47 [==============================] - 17s 358ms/step - d_loss: -658.7720 - g_loss: 59.4880\n",
      "Epoch 2/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -7181.0693 - g_loss: 46866.8914\n",
      "Epoch 3/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -7289.9818 - g_loss: 172578.3044\n",
      "Epoch 4/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -330.6437 - g_loss: 127790.9101\n",
      "Epoch 5/2000\n",
      "47/47 [==============================] - 16s 338ms/step - d_loss: -324.2936 - g_loss: 13242.5623\n",
      "Epoch 6/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -282.1218 - g_loss: -35569.8876\n",
      "Epoch 7/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -256.9759 - g_loss: -183305.2596\n",
      "Epoch 8/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -516.9302 - g_loss: -91402.9119\n",
      "Epoch 9/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -62.9814 - g_loss: 13105.2543\n",
      "Epoch 10/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 247.6522 - g_loss: 186554.8507\n",
      "Epoch 11/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -211.9086 - g_loss: 198834.0355\n",
      "Epoch 12/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 189.4298 - g_loss: 230907.9222\n",
      "Epoch 13/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -373.3350 - g_loss: 386794.4150\n",
      "Epoch 14/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 558.1092 - g_loss: 392528.2249\n",
      "Epoch 15/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -422.6790 - g_loss: 587350.5814\n",
      "Epoch 16/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 366.5217 - g_loss: 161447.9783\n",
      "Epoch 17/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -141.6549 - g_loss: -295802.6523\n",
      "Epoch 18/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -114.9240 - g_loss: -465739.4094\n",
      "Epoch 19/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 68.9422 - g_loss: -194282.9207\n",
      "Epoch 20/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -605.4007 - g_loss: -130549.3764\n",
      "Epoch 21/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 141.5041 - g_loss: -630998.1391\n",
      "Epoch 22/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -774.0233 - g_loss: -645958.3963\n",
      "Epoch 23/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -902.6231 - g_loss: -2463199.4609\n",
      "Epoch 24/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 1204.0328 - g_loss: -4043709.9844\n",
      "Epoch 25/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -7427.8611 - g_loss: -3567013.1198\n",
      "Epoch 26/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -5468.7215 - g_loss: -2825986.6042\n",
      "Epoch 27/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 1467.5808 - g_loss: -2100940.7266\n",
      "Epoch 28/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -9682.9121 - g_loss: -3101657.8047\n",
      "Epoch 29/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 8420.1063 - g_loss: -3160477.2344\n",
      "Epoch 30/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 3225.1470 - g_loss: -817261.1621\n",
      "Epoch 31/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 4541.4150 - g_loss: -159010.8562\n",
      "Epoch 32/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1144.4789 - g_loss: -291120.6930\n",
      "Epoch 33/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 1791.5277 - g_loss: -1944524.3984\n",
      "Epoch 34/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1637.4817 - g_loss: -2302013.4531\n",
      "Epoch 35/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 2420.5744 - g_loss: -1846897.6432\n",
      "Epoch 36/2000\n",
      "47/47 [==============================] - 15s 330ms/step - d_loss: 1865.6215 - g_loss: -3893262.4193\n",
      "Epoch 37/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 164.9241 - g_loss: -6105355.9062\n",
      "Epoch 38/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -3711.0188 - g_loss: -6966375.4271\n",
      "Epoch 39/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -9388.0723 - g_loss: -4908437.8021\n",
      "Epoch 40/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 8883.0545 - g_loss: -2271702.3932\n",
      "Epoch 41/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -1601.7749 - g_loss: -4007450.2292\n",
      "Epoch 42/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -3347.4593 - g_loss: -6106938.3333\n",
      "Epoch 43/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -5696.1791 - g_loss: -11124444.8333\n",
      "Epoch 44/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1236.0968 - g_loss: -9217776.3333\n",
      "Epoch 45/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 9393.3699 - g_loss: -9043309.5000\n",
      "Epoch 46/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1.5478 - g_loss: -9424638.3542\n",
      "Epoch 47/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -2044.8124 - g_loss: -5677763.6771\n",
      "Epoch 48/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 7781.8223 - g_loss: -5606786.4792\n",
      "Epoch 49/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 3971.7633 - g_loss: -4899089.5365\n",
      "Epoch 50/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -1052.8392 - g_loss: -3262327.2135\n",
      "Epoch 51/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 7262.2218 - g_loss: -2848965.5469\n",
      "Epoch 52/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -1635.0010 - g_loss: -1794152.7786\n",
      "Epoch 53/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -97.0724 - g_loss: -2140339.5469\n",
      "Epoch 54/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 2750.7368 - g_loss: -2771033.5573\n",
      "Epoch 55/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 7884.6578 - g_loss: -3993434.5729\n",
      "Epoch 56/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: -1172.4237 - g_loss: -4483353.1875\n",
      "Epoch 57/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -2971.7653 - g_loss: -3809307.5677\n",
      "Epoch 58/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -4189.1418 - g_loss: -2388419.3073\n",
      "Epoch 59/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -141.5741 - g_loss: -1283599.5820\n",
      "Epoch 60/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -7173.0535 - g_loss: -4871447.8281\n",
      "Epoch 61/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: 1324.1959 - g_loss: -6561607.4167\n",
      "Epoch 62/2000\n",
      "47/47 [==============================] - 16s 337ms/step - d_loss: 2475.5412 - g_loss: -4387236.5208\n",
      "Epoch 63/2000\n",
      "47/47 [==============================] - 16s 343ms/step - d_loss: 10554.0801 - g_loss: -5448526.5104\n",
      "Epoch 64/2000\n",
      "47/47 [==============================] - 16s 336ms/step - d_loss: 1879.6916 - g_loss: -4313346.2344\n",
      "Epoch 65/2000\n",
      "47/47 [==============================] - 16s 342ms/step - d_loss: 133.3216 - g_loss: -2620247.3203\n",
      "Epoch 66/2000\n",
      "47/47 [==============================] - 16s 342ms/step - d_loss: 531.3524 - g_loss: -628041.2891\n",
      "Epoch 67/2000\n",
      "47/47 [==============================] - 16s 335ms/step - d_loss: -1100.7955 - g_loss: -878397.4172\n",
      "Epoch 68/2000\n",
      "47/47 [==============================] - 16s 348ms/step - d_loss: 689.6845 - g_loss: -264524.9804\n",
      "Epoch 69/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 575.2974 - g_loss: -803500.7363\n",
      "Epoch 70/2000\n",
      "47/47 [==============================] - 16s 334ms/step - d_loss: -172.5777 - g_loss: -1291486.7370\n",
      "Epoch 71/2000\n",
      "47/47 [==============================] - 16s 335ms/step - d_loss: -613.7945 - g_loss: -135793.8659\n",
      "Epoch 72/2000\n",
      "47/47 [==============================] - 16s 336ms/step - d_loss: 22.5233 - g_loss: -220980.2719\n",
      "Epoch 73/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 454.5416 - g_loss: -145993.1068\n",
      "Epoch 74/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: -14.7691 - g_loss: -112313.5701\n",
      "Epoch 75/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -38.3961 - g_loss: -58586.8005\n",
      "Epoch 76/2000\n",
      "47/47 [==============================] - 16s 339ms/step - d_loss: 94.6057 - g_loss: -20823.3021\n",
      "Epoch 77/2000\n",
      "47/47 [==============================] - 16s 332ms/step - d_loss: -333.6611 - g_loss: -199406.5470\n",
      "Epoch 78/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 138.6495 - g_loss: -204328.8300\n",
      "Epoch 79/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -12.8051 - g_loss: -78806.1659\n",
      "Epoch 80/2000\n",
      "47/47 [==============================] - 16s 331ms/step - d_loss: -58.7666 - g_loss: -247366.9648\n",
      "Epoch 81/2000\n",
      "47/47 [==============================] - 16s 344ms/step - d_loss: -399.1295 - g_loss: -817556.5052\n",
      "Epoch 82/2000\n",
      "47/47 [==============================] - 16s 335ms/step - d_loss: -81.5636 - g_loss: -705177.2227\n",
      "Epoch 83/2000\n",
      "47/47 [==============================] - 16s 337ms/step - d_loss: 433.0053 - g_loss: -691235.8763\n",
      "Epoch 84/2000\n",
      "47/47 [==============================] - 16s 334ms/step - d_loss: -655.5697 - g_loss: -443586.4512\n",
      "Epoch 85/2000\n",
      "47/47 [==============================] - 16s 333ms/step - d_loss: 330.4311 - g_loss: -701171.2057\n",
      "Epoch 86/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 257.0298 - g_loss: -516687.8659\n",
      "Epoch 87/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: -9.6797 - g_loss: -487001.6589\n",
      "Epoch 88/2000\n",
      "47/47 [==============================] - 16s 338ms/step - d_loss: 898.7763 - g_loss: -1258081.4583\n",
      "Epoch 89/2000\n",
      "47/47 [==============================] - 16s 340ms/step - d_loss: 231.8167 - g_loss: -2365264.6276\n",
      "Epoch 90/2000\n",
      "47/47 [==============================] - 16s 332ms/step - d_loss: -11522.4044 - g_loss: -4125740.5104\n",
      "Epoch 91/2000\n",
      "47/47 [==============================] - 16s 331ms/step - d_loss: -7811.8380 - g_loss: -7455785.6771\n",
      "Epoch 92/2000\n",
      "47/47 [==============================] - 16s 332ms/step - d_loss: -14278.6540 - g_loss: -11067545.3125\n",
      "Epoch 93/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 5949.6981 - g_loss: -8737531.1146\n",
      "Epoch 94/2000\n",
      "47/47 [==============================] - 16s 348ms/step - d_loss: -7807.0095 - g_loss: -6014156.4062\n",
      "Epoch 95/2000\n",
      "47/47 [==============================] - 16s 331ms/step - d_loss: 2338.6815 - g_loss: -3531949.1536\n",
      "Epoch 96/2000\n",
      "47/47 [==============================] - 16s 340ms/step - d_loss: 9168.3076 - g_loss: -3503352.9375\n",
      "Epoch 97/2000\n",
      "47/47 [==============================] - 16s 335ms/step - d_loss: 4832.7874 - g_loss: -2105265.8047\n",
      "Epoch 98/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: -2573.0028 - g_loss: -1079436.2734\n",
      "Epoch 99/2000\n",
      "47/47 [==============================] - 16s 334ms/step - d_loss: 463.6020 - g_loss: -295969.4925\n",
      "Epoch 100/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -4769.4124 - g_loss: -142766.9840\n",
      "Epoch 101/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 3298.5612 - g_loss: -70634.5846\n",
      "Epoch 102/2000\n",
      "47/47 [==============================] - 16s 341ms/step - d_loss: -1306.1433 - g_loss: 314721.5988\n",
      "Epoch 103/2000\n",
      "47/47 [==============================] - 16s 331ms/step - d_loss: -5785.5446 - g_loss: -45664.4484\n",
      "Epoch 104/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 2070.2503 - g_loss: -1349871.1094\n",
      "Epoch 105/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -1742.6944 - g_loss: -1663891.7839\n",
      "Epoch 106/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 11031.0040 - g_loss: -1306275.9336\n",
      "Epoch 107/2000\n",
      "47/47 [==============================] - 16s 339ms/step - d_loss: 4669.9601 - g_loss: -1175744.7734\n",
      "Epoch 108/2000\n",
      "47/47 [==============================] - 16s 333ms/step - d_loss: -3981.7645 - g_loss: -1773381.3073\n",
      "Epoch 109/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 2203.3723 - g_loss: -2696145.6562\n",
      "Epoch 110/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 6259.2851 - g_loss: -2255443.7214\n",
      "Epoch 111/2000\n",
      "47/47 [==============================] - 16s 346ms/step - d_loss: 2211.7632 - g_loss: -2843727.7344\n",
      "Epoch 112/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 9435.7111 - g_loss: -3650855.7917\n",
      "Epoch 113/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -8974.9001 - g_loss: -4337147.3229\n",
      "Epoch 114/2000\n",
      "47/47 [==============================] - 16s 332ms/step - d_loss: 31562.8074 - g_loss: -3655348.4427\n",
      "Epoch 115/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -16336.6063 - g_loss: -6030822.0938\n",
      "Epoch 116/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -7954.8505 - g_loss: -5508274.8646\n",
      "Epoch 117/2000\n",
      "47/47 [==============================] - 16s 334ms/step - d_loss: -9093.0715 - g_loss: -7188536.2188\n",
      "Epoch 118/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 15510.6006 - g_loss: -7852441.0104\n",
      "Epoch 119/2000\n",
      "47/47 [==============================] - 16s 333ms/step - d_loss: 19197.3930 - g_loss: -7837930.3125\n",
      "Epoch 120/2000\n",
      "47/47 [==============================] - 16s 337ms/step - d_loss: 4453.7770 - g_loss: -7857752.2708\n",
      "Epoch 121/2000\n",
      "47/47 [==============================] - 16s 332ms/step - d_loss: -3367.2424 - g_loss: -6633114.4375\n",
      "Epoch 122/2000\n",
      "47/47 [==============================] - 16s 338ms/step - d_loss: 11431.6414 - g_loss: -8623964.9688\n",
      "Epoch 123/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 21697.5348 - g_loss: -8761330.6979\n",
      "Epoch 124/2000\n",
      "47/47 [==============================] - 16s 337ms/step - d_loss: -24615.1036 - g_loss: -9739292.5104\n",
      "Epoch 125/2000\n",
      "47/47 [==============================] - 16s 336ms/step - d_loss: 13413.6404 - g_loss: -10846653.5208\n",
      "Epoch 126/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 11179.5250 - g_loss: -13621632.9792\n",
      "Epoch 127/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 23215.7994 - g_loss: -13421881.6667\n",
      "Epoch 128/2000\n",
      "47/47 [==============================] - 16s 331ms/step - d_loss: -24072.8533 - g_loss: -9451830.0833\n",
      "Epoch 129/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 5467.6154 - g_loss: -5869386.4271\n",
      "Epoch 130/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -3540.9352 - g_loss: -5440037.7604\n",
      "Epoch 131/2000\n",
      "47/47 [==============================] - 16s 332ms/step - d_loss: 6793.0499 - g_loss: -3488356.6927\n",
      "Epoch 132/2000\n",
      "47/47 [==============================] - 15s 330ms/step - d_loss: 4651.5393 - g_loss: -4391083.5312\n",
      "Epoch 133/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: -11965.9746 - g_loss: -7678072.9583\n",
      "Epoch 134/2000\n",
      "47/47 [==============================] - 16s 346ms/step - d_loss: 3615.7719 - g_loss: -7783577.6771\n",
      "Epoch 135/2000\n",
      "47/47 [==============================] - 16s 334ms/step - d_loss: 11712.9976 - g_loss: -7280276.5312\n",
      "Epoch 136/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 5460.2954 - g_loss: -8033061.0417\n",
      "Epoch 137/2000\n",
      "47/47 [==============================] - 16s 338ms/step - d_loss: -1804.9454 - g_loss: -9000706.3646\n",
      "Epoch 138/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -236.2706 - g_loss: -8167547.3646\n",
      "Epoch 139/2000\n",
      "47/47 [==============================] - 16s 335ms/step - d_loss: -5121.6935 - g_loss: -5183592.8646\n",
      "Epoch 140/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -3931.5772 - g_loss: -3204538.6146\n",
      "Epoch 141/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -3599.3130 - g_loss: -4243495.8490\n",
      "Epoch 142/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -712.2264 - g_loss: -4515190.7396\n",
      "Epoch 143/2000\n",
      "47/47 [==============================] - 16s 334ms/step - d_loss: -12083.4724 - g_loss: -5073765.3906\n",
      "Epoch 144/2000\n",
      "47/47 [==============================] - 16s 335ms/step - d_loss: 14855.8926 - g_loss: -7170269.0000\n",
      "Epoch 145/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: -600.9073 - g_loss: -6861787.3333\n",
      "Epoch 146/2000\n",
      "47/47 [==============================] - 16s 332ms/step - d_loss: 4780.9242 - g_loss: -10540732.6146\n",
      "Epoch 147/2000\n",
      "47/47 [==============================] - 17s 369ms/step - d_loss: 11445.0213 - g_loss: -15553106.7292\n",
      "Epoch 148/2000\n",
      "47/47 [==============================] - 16s 351ms/step - d_loss: 7542.1973 - g_loss: -17242303.6875\n",
      "Epoch 149/2000\n",
      "47/47 [==============================] - 16s 332ms/step - d_loss: 18125.3553 - g_loss: -12667491.1875\n",
      "Epoch 150/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 10680.3926 - g_loss: -10366114.6875\n",
      "Epoch 151/2000\n",
      "47/47 [==============================] - 16s 331ms/step - d_loss: -1231.1076 - g_loss: -11882446.2917\n",
      "Epoch 152/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 7114.4450 - g_loss: -15628654.8750\n",
      "Epoch 153/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 34262.6927 - g_loss: -19721162.4375\n",
      "Epoch 154/2000\n",
      "47/47 [==============================] - 16s 334ms/step - d_loss: -816.3085 - g_loss: -13467887.7708\n",
      "Epoch 155/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -2815.4048 - g_loss: -11090151.2708\n",
      "Epoch 156/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 12237.2138 - g_loss: -10435523.7708\n",
      "Epoch 157/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: -4806.0033 - g_loss: -11763693.0208\n",
      "Epoch 158/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 6097.9889 - g_loss: -11873294.4167\n",
      "Epoch 159/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -9350.0502 - g_loss: -12411209.2917\n",
      "Epoch 160/2000\n",
      "47/47 [==============================] - 16s 334ms/step - d_loss: 10202.6052 - g_loss: -15089715.6042\n",
      "Epoch 161/2000\n",
      "47/47 [==============================] - 16s 336ms/step - d_loss: 4599.3550 - g_loss: -18356917.9375\n",
      "Epoch 162/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -17885.8887 - g_loss: -17119176.5208\n",
      "Epoch 163/2000\n",
      "47/47 [==============================] - 16s 331ms/step - d_loss: 358.8826 - g_loss: -14493676.3333\n",
      "Epoch 164/2000\n",
      "47/47 [==============================] - 16s 337ms/step - d_loss: 10868.0570 - g_loss: -16493899.8125\n",
      "Epoch 165/2000\n",
      "47/47 [==============================] - 16s 338ms/step - d_loss: -947.5263 - g_loss: -19469562.8333\n",
      "Epoch 166/2000\n",
      "47/47 [==============================] - 16s 338ms/step - d_loss: -27569.0556 - g_loss: -28486107.0833\n",
      "Epoch 167/2000\n",
      "47/47 [==============================] - 16s 347ms/step - d_loss: -27702.1683 - g_loss: -48911730.0000\n",
      "Epoch 168/2000\n",
      "47/47 [==============================] - 16s 342ms/step - d_loss: 32826.7935 - g_loss: -61772049.4167\n",
      "Epoch 169/2000\n",
      "47/47 [==============================] - 16s 338ms/step - d_loss: 39721.3117 - g_loss: -66955685.9167\n",
      "Epoch 170/2000\n",
      "47/47 [==============================] - 16s 335ms/step - d_loss: -51422.2669 - g_loss: -59252806.9167\n",
      "Epoch 171/2000\n",
      "47/47 [==============================] - 16s 340ms/step - d_loss: -17373.9980 - g_loss: -68340228.4167\n",
      "Epoch 172/2000\n",
      "47/47 [==============================] - 15s 330ms/step - d_loss: 77033.9810 - g_loss: -62762811.0000\n",
      "Epoch 173/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: 37998.1126 - g_loss: -59542251.3333\n",
      "Epoch 174/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 12854.0003 - g_loss: -53490377.8333\n",
      "Epoch 175/2000\n",
      "47/47 [==============================] - 16s 337ms/step - d_loss: 66204.3287 - g_loss: -53031471.5000\n",
      "Epoch 176/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -13131.8236 - g_loss: -58055538.2500\n",
      "Epoch 177/2000\n",
      "47/47 [==============================] - 16s 333ms/step - d_loss: 38596.1739 - g_loss: -51622616.0833\n",
      "Epoch 178/2000\n",
      "47/47 [==============================] - 16s 338ms/step - d_loss: -59567.8866 - g_loss: -50362680.5833\n",
      "Epoch 179/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: -2951.4492 - g_loss: -46157121.9167\n",
      "Epoch 180/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 17229.2532 - g_loss: -33746778.6667\n",
      "Epoch 181/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 3765.5860 - g_loss: -35409541.0417\n",
      "Epoch 182/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -39230.7539 - g_loss: -34297256.0417\n",
      "Epoch 183/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 31317.2747 - g_loss: -32648807.6667\n",
      "Epoch 184/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 38228.1584 - g_loss: -32760120.2917\n",
      "Epoch 185/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 30521.9898 - g_loss: -25885547.1667\n",
      "Epoch 186/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 8968.6376 - g_loss: -24468198.0000\n",
      "Epoch 187/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -17220.3753 - g_loss: -30824795.6667\n",
      "Epoch 188/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 36355.3789 - g_loss: -37242545.7083\n",
      "Epoch 189/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -12192.2381 - g_loss: -39909249.5833\n",
      "Epoch 190/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 16446.9816 - g_loss: -34440991.2917\n",
      "Epoch 191/2000\n",
      "47/47 [==============================] - 16s 346ms/step - d_loss: -57334.0750 - g_loss: -38619786.3333\n",
      "Epoch 192/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 25415.5591 - g_loss: -34326826.3750\n",
      "Epoch 193/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 361.1866 - g_loss: -28821255.1667\n",
      "Epoch 194/2000\n",
      "47/47 [==============================] - 16s 331ms/step - d_loss: 37059.5283 - g_loss: -28136019.4167\n",
      "Epoch 195/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -19879.0312 - g_loss: -27087656.5833\n",
      "Epoch 196/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 39210.7024 - g_loss: -35871686.7500\n",
      "Epoch 197/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -76735.7155 - g_loss: -45200843.4167\n",
      "Epoch 198/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 23521.7632 - g_loss: -54186649.8333\n",
      "Epoch 199/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 25892.0788 - g_loss: -48688280.3333\n",
      "Epoch 200/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -11299.9908 - g_loss: -47487779.1667\n",
      "Epoch 201/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -65048.1594 - g_loss: -46414350.1667\n",
      "Epoch 202/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -33261.1180 - g_loss: -48632146.6667\n",
      "Epoch 203/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: -39374.0849 - g_loss: -54394410.1667\n",
      "Epoch 204/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -89135.1644 - g_loss: -66290184.8333\n",
      "Epoch 205/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 101564.0103 - g_loss: -74936608.3333\n",
      "Epoch 206/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -62252.4913 - g_loss: -91444351.1667\n",
      "Epoch 207/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -67447.1486 - g_loss: -116895803.5000\n",
      "Epoch 208/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 187767.7978 - g_loss: -100541890.1667\n",
      "Epoch 209/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -177924.7604 - g_loss: -94725482.0000\n",
      "Epoch 210/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 139740.0787 - g_loss: -106157160.0000\n",
      "Epoch 211/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -149986.1543 - g_loss: -154203268.1667\n",
      "Epoch 212/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 157835.0239 - g_loss: -176535400.6667\n",
      "Epoch 213/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -157960.3254 - g_loss: -166180806.0000\n",
      "Epoch 214/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 260803.1252 - g_loss: -222009429.6667\n",
      "Epoch 215/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -36781.1802 - g_loss: -192621407.3333\n",
      "Epoch 216/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 116242.2646 - g_loss: -184980937.6667\n",
      "Epoch 217/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 26122.7932 - g_loss: -198913524.0000\n",
      "Epoch 218/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 209412.4220 - g_loss: -176985068.0000\n",
      "Epoch 219/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -130003.3338 - g_loss: -167993898.0000\n",
      "Epoch 220/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -20801.3709 - g_loss: -154367482.6667\n",
      "Epoch 221/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 337909.3315 - g_loss: -152565958.3333\n",
      "Epoch 222/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -131107.6249 - g_loss: -148706811.6667\n",
      "Epoch 223/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -66264.5072 - g_loss: -142323378.1667\n",
      "Epoch 224/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -271771.6253 - g_loss: -121896311.8333\n",
      "Epoch 225/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -50398.6630 - g_loss: -126620238.8333\n",
      "Epoch 226/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -30617.8223 - g_loss: -121893343.0000\n",
      "Epoch 227/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -437996.5856 - g_loss: -130166933.8333\n",
      "Epoch 228/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 148939.3713 - g_loss: -129685701.1667\n",
      "Epoch 229/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 124069.1805 - g_loss: -131618090.5000\n",
      "Epoch 230/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 372211.1363 - g_loss: -146136014.3333\n",
      "Epoch 231/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -230118.7437 - g_loss: -162270416.6667\n",
      "Epoch 232/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 70673.0037 - g_loss: -155183660.3333\n",
      "Epoch 233/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 28638.9155 - g_loss: -133369963.1667\n",
      "Epoch 234/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -187343.2086 - g_loss: -129370271.5000\n",
      "Epoch 235/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 20184.9220 - g_loss: -146156600.0000\n",
      "Epoch 236/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 253156.8941 - g_loss: -148495528.3333\n",
      "Epoch 237/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -128383.3344 - g_loss: -151003747.6667\n",
      "Epoch 238/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -363795.0300 - g_loss: -149507693.6667\n",
      "Epoch 239/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 192882.9928 - g_loss: -124816257.8333\n",
      "Epoch 240/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -26606.1774 - g_loss: -153136825.5000\n",
      "Epoch 241/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 193635.6324 - g_loss: -159001837.0000\n",
      "Epoch 242/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 318593.4003 - g_loss: -158918256.0000\n",
      "Epoch 243/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -25757.1610 - g_loss: -143047072.6667\n",
      "Epoch 244/2000\n",
      "47/47 [==============================] - 16s 350ms/step - d_loss: -41997.1435 - g_loss: -137816747.1667\n",
      "Epoch 245/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -158230.5290 - g_loss: -154611912.3333\n",
      "Epoch 246/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -562360.2904 - g_loss: -166718472.6667\n",
      "Epoch 247/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -33791.5256 - g_loss: -200695582.6667\n",
      "Epoch 248/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -176413.4401 - g_loss: -224671305.3333\n",
      "Epoch 249/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -332772.9159 - g_loss: -248182181.6667\n",
      "Epoch 250/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 710732.8021 - g_loss: -248495467.6667\n",
      "Epoch 251/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -125464.1899 - g_loss: -227953825.0000\n",
      "Epoch 252/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -199947.3070 - g_loss: -247167705.6667\n",
      "Epoch 253/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -506265.9619 - g_loss: -243270829.3333\n",
      "Epoch 254/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -319496.7682 - g_loss: -226148721.0000\n",
      "Epoch 255/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -179052.4497 - g_loss: -200856340.0000\n",
      "Epoch 256/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -107892.5908 - g_loss: -200408444.3333\n",
      "Epoch 257/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -223352.7540 - g_loss: -209141513.0000\n",
      "Epoch 258/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 573315.1073 - g_loss: -223774641.6667\n",
      "Epoch 259/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 137828.2792 - g_loss: -192279892.0000\n",
      "Epoch 260/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 312429.8570 - g_loss: -166891908.0000\n",
      "Epoch 261/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 233871.7167 - g_loss: -142310253.8333\n",
      "Epoch 262/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -270645.4319 - g_loss: -130212050.5000\n",
      "Epoch 263/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -58540.3774 - g_loss: -138887511.3333\n",
      "Epoch 264/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -106417.2863 - g_loss: -133657371.5000\n",
      "Epoch 265/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -132013.0559 - g_loss: -112042993.0000\n",
      "Epoch 266/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -238340.9876 - g_loss: -103167004.3333\n",
      "Epoch 267/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -155421.7183 - g_loss: -116061692.0000\n",
      "Epoch 268/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -110504.4135 - g_loss: -127859055.8333\n",
      "Epoch 269/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -162764.2523 - g_loss: -145366653.3333\n",
      "Epoch 270/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -319289.5775 - g_loss: -182744022.0000\n",
      "Epoch 271/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 197639.7615 - g_loss: -224810704.0000\n",
      "Epoch 272/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -202815.4967 - g_loss: -246350087.6667\n",
      "Epoch 273/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 371278.7467 - g_loss: -275423651.3333\n",
      "Epoch 274/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 402637.5371 - g_loss: -280903104.6667\n",
      "Epoch 275/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 274859.9079 - g_loss: -261929745.3333\n",
      "Epoch 276/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 171975.1587 - g_loss: -249424026.3333\n",
      "Epoch 277/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -306952.2263 - g_loss: -263193690.6667\n",
      "Epoch 278/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 157200.5153 - g_loss: -248522506.3333\n",
      "Epoch 279/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 184716.6466 - g_loss: -202552785.3333\n",
      "Epoch 280/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 119697.8070 - g_loss: -163454859.0000\n",
      "Epoch 281/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -21058.6211 - g_loss: -141082328.5000\n",
      "Epoch 282/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 179266.7718 - g_loss: -126324989.0000\n",
      "Epoch 283/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 21713.2444 - g_loss: -97770389.8333\n",
      "Epoch 284/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 277248.3899 - g_loss: -98644206.0000\n",
      "Epoch 285/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 24160.6698 - g_loss: -103305155.0000\n",
      "Epoch 286/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -72992.5741 - g_loss: -100216568.0000\n",
      "Epoch 287/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 9380.9497 - g_loss: -92169995.0000\n",
      "Epoch 288/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 31263.2375 - g_loss: -92005500.0000\n",
      "Epoch 289/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 115497.0052 - g_loss: -101511448.8333\n",
      "Epoch 290/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 237564.1308 - g_loss: -107256562.5000\n",
      "Epoch 291/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 20075.0849 - g_loss: -117196951.6667\n",
      "Epoch 292/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 308418.1913 - g_loss: -106934967.8333\n",
      "Epoch 293/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 112533.5926 - g_loss: -105498377.3333\n",
      "Epoch 294/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 77085.3615 - g_loss: -99860743.5000\n",
      "Epoch 295/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -143485.3721 - g_loss: -99178241.8333\n",
      "Epoch 296/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -201070.3846 - g_loss: -109767584.0000\n",
      "Epoch 297/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 226929.0451 - g_loss: -119679208.5000\n",
      "Epoch 298/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 164211.2018 - g_loss: -133721572.1667\n",
      "Epoch 299/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 201002.3697 - g_loss: -122272354.6667\n",
      "Epoch 300/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 21522.1208 - g_loss: -104852220.8333\n",
      "Epoch 301/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 124599.9665 - g_loss: -108095129.3333\n",
      "Epoch 302/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -323179.3081 - g_loss: -126770441.6667\n",
      "Epoch 303/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 229266.5941 - g_loss: -110549460.6667\n",
      "Epoch 304/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -52800.0286 - g_loss: -87813366.8333\n",
      "Epoch 305/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 171151.0342 - g_loss: -83856128.3333\n",
      "Epoch 306/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 140011.6172 - g_loss: -97122464.0000\n",
      "Epoch 307/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 87553.7568 - g_loss: -123757491.3333\n",
      "Epoch 308/2000\n",
      "47/47 [==============================] - 17s 358ms/step - d_loss: 80504.4931 - g_loss: -163408412.6667\n",
      "Epoch 309/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 122725.0496 - g_loss: -220424650.3333\n",
      "Epoch 310/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -364998.1956 - g_loss: -251102106.0000\n",
      "Epoch 311/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -121816.1751 - g_loss: -275372239.0000\n",
      "Epoch 312/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -389154.7582 - g_loss: -289609566.0000\n",
      "Epoch 313/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -238343.8851 - g_loss: -255936231.6667\n",
      "Epoch 314/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -564619.2686 - g_loss: -296052376.0000\n",
      "Epoch 315/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 711114.8307 - g_loss: -314509113.3333\n",
      "Epoch 316/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 62312.4390 - g_loss: -291348503.3333\n",
      "Epoch 317/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -14436.9005 - g_loss: -292688605.3333\n",
      "Epoch 318/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -462216.1898 - g_loss: -282457471.3333\n",
      "Epoch 319/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 181287.8336 - g_loss: -292543870.0000\n",
      "Epoch 320/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -355652.7784 - g_loss: -255438914.0000\n",
      "Epoch 321/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 32088.5965 - g_loss: -270026012.0000\n",
      "Epoch 322/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -409989.0828 - g_loss: -233395891.6667\n",
      "Epoch 323/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 100744.3727 - g_loss: -250054708.3333\n",
      "Epoch 324/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -54332.1837 - g_loss: -262235594.6667\n",
      "Epoch 325/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 500031.5435 - g_loss: -266378309.6667\n",
      "Epoch 326/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -305524.8791 - g_loss: -260921596.3333\n",
      "Epoch 327/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 99895.5221 - g_loss: -274570917.0000\n",
      "Epoch 328/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 264370.0025 - g_loss: -248407014.3333\n",
      "Epoch 329/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 96314.6261 - g_loss: -256264851.6667\n",
      "Epoch 330/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 504962.3278 - g_loss: -241315398.0000\n",
      "Epoch 331/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -15588.8114 - g_loss: -221444175.3333\n",
      "Epoch 332/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -219786.9082 - g_loss: -226226162.3333\n",
      "Epoch 333/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 65494.1074 - g_loss: -212271941.3333\n",
      "Epoch 334/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 431858.3838 - g_loss: -204391358.3333\n",
      "Epoch 335/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 63728.1388 - g_loss: -224825013.0000\n",
      "Epoch 336/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -330671.8662 - g_loss: -260749698.6667\n",
      "Epoch 337/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 13412.3602 - g_loss: -306362852.6667\n",
      "Epoch 338/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -449259.6426 - g_loss: -322686541.3333\n",
      "Epoch 339/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 532317.7372 - g_loss: -321982425.3333\n",
      "Epoch 340/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -368822.3954 - g_loss: -300551986.0000\n",
      "Epoch 341/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 516014.9205 - g_loss: -315565838.0000\n",
      "Epoch 342/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -592974.0689 - g_loss: -307845078.6667\n",
      "Epoch 343/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -40186.6433 - g_loss: -306998752.6667\n",
      "Epoch 344/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 387213.0977 - g_loss: -302497420.0000\n",
      "Epoch 345/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -41422.2786 - g_loss: -317643590.6667\n",
      "Epoch 346/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -97165.0839 - g_loss: -322698173.3333\n",
      "Epoch 347/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -114925.4025 - g_loss: -326593858.0000\n",
      "Epoch 348/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -500915.1484 - g_loss: -306543857.3333\n",
      "Epoch 349/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -463607.5334 - g_loss: -372090666.0000\n",
      "Epoch 350/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 191715.2701 - g_loss: -400315600.6667\n",
      "Epoch 351/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 671341.4076 - g_loss: -376335159.3333\n",
      "Epoch 352/2000\n",
      "47/47 [==============================] - 15s 315ms/step - d_loss: -256151.3255 - g_loss: -314754213.3333\n",
      "Epoch 353/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 36225.9304 - g_loss: -305745362.0000\n",
      "Epoch 354/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -94042.2414 - g_loss: -314009404.6667\n",
      "Epoch 355/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 15401.6467 - g_loss: -315801880.0000\n",
      "Epoch 356/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -267630.8020 - g_loss: -304192610.6667\n",
      "Epoch 357/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -460478.3320 - g_loss: -297304558.0000\n",
      "Epoch 358/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -411972.4875 - g_loss: -318542713.3333\n",
      "Epoch 359/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -461787.7818 - g_loss: -344922704.0000\n",
      "Epoch 360/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 643056.2174 - g_loss: -351957533.3333\n",
      "Epoch 361/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 8631.5649 - g_loss: -350343528.6667\n",
      "Epoch 362/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 496098.3618 - g_loss: -360567825.3333\n",
      "Epoch 363/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -498191.7599 - g_loss: -354278796.6667\n",
      "Epoch 364/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 964297.6449 - g_loss: -332458318.6667\n",
      "Epoch 365/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -520027.0116 - g_loss: -300003227.3333\n",
      "Epoch 366/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 279496.9948 - g_loss: -308359702.0000\n",
      "Epoch 367/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 345584.8636 - g_loss: -321622293.3333\n",
      "Epoch 368/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 310144.5214 - g_loss: -317564875.3333\n",
      "Epoch 369/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -197359.8893 - g_loss: -390910634.0000\n",
      "Epoch 370/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -257166.8698 - g_loss: -441765910.6667\n",
      "Epoch 371/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 212139.4685 - g_loss: -429875404.6667\n",
      "Epoch 372/2000\n",
      "47/47 [==============================] - 15s 315ms/step - d_loss: 375964.1711 - g_loss: -370664092.6667\n",
      "Epoch 373/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 367471.4115 - g_loss: -333787826.6667\n",
      "Epoch 374/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 416882.9677 - g_loss: -322043179.3333\n",
      "Epoch 375/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -861355.8807 - g_loss: -323084913.3333\n",
      "Epoch 376/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 713271.8767 - g_loss: -350574721.3333\n",
      "Epoch 377/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -347581.2240 - g_loss: -366182076.6667\n",
      "Epoch 378/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -506591.0814 - g_loss: -340014272.0000\n",
      "Epoch 379/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -16864.6637 - g_loss: -352113652.0000\n",
      "Epoch 380/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -91919.8429 - g_loss: -358668610.6667\n",
      "Epoch 381/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 314649.5074 - g_loss: -368687898.0000\n",
      "Epoch 382/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -276837.6873 - g_loss: -359125207.3333\n",
      "Epoch 383/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 792795.2276 - g_loss: -318243294.6667\n",
      "Epoch 384/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 753695.9375 - g_loss: -328127104.0000\n",
      "Epoch 385/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 123657.9251 - g_loss: -332083246.0000\n",
      "Epoch 386/2000\n",
      "47/47 [==============================] - 17s 372ms/step - d_loss: 488373.9617 - g_loss: -289247384.6667\n",
      "Epoch 387/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 139750.4844 - g_loss: -287053064.6667\n",
      "Epoch 388/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 262809.4771 - g_loss: -302173720.0000\n",
      "Epoch 389/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 250873.6007 - g_loss: -275353570.0000\n",
      "Epoch 390/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -382539.7589 - g_loss: -265490560.0000\n",
      "Epoch 391/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -16796.8516 - g_loss: -264515218.3333\n",
      "Epoch 392/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -65684.2425 - g_loss: -242655793.0000\n",
      "Epoch 393/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 194863.5143 - g_loss: -252830504.0000\n",
      "Epoch 394/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -261301.4021 - g_loss: -269421111.0000\n",
      "Epoch 395/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -465268.5815 - g_loss: -255482571.6667\n",
      "Epoch 396/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 369293.3289 - g_loss: -263268073.6667\n",
      "Epoch 397/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -777711.1655 - g_loss: -253706483.3333\n",
      "Epoch 398/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -108927.7719 - g_loss: -265484954.6667\n",
      "Epoch 399/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 395918.0202 - g_loss: -253290361.0000\n",
      "Epoch 400/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -208447.9935 - g_loss: -285282613.0000\n",
      "Epoch 401/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 44884.7725 - g_loss: -303887200.0000\n",
      "Epoch 402/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 928927.0511 - g_loss: -328305504.6667\n",
      "Epoch 403/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -449717.0653 - g_loss: -332914626.0000\n",
      "Epoch 404/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -954049.9878 - g_loss: -357826838.6667\n",
      "Epoch 405/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 10747.9619 - g_loss: -361017984.6667\n",
      "Epoch 406/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -321466.3014 - g_loss: -404747893.3333\n",
      "Epoch 407/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 923251.2039 - g_loss: -398327782.6667\n",
      "Epoch 408/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 495034.4419 - g_loss: -386747848.6667\n",
      "Epoch 409/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -151794.3640 - g_loss: -374694876.6667\n",
      "Epoch 410/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 639422.2261 - g_loss: -336173654.6667\n",
      "Epoch 411/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -673005.5234 - g_loss: -377505360.6667\n",
      "Epoch 412/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 691489.1301 - g_loss: -396552564.0000\n",
      "Epoch 413/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 414736.4866 - g_loss: -383739417.3333\n",
      "Epoch 414/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 691763.3312 - g_loss: -437226594.0000\n",
      "Epoch 415/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -287521.5933 - g_loss: -427373394.0000\n",
      "Epoch 416/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -322540.4033 - g_loss: -452681028.0000\n",
      "Epoch 417/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -1044765.0713 - g_loss: -501353038.6667\n",
      "Epoch 418/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -933466.6044 - g_loss: -649199445.3333\n",
      "Epoch 419/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -174181.9772 - g_loss: -724093960.0000\n",
      "Epoch 420/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -1533868.8884 - g_loss: -727502302.6667\n",
      "Epoch 421/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -1307847.2383 - g_loss: -705735993.3333\n",
      "Epoch 422/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -939781.2493 - g_loss: -659875529.3333\n",
      "Epoch 423/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 139705.6608 - g_loss: -657792528.0000\n",
      "Epoch 424/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -197627.8997 - g_loss: -628021164.0000\n",
      "Epoch 425/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 429662.8542 - g_loss: -667914262.6667\n",
      "Epoch 426/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 732951.1611 - g_loss: -677892148.0000\n",
      "Epoch 427/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1212395.0169 - g_loss: -664594568.0000\n",
      "Epoch 428/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -808010.3729 - g_loss: -591598560.0000\n",
      "Epoch 429/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 698851.1442 - g_loss: -519808504.6667\n",
      "Epoch 430/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 528674.6803 - g_loss: -527130264.6667\n",
      "Epoch 431/2000\n",
      "47/47 [==============================] - 16s 331ms/step - d_loss: 226848.8424 - g_loss: -510374415.3333\n",
      "Epoch 432/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 608173.7393 - g_loss: -448172867.3333\n",
      "Epoch 433/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -1166666.2642 - g_loss: -445612700.6667\n",
      "Epoch 434/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 64113.2842 - g_loss: -440849448.0000\n",
      "Epoch 435/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 24403.1810 - g_loss: -418490170.6667\n",
      "Epoch 436/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 787004.7047 - g_loss: -390306736.6667\n",
      "Epoch 437/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 878374.5226 - g_loss: -393468220.6667\n",
      "Epoch 438/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -492724.4987 - g_loss: -410957152.0000\n",
      "Epoch 439/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 16922.1387 - g_loss: -397605594.0000\n",
      "Epoch 440/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -606241.9153 - g_loss: -415554351.3333\n",
      "Epoch 441/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 297668.4505 - g_loss: -385992742.6667\n",
      "Epoch 442/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 100193.2002 - g_loss: -403814138.6667\n",
      "Epoch 443/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 331507.8011 - g_loss: -402988907.3333\n",
      "Epoch 444/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -819709.7086 - g_loss: -406528397.3333\n",
      "Epoch 445/2000\n",
      "47/47 [==============================] - 15s 315ms/step - d_loss: 173454.7176 - g_loss: -453785062.0000\n",
      "Epoch 446/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -800154.0586 - g_loss: -480525678.0000\n",
      "Epoch 447/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -960501.2904 - g_loss: -525791515.3333\n",
      "Epoch 448/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 99871.2431 - g_loss: -544622037.3333\n",
      "Epoch 449/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -733848.4253 - g_loss: -540007455.3333\n",
      "Epoch 450/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -332864.8522 - g_loss: -541868718.6667\n",
      "Epoch 451/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -325013.0146 - g_loss: -537774446.6667\n",
      "Epoch 452/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -485239.6047 - g_loss: -542142390.6667\n",
      "Epoch 453/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 586600.5125 - g_loss: -528847137.3333\n",
      "Epoch 454/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 685671.3639 - g_loss: -523543120.0000\n",
      "Epoch 455/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1062155.4906 - g_loss: -478420046.6667\n",
      "Epoch 456/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -1964273.0454 - g_loss: -483671547.3333\n",
      "Epoch 457/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -123657.9298 - g_loss: -522682329.3333\n",
      "Epoch 458/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 231830.5833 - g_loss: -495964990.0000\n",
      "Epoch 459/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 74163.1954 - g_loss: -504965338.0000\n",
      "Epoch 460/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 501495.9792 - g_loss: -591272204.0000\n",
      "Epoch 461/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 284799.9780 - g_loss: -698886149.3333\n",
      "Epoch 462/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 498925.2188 - g_loss: -659313946.6667\n",
      "Epoch 463/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 444346.6640 - g_loss: -652330358.6667\n",
      "Epoch 464/2000\n",
      "47/47 [==============================] - 16s 334ms/step - d_loss: -451404.5529 - g_loss: -650666368.0000\n",
      "Epoch 465/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -589886.5586 - g_loss: -726043834.6667\n",
      "Epoch 466/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 205360.2637 - g_loss: -743559010.6667\n",
      "Epoch 467/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1588429.3151 - g_loss: -798678221.3333\n",
      "Epoch 468/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1986026.6328 - g_loss: -750327429.3333\n",
      "Epoch 469/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -211649.5024 - g_loss: -732153249.3333\n",
      "Epoch 470/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -26109.9669 - g_loss: -756419810.6667\n",
      "Epoch 471/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 806632.6445 - g_loss: -751377677.3333\n",
      "Epoch 472/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -20998.8184 - g_loss: -821219622.6667\n",
      "Epoch 473/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 827077.0609 - g_loss: -738196173.3333\n",
      "Epoch 474/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 124509.3224 - g_loss: -718210338.6667\n",
      "Epoch 475/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -17019.2038 - g_loss: -650204237.3333\n",
      "Epoch 476/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -446359.7257 - g_loss: -560315858.6667\n",
      "Epoch 477/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -655659.6877 - g_loss: -534404189.3333\n",
      "Epoch 478/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 821389.5412 - g_loss: -525396303.3333\n",
      "Epoch 479/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 111979.7982 - g_loss: -572169717.3333\n",
      "Epoch 480/2000\n",
      "47/47 [==============================] - 18s 382ms/step - d_loss: 262269.0026 - g_loss: -570528736.0000\n",
      "Epoch 481/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 3617.2876 - g_loss: -565130442.0000\n",
      "Epoch 482/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -628226.1288 - g_loss: -583967292.0000\n",
      "Epoch 483/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -2070172.4128 - g_loss: -560615118.6667\n",
      "Epoch 484/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -207590.2424 - g_loss: -529587502.0000\n",
      "Epoch 485/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -604776.4225 - g_loss: -464186782.0000\n",
      "Epoch 486/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -256675.9076 - g_loss: -523490403.3333\n",
      "Epoch 487/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -651634.2823 - g_loss: -490230014.6667\n",
      "Epoch 488/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 755306.0482 - g_loss: -434848494.0000\n",
      "Epoch 489/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 706398.4886 - g_loss: -464204630.0000\n",
      "Epoch 490/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -590945.3803 - g_loss: -451317552.6667\n",
      "Epoch 491/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 658693.6531 - g_loss: -426798204.0000\n",
      "Epoch 492/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 118241.2754 - g_loss: -414773352.0000\n",
      "Epoch 493/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 1021732.1982 - g_loss: -379053604.6667\n",
      "Epoch 494/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -309863.7941 - g_loss: -329437556.6667\n",
      "Epoch 495/2000\n",
      "47/47 [==============================] - 16s 335ms/step - d_loss: 341819.8525 - g_loss: -288919580.0000\n",
      "Epoch 496/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -212113.3913 - g_loss: -286486694.6667\n",
      "Epoch 497/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -36169.2765 - g_loss: -281536417.0000\n",
      "Epoch 498/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 778530.1751 - g_loss: -266868941.6667\n",
      "Epoch 499/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 359488.1673 - g_loss: -272990696.0000\n",
      "Epoch 500/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 110110.7400 - g_loss: -248768620.0000\n",
      "Epoch 501/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -201357.4971 - g_loss: -238460404.6667\n",
      "Epoch 502/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 428988.2168 - g_loss: -223583991.0000\n",
      "Epoch 503/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -114048.8581 - g_loss: -235391666.3333\n",
      "Epoch 504/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 221550.8389 - g_loss: -242175653.6667\n",
      "Epoch 505/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -38817.5672 - g_loss: -276183018.0000\n",
      "Epoch 506/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -219456.5367 - g_loss: -261603951.0000\n",
      "Epoch 507/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -79362.1869 - g_loss: -231016128.3333\n",
      "Epoch 508/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -205763.0236 - g_loss: -190095586.0000\n",
      "Epoch 509/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 59216.5733 - g_loss: -187132741.0000\n",
      "Epoch 510/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 18352.6956 - g_loss: -210765059.6667\n",
      "Epoch 511/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 331348.1418 - g_loss: -200381733.0000\n",
      "Epoch 512/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -81196.1147 - g_loss: -180720580.6667\n",
      "Epoch 513/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 211802.1156 - g_loss: -180829104.3333\n",
      "Epoch 514/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 236688.7749 - g_loss: -172688917.0000\n",
      "Epoch 515/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 273766.6479 - g_loss: -162441168.6667\n",
      "Epoch 516/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 82560.5160 - g_loss: -146908599.0000\n",
      "Epoch 517/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 282222.7967 - g_loss: -154074010.6667\n",
      "Epoch 518/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 469437.3546 - g_loss: -157337660.3333\n",
      "Epoch 519/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 113239.7539 - g_loss: -175397519.0000\n",
      "Epoch 520/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 355474.8880 - g_loss: -189304351.0000\n",
      "Epoch 521/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -235633.0090 - g_loss: -193329364.0000\n",
      "Epoch 522/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 37543.8497 - g_loss: -208670629.3333\n",
      "Epoch 523/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 426042.7819 - g_loss: -244986674.3333\n",
      "Epoch 524/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 404528.1964 - g_loss: -231497187.3333\n",
      "Epoch 525/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -100421.3101 - g_loss: -201639393.6667\n",
      "Epoch 526/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -86027.4945 - g_loss: -211109607.6667\n",
      "Epoch 527/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -109118.2907 - g_loss: -204194161.0000\n",
      "Epoch 528/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 71081.5188 - g_loss: -189351892.0000\n",
      "Epoch 529/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 269230.8737 - g_loss: -167720866.3333\n",
      "Epoch 530/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 11379.9115 - g_loss: -147104200.3333\n",
      "Epoch 531/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -206188.7613 - g_loss: -134218613.3333\n",
      "Epoch 532/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -32675.0970 - g_loss: -127649122.0000\n",
      "Epoch 533/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 116898.5345 - g_loss: -113309095.6667\n",
      "Epoch 534/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 48274.3316 - g_loss: -101563977.6667\n",
      "Epoch 535/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -19368.2154 - g_loss: -104875090.0000\n",
      "Epoch 536/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -75849.4786 - g_loss: -109178483.5000\n",
      "Epoch 537/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -178565.2887 - g_loss: -103291595.8333\n",
      "Epoch 538/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 58927.9300 - g_loss: -145259175.8333\n",
      "Epoch 539/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -69260.7860 - g_loss: -163343639.3333\n",
      "Epoch 540/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -164429.8192 - g_loss: -190191577.3333\n",
      "Epoch 541/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -176089.7935 - g_loss: -200221630.6667\n",
      "Epoch 542/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 294987.4910 - g_loss: -177704956.0000\n",
      "Epoch 543/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -87872.3171 - g_loss: -184122409.3333\n",
      "Epoch 544/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 289632.3164 - g_loss: -236119741.6667\n",
      "Epoch 545/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 535739.8155 - g_loss: -226506679.6667\n",
      "Epoch 546/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 620517.5540 - g_loss: -268735162.6667\n",
      "Epoch 547/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -209251.3867 - g_loss: -287257157.6667\n",
      "Epoch 548/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -576104.0677 - g_loss: -323091341.3333\n",
      "Epoch 549/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -828592.8792 - g_loss: -369174717.3333\n",
      "Epoch 550/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 291424.2126 - g_loss: -343282454.6667\n",
      "Epoch 551/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -598509.6597 - g_loss: -345141271.3333\n",
      "Epoch 552/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -420713.7233 - g_loss: -364535594.0000\n",
      "Epoch 553/2000\n",
      "47/47 [==============================] - 15s 330ms/step - d_loss: -92401.2697 - g_loss: -360761944.6667\n",
      "Epoch 554/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 480532.6882 - g_loss: -363373112.0000\n",
      "Epoch 555/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -356724.7043 - g_loss: -415502277.3333\n",
      "Epoch 556/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 646844.4873 - g_loss: -417840676.0000\n",
      "Epoch 557/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 593902.1966 - g_loss: -437183253.3333\n",
      "Epoch 558/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -192723.8044 - g_loss: -474755765.3333\n",
      "Epoch 559/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 76933.3608 - g_loss: -476234591.3333\n",
      "Epoch 560/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -50871.2275 - g_loss: -400198933.3333\n",
      "Epoch 561/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -1155659.8636 - g_loss: -366248838.0000\n",
      "Epoch 562/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 227900.5948 - g_loss: -276229577.6667\n",
      "Epoch 563/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -437736.0552 - g_loss: -206894593.0000\n",
      "Epoch 564/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 282359.0308 - g_loss: -208075174.3333\n",
      "Epoch 565/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -33587.2214 - g_loss: -168162607.6667\n",
      "Epoch 566/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -25331.7344 - g_loss: -151901786.3333\n",
      "Epoch 567/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -100055.2492 - g_loss: -144200630.1667\n",
      "Epoch 568/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 108221.3383 - g_loss: -122240232.8333\n",
      "Epoch 569/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 147824.0811 - g_loss: -98336976.3333\n",
      "Epoch 570/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 3500.7277 - g_loss: -92810348.8333\n",
      "Epoch 571/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 12321.6262 - g_loss: -96086519.3333\n",
      "Epoch 572/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -857.1802 - g_loss: -122675843.0000\n",
      "Epoch 573/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 384186.4111 - g_loss: -127437363.3333\n",
      "Epoch 574/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -82577.7137 - g_loss: -104748744.5000\n",
      "Epoch 575/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -63971.0548 - g_loss: -114659969.8333\n",
      "Epoch 576/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -238946.0541 - g_loss: -119965148.1667\n",
      "Epoch 577/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 241793.7216 - g_loss: -112249124.1667\n",
      "Epoch 578/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -42234.7477 - g_loss: -92851494.1667\n",
      "Epoch 579/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 349988.3173 - g_loss: -80466144.5000\n",
      "Epoch 580/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 157448.6669 - g_loss: -75648289.1667\n",
      "Epoch 581/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 149662.4898 - g_loss: -76449010.4167\n",
      "Epoch 582/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 64083.2584 - g_loss: -79685943.4167\n",
      "Epoch 583/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -195120.1082 - g_loss: -104201253.3333\n",
      "Epoch 584/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -11641.0581 - g_loss: -111457838.5000\n",
      "Epoch 585/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -10049.1886 - g_loss: -103923204.8333\n",
      "Epoch 586/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -226041.7316 - g_loss: -97291063.5000\n",
      "Epoch 587/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 11788.0515 - g_loss: -70941940.1667\n",
      "Epoch 588/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 81588.5714 - g_loss: -70841858.5833\n",
      "Epoch 589/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -126281.4130 - g_loss: -76516694.1667\n",
      "Epoch 590/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -69762.0141 - g_loss: -90618792.6667\n",
      "Epoch 591/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -178865.4626 - g_loss: -99702095.5000\n",
      "Epoch 592/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 6743.2727 - g_loss: -96489633.6667\n",
      "Epoch 593/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 4547.2558 - g_loss: -62893967.8333\n",
      "Epoch 594/2000\n",
      "47/47 [==============================] - 18s 388ms/step - d_loss: -60806.8278 - g_loss: -45208962.5833\n",
      "Epoch 595/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -130790.1673 - g_loss: -51269423.6667\n",
      "Epoch 596/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 67472.2085 - g_loss: -42274839.4167\n",
      "Epoch 597/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -20679.0365 - g_loss: -35126088.1667\n",
      "Epoch 598/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -32105.1544 - g_loss: -23679799.0000\n",
      "Epoch 599/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: -41778.4053 - g_loss: -31810148.9167\n",
      "Epoch 600/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -14952.7086 - g_loss: -35688267.0833\n",
      "Epoch 601/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 46955.1054 - g_loss: -38827080.4167\n",
      "Epoch 602/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 32583.7111 - g_loss: -23285299.2083\n",
      "Epoch 603/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 12820.9910 - g_loss: -13378151.5625\n",
      "Epoch 604/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 10138.8946 - g_loss: -10003135.5938\n",
      "Epoch 605/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -14777.3289 - g_loss: -10117333.2708\n",
      "Epoch 606/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -11563.3936 - g_loss: -10182973.1875\n",
      "Epoch 607/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -30033.9393 - g_loss: -10759726.8125\n",
      "Epoch 608/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -71111.8295 - g_loss: -31259738.9167\n",
      "Epoch 609/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 78625.6837 - g_loss: -45354575.6667\n",
      "Epoch 610/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 46216.4395 - g_loss: -38835738.0000\n",
      "Epoch 611/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -35411.0663 - g_loss: -37062407.2917\n",
      "Epoch 612/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -265.5685 - g_loss: -40673805.6667\n",
      "Epoch 613/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 57888.8388 - g_loss: -32954278.5833\n",
      "Epoch 614/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -20415.6666 - g_loss: -29924902.5000\n",
      "Epoch 615/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 18859.0848 - g_loss: -37141381.0000\n",
      "Epoch 616/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -79241.4653 - g_loss: -39979341.7083\n",
      "Epoch 617/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -39231.2501 - g_loss: -53981923.4167\n",
      "Epoch 618/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -14891.7292 - g_loss: -46934817.4167\n",
      "Epoch 619/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -75552.6315 - g_loss: -52193982.3333\n",
      "Epoch 620/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 31938.5313 - g_loss: -48514736.7500\n",
      "Epoch 621/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -72165.8846 - g_loss: -63905589.7500\n",
      "Epoch 622/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -101627.9837 - g_loss: -66509523.0000\n",
      "Epoch 623/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 125369.7125 - g_loss: -67406324.8333\n",
      "Epoch 624/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 43343.5159 - g_loss: -54511761.1667\n",
      "Epoch 625/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 102498.0470 - g_loss: -56551917.4167\n",
      "Epoch 626/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 92952.7218 - g_loss: -66785243.8333\n",
      "Epoch 627/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -96576.4529 - g_loss: -75753951.2500\n",
      "Epoch 628/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -450702.0202 - g_loss: -91272970.5000\n",
      "Epoch 629/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -182472.9766 - g_loss: -128618913.5000\n",
      "Epoch 630/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 146954.3947 - g_loss: -150052482.3333\n",
      "Epoch 631/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 59611.1968 - g_loss: -122290541.8333\n",
      "Epoch 632/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -327025.4741 - g_loss: -132609713.1667\n",
      "Epoch 633/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 176581.1284 - g_loss: -133495133.5000\n",
      "Epoch 634/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -123959.1812 - g_loss: -146410819.6667\n",
      "Epoch 635/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -82850.2262 - g_loss: -181473217.6667\n",
      "Epoch 636/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 149137.7548 - g_loss: -161237307.6667\n",
      "Epoch 637/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 103756.1929 - g_loss: -154674238.3333\n",
      "Epoch 638/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 306618.2941 - g_loss: -128714217.8333\n",
      "Epoch 639/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 197899.4213 - g_loss: -88815364.3333\n",
      "Epoch 640/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 9854.7009 - g_loss: -53693014.9167\n",
      "Epoch 641/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -71359.3304 - g_loss: -55549445.3333\n",
      "Epoch 642/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 25177.1637 - g_loss: -69515900.2500\n",
      "Epoch 643/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 73878.9170 - g_loss: -77949084.5000\n",
      "Epoch 644/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 77496.4387 - g_loss: -78208503.5000\n",
      "Epoch 645/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -55350.1588 - g_loss: -73879771.6667\n",
      "Epoch 646/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -95420.2489 - g_loss: -66253715.5833\n",
      "Epoch 647/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 114311.1614 - g_loss: -45308023.7500\n",
      "Epoch 648/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 130099.5066 - g_loss: -39912959.5000\n",
      "Epoch 649/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 40483.1516 - g_loss: -38907823.1250\n",
      "Epoch 650/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -15364.9220 - g_loss: -38370821.3333\n",
      "Epoch 651/2000\n",
      "47/47 [==============================] - 15s 315ms/step - d_loss: -22884.4639 - g_loss: -44332627.0833\n",
      "Epoch 652/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -42856.4832 - g_loss: -49656295.6667\n",
      "Epoch 653/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 93470.2909 - g_loss: -45374302.1667\n",
      "Epoch 654/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -109198.9703 - g_loss: -50115309.2500\n",
      "Epoch 655/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 74834.0355 - g_loss: -51062509.7500\n",
      "Epoch 656/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 205144.1748 - g_loss: -50588076.0000\n",
      "Epoch 657/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 47882.4324 - g_loss: -40847566.2500\n",
      "Epoch 658/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -8721.7444 - g_loss: -40451973.6667\n",
      "Epoch 659/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 107013.0591 - g_loss: -36509041.3750\n",
      "Epoch 660/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -73925.7500 - g_loss: -35974323.1250\n",
      "Epoch 661/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -172285.9915 - g_loss: -54229867.5833\n",
      "Epoch 662/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 168414.8880 - g_loss: -47760546.3333\n",
      "Epoch 663/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -15541.2658 - g_loss: -30993314.5000\n",
      "Epoch 664/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -65194.7804 - g_loss: -31982283.7917\n",
      "Epoch 665/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -3758.9818 - g_loss: -29126126.2500\n",
      "Epoch 666/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 67827.0946 - g_loss: -24641808.3750\n",
      "Epoch 667/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -303.2181 - g_loss: -27354365.9167\n",
      "Epoch 668/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 67052.5678 - g_loss: -28056612.5000\n",
      "Epoch 669/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -31670.3134 - g_loss: -30487246.9167\n",
      "Epoch 670/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 33626.5851 - g_loss: -34937543.2917\n",
      "Epoch 671/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -43550.3906 - g_loss: -37288131.9583\n",
      "Epoch 672/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 80080.4915 - g_loss: -46778057.5000\n",
      "Epoch 673/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -9752.5233 - g_loss: -34452364.9583\n",
      "Epoch 674/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 23054.9113 - g_loss: -28441054.0000\n",
      "Epoch 675/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -21517.0532 - g_loss: -24734507.7500\n",
      "Epoch 676/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 67847.6962 - g_loss: -21303882.1042\n",
      "Epoch 677/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 61482.2065 - g_loss: -24809807.7083\n",
      "Epoch 678/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 74702.6625 - g_loss: -22839954.9167\n",
      "Epoch 679/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -44406.0769 - g_loss: -19512195.9167\n",
      "Epoch 680/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -12461.5495 - g_loss: -13138634.4167\n",
      "Epoch 681/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -17378.5535 - g_loss: -12968287.6146\n",
      "Epoch 682/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 20495.0778 - g_loss: -15650004.6458\n",
      "Epoch 683/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 15078.1316 - g_loss: -17412543.1667\n",
      "Epoch 684/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -12581.6519 - g_loss: -19706729.7500\n",
      "Epoch 685/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 7796.9476 - g_loss: -24501525.9583\n",
      "Epoch 686/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 5497.7427 - g_loss: -23058429.2500\n",
      "Epoch 687/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 84177.6090 - g_loss: -34541045.2500\n",
      "Epoch 688/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 118800.8298 - g_loss: -53001688.2500\n",
      "Epoch 689/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -54831.3759 - g_loss: -46766007.3333\n",
      "Epoch 690/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -56196.2295 - g_loss: -42692649.7500\n",
      "Epoch 691/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 32969.2925 - g_loss: -53856254.6667\n",
      "Epoch 692/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 149744.0033 - g_loss: -38483924.3750\n",
      "Epoch 693/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -217605.5088 - g_loss: -59240213.0000\n",
      "Epoch 694/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -164863.5586 - g_loss: -80518703.8333\n",
      "Epoch 695/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 26305.4638 - g_loss: -100861520.6667\n",
      "Epoch 696/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -200026.8026 - g_loss: -99253576.8333\n",
      "Epoch 697/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -462576.8227 - g_loss: -111366761.5000\n",
      "Epoch 698/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 234816.0691 - g_loss: -99496035.5000\n",
      "Epoch 699/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 105518.7591 - g_loss: -121906947.6667\n",
      "Epoch 700/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -88150.6600 - g_loss: -126005197.8333\n",
      "Epoch 701/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: -135371.4355 - g_loss: -132348596.8333\n",
      "Epoch 702/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 198940.9350 - g_loss: -163818482.6667\n",
      "Epoch 703/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 161469.9951 - g_loss: -169473883.6667\n",
      "Epoch 704/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 183117.1151 - g_loss: -149220728.0000\n",
      "Epoch 705/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 356643.7696 - g_loss: -152333880.3333\n",
      "Epoch 706/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 166004.2943 - g_loss: -137150178.3333\n",
      "Epoch 707/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -234114.5470 - g_loss: -133618637.5000\n",
      "Epoch 708/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -600.6899 - g_loss: -120151426.3333\n",
      "Epoch 709/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 123076.3848 - g_loss: -107418896.8333\n",
      "Epoch 710/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 52306.0510 - g_loss: -141232032.8333\n",
      "Epoch 711/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1038472.9749 - g_loss: -173134512.3333\n",
      "Epoch 712/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -236665.6439 - g_loss: -174923515.6667\n",
      "Epoch 713/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -41915.0577 - g_loss: -150461908.0000\n",
      "Epoch 714/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 77942.5996 - g_loss: -117756788.5000\n",
      "Epoch 715/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 367893.5915 - g_loss: -112431544.1667\n",
      "Epoch 716/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 147584.8265 - g_loss: -131386129.0000\n",
      "Epoch 717/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -359340.4475 - g_loss: -145995794.3333\n",
      "Epoch 718/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -14199.7535 - g_loss: -152505838.3333\n",
      "Epoch 719/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -280840.0553 - g_loss: -178818466.3333\n",
      "Epoch 720/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 119879.1313 - g_loss: -166576053.3333\n",
      "Epoch 721/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 634469.7439 - g_loss: -156577416.6667\n",
      "Epoch 722/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 102646.7804 - g_loss: -114460389.1667\n",
      "Epoch 723/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -150870.5160 - g_loss: -113469283.0000\n",
      "Epoch 724/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -536806.5029 - g_loss: -139503268.8333\n",
      "Epoch 725/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 486623.1283 - g_loss: -140374378.8333\n",
      "Epoch 726/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -57478.7741 - g_loss: -122885782.5000\n",
      "Epoch 727/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -357492.2061 - g_loss: -115216596.5000\n",
      "Epoch 728/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 159869.7916 - g_loss: -145512700.6667\n",
      "Epoch 729/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -643438.7975 - g_loss: -175919221.6667\n",
      "Epoch 730/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 459653.4021 - g_loss: -198251152.6667\n",
      "Epoch 731/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 538515.9623 - g_loss: -159521010.0000\n",
      "Epoch 732/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 262106.1351 - g_loss: -151144620.3333\n",
      "Epoch 733/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 389791.8845 - g_loss: -142611254.6667\n",
      "Epoch 734/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 164004.8079 - g_loss: -140374343.3333\n",
      "Epoch 735/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 147015.6626 - g_loss: -122541408.1667\n",
      "Epoch 736/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -357452.5323 - g_loss: -114529271.5000\n",
      "Epoch 737/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 11957.7464 - g_loss: -89482904.3333\n",
      "Epoch 738/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 62890.7109 - g_loss: -93691894.6667\n",
      "Epoch 739/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -240893.8437 - g_loss: -106804946.1667\n",
      "Epoch 740/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 184503.9603 - g_loss: -102594116.5000\n",
      "Epoch 741/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -323687.5133 - g_loss: -121629943.8333\n",
      "Epoch 742/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -14923.6467 - g_loss: -148844695.6667\n",
      "Epoch 743/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -380163.0644 - g_loss: -142168721.8333\n",
      "Epoch 744/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1290.4403 - g_loss: -135723997.5000\n",
      "Epoch 745/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 58918.0124 - g_loss: -131938395.6667\n",
      "Epoch 746/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 81754.1758 - g_loss: -113216508.8333\n",
      "Epoch 747/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 449429.0605 - g_loss: -109896862.3333\n",
      "Epoch 748/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -49087.4950 - g_loss: -123712868.1667\n",
      "Epoch 749/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -62458.0493 - g_loss: -131316751.1667\n",
      "Epoch 750/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 19914.3975 - g_loss: -118199665.8333\n",
      "Epoch 751/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -295802.9294 - g_loss: -130908030.8333\n",
      "Epoch 752/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 345395.1530 - g_loss: -149851282.1667\n",
      "Epoch 753/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -16860.1720 - g_loss: -166039055.6667\n",
      "Epoch 754/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 50781.6177 - g_loss: -169270793.3333\n",
      "Epoch 755/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 375252.1102 - g_loss: -155304275.0000\n",
      "Epoch 756/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -19212.4766 - g_loss: -151439888.3333\n",
      "Epoch 757/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 237478.9008 - g_loss: -154005069.3333\n",
      "Epoch 758/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -232974.8144 - g_loss: -134217345.0000\n",
      "Epoch 759/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 405129.0809 - g_loss: -114516389.5000\n",
      "Epoch 760/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 50819.4432 - g_loss: -133522164.1667\n",
      "Epoch 761/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 669138.6222 - g_loss: -120519740.3333\n",
      "Epoch 762/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -345852.7953 - g_loss: -136328818.5000\n",
      "Epoch 763/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -65155.1688 - g_loss: -157463824.6667\n",
      "Epoch 764/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 892082.2561 - g_loss: -162423619.0000\n",
      "Epoch 765/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 156893.2050 - g_loss: -119965921.5000\n",
      "Epoch 766/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -146934.4490 - g_loss: -85619748.3333\n",
      "Epoch 767/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -145124.7019 - g_loss: -88483041.6667\n",
      "Epoch 768/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 60590.0695 - g_loss: -95835111.5000\n",
      "Epoch 769/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -222220.5035 - g_loss: -104890654.6667\n",
      "Epoch 770/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -121165.2603 - g_loss: -137677228.1667\n",
      "Epoch 771/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -40177.2507 - g_loss: -155647329.3333\n",
      "Epoch 772/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 345839.7054 - g_loss: -168463414.3333\n",
      "Epoch 773/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -9185.2843 - g_loss: -146484844.0000\n",
      "Epoch 774/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 386378.6886 - g_loss: -107891495.8333\n",
      "Epoch 775/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 337206.5492 - g_loss: -87045894.6667\n",
      "Epoch 776/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 90425.8512 - g_loss: -73357942.8333\n",
      "Epoch 777/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 103505.6761 - g_loss: -68620745.5833\n",
      "Epoch 778/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 120079.8585 - g_loss: -64123988.7500\n",
      "Epoch 779/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -72631.2516 - g_loss: -70978512.1667\n",
      "Epoch 780/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 108791.0612 - g_loss: -73401777.3333\n",
      "Epoch 781/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -22916.2564 - g_loss: -82787254.5000\n",
      "Epoch 782/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 63604.7171 - g_loss: -92094091.0000\n",
      "Epoch 783/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 204813.7887 - g_loss: -92697339.3333\n",
      "Epoch 784/2000\n",
      "47/47 [==============================] - 15s 315ms/step - d_loss: 356881.6284 - g_loss: -81845858.5000\n",
      "Epoch 785/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -39772.7425 - g_loss: -65873877.7500\n",
      "Epoch 786/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 159432.5196 - g_loss: -62927998.1667\n",
      "Epoch 787/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 111234.4159 - g_loss: -60897447.9167\n",
      "Epoch 788/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 42586.7660 - g_loss: -65572061.5000\n",
      "Epoch 789/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 40353.6955 - g_loss: -64656686.2500\n",
      "Epoch 790/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -91289.0888 - g_loss: -64970524.7500\n",
      "Epoch 791/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -116033.2947 - g_loss: -63015374.3333\n",
      "Epoch 792/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 28799.8884 - g_loss: -71269816.2500\n",
      "Epoch 793/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -102785.2232 - g_loss: -68117274.0000\n",
      "Epoch 794/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -233599.6958 - g_loss: -68981878.5000\n",
      "Epoch 795/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 70930.2517 - g_loss: -79900679.1667\n",
      "Epoch 796/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 105702.1265 - g_loss: -66833908.9167\n",
      "Epoch 797/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 91129.5343 - g_loss: -62398279.1667\n",
      "Epoch 798/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -54904.9774 - g_loss: -77225426.8333\n",
      "Epoch 799/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 56414.9136 - g_loss: -67099586.0000\n",
      "Epoch 800/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -50128.6879 - g_loss: -84869829.5000\n",
      "Epoch 801/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 39534.9525 - g_loss: -86516442.0000\n",
      "Epoch 802/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 214670.5424 - g_loss: -80898459.5000\n",
      "Epoch 803/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -224962.0813 - g_loss: -81246309.3333\n",
      "Epoch 804/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -318970.7052 - g_loss: -91956506.3333\n",
      "Epoch 805/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 268683.8735 - g_loss: -134728160.5000\n",
      "Epoch 806/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -76286.8803 - g_loss: -136978650.0000\n",
      "Epoch 807/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -32150.0312 - g_loss: -154874698.0000\n",
      "Epoch 808/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 801785.4948 - g_loss: -140313595.8333\n",
      "Epoch 809/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 74034.3007 - g_loss: -167808041.3333\n",
      "Epoch 810/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 282103.1340 - g_loss: -154451413.6667\n",
      "Epoch 811/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 546709.5370 - g_loss: -168510429.6667\n",
      "Epoch 812/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 291558.2788 - g_loss: -163827041.0000\n",
      "Epoch 813/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -940227.8919 - g_loss: -177005591.0000\n",
      "Epoch 814/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 495111.6185 - g_loss: -170383024.6667\n",
      "Epoch 815/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 106513.3411 - g_loss: -167341132.6667\n",
      "Epoch 816/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -152872.5133 - g_loss: -162456415.0000\n",
      "Epoch 817/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -689371.8815 - g_loss: -156153995.6667\n",
      "Epoch 818/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 81364.4033 - g_loss: -143059510.1667\n",
      "Epoch 819/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -137884.5394 - g_loss: -141974034.8333\n",
      "Epoch 820/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -643873.8223 - g_loss: -196222691.3333\n",
      "Epoch 821/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 647175.1012 - g_loss: -223737343.3333\n",
      "Epoch 822/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 284407.8963 - g_loss: -249050964.0000\n",
      "Epoch 823/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 397623.6346 - g_loss: -226487782.6667\n",
      "Epoch 824/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -546108.7304 - g_loss: -238368362.6667\n",
      "Epoch 825/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -360345.0524 - g_loss: -264969372.6667\n",
      "Epoch 826/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -326291.5487 - g_loss: -296662064.0000\n",
      "Epoch 827/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -590139.8015 - g_loss: -301125784.6667\n",
      "Epoch 828/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -378607.8420 - g_loss: -304446605.3333\n",
      "Epoch 829/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 904373.9474 - g_loss: -320484052.0000\n",
      "Epoch 830/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 630005.1214 - g_loss: -278108227.6667\n",
      "Epoch 831/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 83353.0894 - g_loss: -255354001.6667\n",
      "Epoch 832/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -297097.5319 - g_loss: -233417801.3333\n",
      "Epoch 833/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -224897.4290 - g_loss: -251402002.0000\n",
      "Epoch 834/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 352306.0605 - g_loss: -235426764.6667\n",
      "Epoch 835/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 279158.6354 - g_loss: -224142365.6667\n",
      "Epoch 836/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 165334.1434 - g_loss: -223923748.0000\n",
      "Epoch 837/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 86687.4827 - g_loss: -204116379.0000\n",
      "Epoch 838/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -426791.0468 - g_loss: -178474797.0000\n",
      "Epoch 839/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 481843.0205 - g_loss: -178220153.6667\n",
      "Epoch 840/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 952760.3704 - g_loss: -175345057.0000\n",
      "Epoch 841/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 538598.6698 - g_loss: -153917471.0000\n",
      "Epoch 842/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 78924.1800 - g_loss: -149979508.3333\n",
      "Epoch 843/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: -332183.6914 - g_loss: -149037367.3333\n",
      "Epoch 844/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 561596.4515 - g_loss: -133739745.1667\n",
      "Epoch 845/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 560947.1777 - g_loss: -124764212.0000\n",
      "Epoch 846/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 308212.3127 - g_loss: -106326665.8333\n",
      "Epoch 847/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 216226.7650 - g_loss: -105333532.1667\n",
      "Epoch 848/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -341228.3090 - g_loss: -116652153.6667\n",
      "Epoch 849/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -813372.3528 - g_loss: -133601836.5000\n",
      "Epoch 850/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -294869.5829 - g_loss: -159296410.0000\n",
      "Epoch 851/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 123462.0475 - g_loss: -168874979.0000\n",
      "Epoch 852/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 656587.5426 - g_loss: -183078061.0000\n",
      "Epoch 853/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -305008.3011 - g_loss: -173046110.3333\n",
      "Epoch 854/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -211276.1060 - g_loss: -169372635.6667\n",
      "Epoch 855/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -11349.8312 - g_loss: -184561639.0000\n",
      "Epoch 856/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 123311.3532 - g_loss: -190020845.3333\n",
      "Epoch 857/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 535514.2096 - g_loss: -197091261.0000\n",
      "Epoch 858/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 361669.1048 - g_loss: -190397165.6667\n",
      "Epoch 859/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 137134.0485 - g_loss: -193431542.3333\n",
      "Epoch 860/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 163571.3929 - g_loss: -218257880.6667\n",
      "Epoch 861/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -153027.4792 - g_loss: -187851995.3333\n",
      "Epoch 862/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -909528.8338 - g_loss: -175831471.0000\n",
      "Epoch 863/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 571214.8439 - g_loss: -197013465.6667\n",
      "Epoch 864/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 855336.3711 - g_loss: -218053628.6667\n",
      "Epoch 865/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -1167345.7594 - g_loss: -250030185.6667\n",
      "Epoch 866/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -388712.4730 - g_loss: -289567272.0000\n",
      "Epoch 867/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -172053.0929 - g_loss: -283015219.0000\n",
      "Epoch 868/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -1005250.4737 - g_loss: -295220559.3333\n",
      "Epoch 869/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 1081227.1888 - g_loss: -294034090.0000\n",
      "Epoch 870/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -1019556.3092 - g_loss: -320905358.6667\n",
      "Epoch 871/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1941893.3595 - g_loss: -300789408.0000\n",
      "Epoch 872/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -1057955.1439 - g_loss: -268158248.6667\n",
      "Epoch 873/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -1010343.0723 - g_loss: -249115756.6667\n",
      "Epoch 874/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -190433.8073 - g_loss: -251165936.0000\n",
      "Epoch 875/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 386625.2057 - g_loss: -233539704.3333\n",
      "Epoch 876/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -963710.2751 - g_loss: -216848127.3333\n",
      "Epoch 877/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -540498.7580 - g_loss: -196328603.0000\n",
      "Epoch 878/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 56840.6401 - g_loss: -192744563.6667\n",
      "Epoch 879/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 991764.8006 - g_loss: -204120504.6667\n",
      "Epoch 880/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -464732.2015 - g_loss: -196215926.3333\n",
      "Epoch 881/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 360517.8327 - g_loss: -171746634.0000\n",
      "Epoch 882/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1065466.6050 - g_loss: -133862881.6667\n",
      "Epoch 883/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -20223.9335 - g_loss: -124144671.3333\n",
      "Epoch 884/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -320361.8276 - g_loss: -135019666.6667\n",
      "Epoch 885/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 1082651.2349 - g_loss: -166676997.6667\n",
      "Epoch 886/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -419317.2117 - g_loss: -173321928.0000\n",
      "Epoch 887/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 676762.0736 - g_loss: -180316435.0000\n",
      "Epoch 888/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 124377.7272 - g_loss: -159938911.3333\n",
      "Epoch 889/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 604396.2827 - g_loss: -145717014.1667\n",
      "Epoch 890/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 477054.8288 - g_loss: -127627729.8333\n",
      "Epoch 891/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 8786.6131 - g_loss: -130772010.5000\n",
      "Epoch 892/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -619866.6471 - g_loss: -156525677.0000\n",
      "Epoch 893/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -151636.0511 - g_loss: -161940882.6667\n",
      "Epoch 894/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 661336.4630 - g_loss: -151762250.6667\n",
      "Epoch 895/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 105516.5874 - g_loss: -155974912.6667\n",
      "Epoch 896/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 1200794.7166 - g_loss: -132390417.6667\n",
      "Epoch 897/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -63235.4469 - g_loss: -138096068.3333\n",
      "Epoch 898/2000\n",
      "47/47 [==============================] - 20s 426ms/step - d_loss: 627264.6904 - g_loss: -154478483.3333\n",
      "Epoch 899/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -298441.1082 - g_loss: -176895570.6667\n",
      "Epoch 900/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1570486.6771 - g_loss: -206023558.6667\n",
      "Epoch 901/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 354358.5482 - g_loss: -205829480.6667\n",
      "Epoch 902/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 112955.7867 - g_loss: -163267809.6667\n",
      "Epoch 903/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 503626.3201 - g_loss: -160362983.3333\n",
      "Epoch 904/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -538111.1118 - g_loss: -152123433.0000\n",
      "Epoch 905/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 321041.4705 - g_loss: -167382572.0000\n",
      "Epoch 906/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -576705.9769 - g_loss: -190634490.0000\n",
      "Epoch 907/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -658927.9434 - g_loss: -211461646.0000\n",
      "Epoch 908/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -264371.8021 - g_loss: -220618676.6667\n",
      "Epoch 909/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -93828.4521 - g_loss: -242589323.6667\n",
      "Epoch 910/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -154965.8343 - g_loss: -264092667.0000\n",
      "Epoch 911/2000\n",
      "47/47 [==============================] - 16s 333ms/step - d_loss: 1165458.2625 - g_loss: -215617376.0000\n",
      "Epoch 912/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 286828.1582 - g_loss: -182992482.0000\n",
      "Epoch 913/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: 383118.0645 - g_loss: -200504923.0000\n",
      "Epoch 914/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -746254.1107 - g_loss: -225361816.3333\n",
      "Epoch 915/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -298933.8063 - g_loss: -211560755.3333\n",
      "Epoch 916/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 502942.1559 - g_loss: -207974541.3333\n",
      "Epoch 917/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -791654.9570 - g_loss: -219036985.3333\n",
      "Epoch 918/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -632057.5469 - g_loss: -261909949.0000\n",
      "Epoch 919/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 984076.4113 - g_loss: -287470685.3333\n",
      "Epoch 920/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -306993.6853 - g_loss: -311010424.6667\n",
      "Epoch 921/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -83313.5820 - g_loss: -347122057.3333\n",
      "Epoch 922/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -1381582.9657 - g_loss: -338324488.0000\n",
      "Epoch 923/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -241744.1022 - g_loss: -402467361.3333\n",
      "Epoch 924/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 1433681.7438 - g_loss: -394397096.6667\n",
      "Epoch 925/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 398263.4753 - g_loss: -350402722.0000\n",
      "Epoch 926/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1658444.5391 - g_loss: -310917366.6667\n",
      "Epoch 927/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1232864.2874 - g_loss: -297403612.6667\n",
      "Epoch 928/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -1252807.9596 - g_loss: -275405990.6667\n",
      "Epoch 929/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -619852.7254 - g_loss: -305783254.6667\n",
      "Epoch 930/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -121808.6969 - g_loss: -322283683.3333\n",
      "Epoch 931/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 181503.5352 - g_loss: -301813230.6667\n",
      "Epoch 932/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1842414.7044 - g_loss: -316994064.0000\n",
      "Epoch 933/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1132119.8685 - g_loss: -394496672.6667\n",
      "Epoch 934/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 511609.6748 - g_loss: -457459080.0000\n",
      "Epoch 935/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 563771.2764 - g_loss: -518899616.6667\n",
      "Epoch 936/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -34869.1380 - g_loss: -504843826.0000\n",
      "Epoch 937/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2885007.9274 - g_loss: -501288616.6667\n",
      "Epoch 938/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 2892825.7106 - g_loss: -491742366.6667\n",
      "Epoch 939/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 827613.4766 - g_loss: -495785308.0000\n",
      "Epoch 940/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 105611.8079 - g_loss: -535226304.0000\n",
      "Epoch 941/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 1962934.7884 - g_loss: -579332349.3333\n",
      "Epoch 942/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -3919363.5967 - g_loss: -629391314.6667\n",
      "Epoch 943/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1356097.0924 - g_loss: -656586524.0000\n",
      "Epoch 944/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 3321704.0378 - g_loss: -653477680.0000\n",
      "Epoch 945/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1631724.7083 - g_loss: -725328304.0000\n",
      "Epoch 946/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 1326688.9232 - g_loss: -831045557.3333\n",
      "Epoch 947/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 25134.4271 - g_loss: -859209852.0000\n",
      "Epoch 948/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 2043562.3151 - g_loss: -907310361.3333\n",
      "Epoch 949/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -307649.0260 - g_loss: -872242717.3333\n",
      "Epoch 950/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 6075953.0879 - g_loss: -782098193.3333\n",
      "Epoch 951/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 514605.6745 - g_loss: -711635408.0000\n",
      "Epoch 952/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -4493239.0182 - g_loss: -748785224.0000\n",
      "Epoch 953/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 994374.5085 - g_loss: -821227864.0000\n",
      "Epoch 954/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1278805.5960 - g_loss: -749858816.0000\n",
      "Epoch 955/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 4180533.1440 - g_loss: -732146302.6667\n",
      "Epoch 956/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 909738.3008 - g_loss: -780941626.6667\n",
      "Epoch 957/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -2009950.7266 - g_loss: -811161002.6667\n",
      "Epoch 958/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 2841928.3802 - g_loss: -871560136.0000\n",
      "Epoch 959/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1055513.2780 - g_loss: -833366760.0000\n",
      "Epoch 960/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 2367075.8503 - g_loss: -791611916.0000\n",
      "Epoch 961/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 1841333.1576 - g_loss: -762294710.6667\n",
      "Epoch 962/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 609033.4792 - g_loss: -704945782.6667\n",
      "Epoch 963/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -6269564.4909 - g_loss: -709427577.3333\n",
      "Epoch 964/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 558513.1960 - g_loss: -774298938.6667\n",
      "Epoch 965/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 5318763.3184 - g_loss: -798665700.0000\n",
      "Epoch 966/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -617910.5677 - g_loss: -798340040.0000\n",
      "Epoch 967/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 7653372.2539 - g_loss: -735410420.0000\n",
      "Epoch 968/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1556398.8906 - g_loss: -690632097.3333\n",
      "Epoch 969/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2199971.0254 - g_loss: -691296688.0000\n",
      "Epoch 970/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -1545669.7852 - g_loss: -675133864.0000\n",
      "Epoch 971/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 428215.9674 - g_loss: -705676750.6667\n",
      "Epoch 972/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -5338036.6237 - g_loss: -676992256.0000\n",
      "Epoch 973/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 3130577.6995 - g_loss: -673192916.0000\n",
      "Epoch 974/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 870450.0833 - g_loss: -582498183.3333\n",
      "Epoch 975/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1081930.0492 - g_loss: -532530906.0000\n",
      "Epoch 976/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -2730173.6357 - g_loss: -579373168.0000\n",
      "Epoch 977/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -609118.5755 - g_loss: -639913726.6667\n",
      "Epoch 978/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -3576200.5030 - g_loss: -648760490.6667\n",
      "Epoch 979/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1101847.8903 - g_loss: -592260810.6667\n",
      "Epoch 980/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 2065796.3936 - g_loss: -601032236.0000\n",
      "Epoch 981/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -2824466.2578 - g_loss: -611499868.0000\n",
      "Epoch 982/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 3350042.2617 - g_loss: -602951564.0000\n",
      "Epoch 983/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 1819672.0326 - g_loss: -569090900.0000\n",
      "Epoch 984/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 1119272.7858 - g_loss: -590427216.0000\n",
      "Epoch 985/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -1037189.2572 - g_loss: -583682302.6667\n",
      "Epoch 986/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -2339462.7617 - g_loss: -605297050.6667\n",
      "Epoch 987/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -812530.4244 - g_loss: -657723753.3333\n",
      "Epoch 988/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1761375.3857 - g_loss: -697703360.0000\n",
      "Epoch 989/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 1880413.0202 - g_loss: -688773940.0000\n",
      "Epoch 990/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -306124.6012 - g_loss: -692932828.0000\n",
      "Epoch 991/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 665729.7778 - g_loss: -683059853.3333\n",
      "Epoch 992/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -2166167.4349 - g_loss: -702710234.6667\n",
      "Epoch 993/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 2643987.3047 - g_loss: -714001070.6667\n",
      "Epoch 994/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 530485.1908 - g_loss: -689346734.6667\n",
      "Epoch 995/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 763724.4049 - g_loss: -689664497.3333\n",
      "Epoch 996/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1897055.0970 - g_loss: -648219956.0000\n",
      "Epoch 997/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 698378.5339 - g_loss: -572190936.0000\n",
      "Epoch 998/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 2317459.4154 - g_loss: -564384937.3333\n",
      "Epoch 999/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1409889.6011 - g_loss: -575598697.3333\n",
      "Epoch 1000/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1892085.4531 - g_loss: -621302094.6667\n",
      "Epoch 1001/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -1812208.3307 - g_loss: -628938454.6667\n",
      "Epoch 1002/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 2107728.1224 - g_loss: -655169442.6667\n",
      "Epoch 1003/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 2281965.5951 - g_loss: -637543166.6667\n",
      "Epoch 1004/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1681928.5488 - g_loss: -628508970.6667\n",
      "Epoch 1005/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -2262899.1042 - g_loss: -687881920.0000\n",
      "Epoch 1006/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -65653.2786 - g_loss: -755835118.6667\n",
      "Epoch 1007/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -6457450.3464 - g_loss: -746007482.6667\n",
      "Epoch 1008/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 2017363.3340 - g_loss: -733849028.0000\n",
      "Epoch 1009/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 124662.7660 - g_loss: -772412569.3333\n",
      "Epoch 1010/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 197671.4831 - g_loss: -757769473.3333\n",
      "Epoch 1011/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -4350798.4816 - g_loss: -804380678.6667\n",
      "Epoch 1012/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 1948324.7474 - g_loss: -779214182.6667\n",
      "Epoch 1013/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -731169.6162 - g_loss: -740769394.6667\n",
      "Epoch 1014/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 2428991.1680 - g_loss: -741585089.3333\n",
      "Epoch 1015/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -3927376.8764 - g_loss: -697915366.6667\n",
      "Epoch 1016/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -4093533.0833 - g_loss: -661157868.0000\n",
      "Epoch 1017/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1952558.7777 - g_loss: -637493593.3333\n",
      "Epoch 1018/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -3510460.0924 - g_loss: -639906048.0000\n",
      "Epoch 1019/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -1818492.3854 - g_loss: -588109313.3333\n",
      "Epoch 1020/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -4875064.2865 - g_loss: -561608920.0000\n",
      "Epoch 1021/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -405685.2552 - g_loss: -484050204.6667\n",
      "Epoch 1022/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -2745659.8229 - g_loss: -454320317.3333\n",
      "Epoch 1023/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1241013.2526 - g_loss: -433976653.3333\n",
      "Epoch 1024/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1806559.8861 - g_loss: -435150326.6667\n",
      "Epoch 1025/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -3166701.7477 - g_loss: -481237804.0000\n",
      "Epoch 1026/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 768861.0846 - g_loss: -511298081.3333\n",
      "Epoch 1027/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1972288.3761 - g_loss: -548504146.0000\n",
      "Epoch 1028/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 4715976.6478 - g_loss: -527482551.3333\n",
      "Epoch 1029/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 973059.6745 - g_loss: -496190832.6667\n",
      "Epoch 1030/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 398135.0911 - g_loss: -442883376.6667\n",
      "Epoch 1031/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 711528.5977 - g_loss: -438919489.3333\n",
      "Epoch 1032/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 354279.3664 - g_loss: -434689972.0000\n",
      "Epoch 1033/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 4502450.8174 - g_loss: -416117038.6667\n",
      "Epoch 1034/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 2582492.7201 - g_loss: -380724488.0000\n",
      "Epoch 1035/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 931516.7038 - g_loss: -341697153.3333\n",
      "Epoch 1036/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 2316971.5321 - g_loss: -349903987.3333\n",
      "Epoch 1037/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -1000931.9733 - g_loss: -360070584.0000\n",
      "Epoch 1038/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1286145.3177 - g_loss: -409575276.0000\n",
      "Epoch 1039/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1443373.9596 - g_loss: -424572478.6667\n",
      "Epoch 1040/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1520967.6481 - g_loss: -382799179.3333\n",
      "Epoch 1041/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 723800.5547 - g_loss: -336358559.3333\n",
      "Epoch 1042/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1050927.4442 - g_loss: -360646895.3333\n",
      "Epoch 1043/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1334992.5928 - g_loss: -395745298.6667\n",
      "Epoch 1044/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -475491.4067 - g_loss: -368441506.0000\n",
      "Epoch 1045/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 1298576.7137 - g_loss: -319668778.0000\n",
      "Epoch 1046/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 637237.9251 - g_loss: -287636568.6667\n",
      "Epoch 1047/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -1586024.2637 - g_loss: -275557035.6667\n",
      "Epoch 1048/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1517922.3709 - g_loss: -276827298.3333\n",
      "Epoch 1049/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -198760.0369 - g_loss: -282789116.0000\n",
      "Epoch 1050/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -379196.1379 - g_loss: -320266466.0000\n",
      "Epoch 1051/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1432193.9875 - g_loss: -270938821.6667\n",
      "Epoch 1052/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 394005.2380 - g_loss: -220865951.0000\n",
      "Epoch 1053/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -581123.2669 - g_loss: -212042139.0000\n",
      "Epoch 1054/2000\n",
      "47/47 [==============================] - 15s 315ms/step - d_loss: 294174.1764 - g_loss: -231829465.3333\n",
      "Epoch 1055/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1437226.3320 - g_loss: -237983410.3333\n",
      "Epoch 1056/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 1532404.0833 - g_loss: -244418007.0000\n",
      "Epoch 1057/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -335352.0666 - g_loss: -256710627.6667\n",
      "Epoch 1058/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 197302.3340 - g_loss: -243384923.6667\n",
      "Epoch 1059/2000\n",
      "47/47 [==============================] - 15s 315ms/step - d_loss: 489335.2756 - g_loss: -267600976.3333\n",
      "Epoch 1060/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 111419.4349 - g_loss: -251286311.3333\n",
      "Epoch 1061/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -548777.6797 - g_loss: -215863937.3333\n",
      "Epoch 1062/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2403963.9137 - g_loss: -241552106.6667\n",
      "Epoch 1063/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -299415.2552 - g_loss: -207171917.0000\n",
      "Epoch 1064/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -742341.0892 - g_loss: -208623333.3333\n",
      "Epoch 1065/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -614758.9779 - g_loss: -201082483.0000\n",
      "Epoch 1066/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 177646.2715 - g_loss: -169205130.3333\n",
      "Epoch 1067/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 866407.2864 - g_loss: -185107982.0000\n",
      "Epoch 1068/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 508262.5916 - g_loss: -156351849.0000\n",
      "Epoch 1069/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1022697.2939 - g_loss: -121112363.0000\n",
      "Epoch 1070/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 294984.7474 - g_loss: -106186826.5000\n",
      "Epoch 1071/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -262124.3457 - g_loss: -107922705.3333\n",
      "Epoch 1072/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 465509.3556 - g_loss: -100823182.6667\n",
      "Epoch 1073/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 39503.2792 - g_loss: -97891706.0000\n",
      "Epoch 1074/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 280738.7684 - g_loss: -77404275.8333\n",
      "Epoch 1075/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -384677.0976 - g_loss: -84359542.1667\n",
      "Epoch 1076/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -307219.2278 - g_loss: -105798827.6667\n",
      "Epoch 1077/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -268228.5280 - g_loss: -121000002.6667\n",
      "Epoch 1078/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -120513.2829 - g_loss: -115766319.8333\n",
      "Epoch 1079/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 76756.7958 - g_loss: -93667889.8333\n",
      "Epoch 1080/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 422912.9627 - g_loss: -83350191.5000\n",
      "Epoch 1081/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -646681.7357 - g_loss: -73262006.5000\n",
      "Epoch 1082/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -805040.4023 - g_loss: -87858888.5000\n",
      "Epoch 1083/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 542364.4889 - g_loss: -91398735.1667\n",
      "Epoch 1084/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -134747.5504 - g_loss: -77356452.3333\n",
      "Epoch 1085/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -137559.8024 - g_loss: -68824096.2500\n",
      "Epoch 1086/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -137277.4925 - g_loss: -70770025.4167\n",
      "Epoch 1087/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 148481.7909 - g_loss: -56679295.0000\n",
      "Epoch 1088/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -81587.5924 - g_loss: -53921395.6667\n",
      "Epoch 1089/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 236219.9410 - g_loss: -59354566.7500\n",
      "Epoch 1090/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -38764.5208 - g_loss: -63607293.8333\n",
      "Epoch 1091/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 181254.3423 - g_loss: -50624466.3333\n",
      "Epoch 1092/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -238811.1018 - g_loss: -53940936.4167\n",
      "Epoch 1093/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -77252.3818 - g_loss: -62872575.5833\n",
      "Epoch 1094/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -204281.7395 - g_loss: -84475448.5000\n",
      "Epoch 1095/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -35257.8322 - g_loss: -78265096.8333\n",
      "Epoch 1096/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 263441.4684 - g_loss: -93908123.8333\n",
      "Epoch 1097/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -196908.8875 - g_loss: -83984759.1667\n",
      "Epoch 1098/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 143203.7567 - g_loss: -108930164.8333\n",
      "Epoch 1099/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 348963.5426 - g_loss: -130481637.8333\n",
      "Epoch 1100/2000\n",
      "47/47 [==============================] - 22s 460ms/step - d_loss: 738737.3664 - g_loss: -131642382.0000\n",
      "Epoch 1101/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -299418.7604 - g_loss: -121662599.0000\n",
      "Epoch 1102/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 78412.3112 - g_loss: -118123998.1667\n",
      "Epoch 1103/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 77469.6549 - g_loss: -101745554.6667\n",
      "Epoch 1104/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 465143.0855 - g_loss: -98256112.0000\n",
      "Epoch 1105/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -109481.7653 - g_loss: -97614584.5000\n",
      "Epoch 1106/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -124854.9342 - g_loss: -99255715.6667\n",
      "Epoch 1107/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -5867.1727 - g_loss: -83234945.1667\n",
      "Epoch 1108/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -237155.7304 - g_loss: -81635781.1667\n",
      "Epoch 1109/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 594064.2205 - g_loss: -79088092.0000\n",
      "Epoch 1110/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -222466.8617 - g_loss: -82240501.3333\n",
      "Epoch 1111/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -169897.3652 - g_loss: -102395736.3333\n",
      "Epoch 1112/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -462704.1960 - g_loss: -102674390.6667\n",
      "Epoch 1113/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 82782.1766 - g_loss: -82243277.8333\n",
      "Epoch 1114/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -5064.7041 - g_loss: -77747048.0000\n",
      "Epoch 1115/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -170747.1623 - g_loss: -76245589.6667\n",
      "Epoch 1116/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 166136.9797 - g_loss: -81641271.6667\n",
      "Epoch 1117/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -216056.9574 - g_loss: -97871991.8333\n",
      "Epoch 1118/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 4046.0356 - g_loss: -128002585.6667\n",
      "Epoch 1119/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -441911.9022 - g_loss: -107916089.6667\n",
      "Epoch 1120/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -245979.9953 - g_loss: -111236220.5000\n",
      "Epoch 1121/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 221205.9219 - g_loss: -104858761.6667\n",
      "Epoch 1122/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -419491.0745 - g_loss: -99193114.8333\n",
      "Epoch 1123/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -424836.0026 - g_loss: -104087363.5000\n",
      "Epoch 1124/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -172371.9208 - g_loss: -128571139.8333\n",
      "Epoch 1125/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 200679.6629 - g_loss: -115495033.0000\n",
      "Epoch 1126/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 304387.6284 - g_loss: -127464635.5000\n",
      "Epoch 1127/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 218193.3813 - g_loss: -105851435.8333\n",
      "Epoch 1128/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 763294.7396 - g_loss: -100650585.0000\n",
      "Epoch 1129/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 297801.2276 - g_loss: -68801022.0833\n",
      "Epoch 1130/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 19058.2384 - g_loss: -67390120.3333\n",
      "Epoch 1131/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 278898.2507 - g_loss: -71673995.5000\n",
      "Epoch 1132/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 304377.1432 - g_loss: -68660844.0000\n",
      "Epoch 1133/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -107765.8395 - g_loss: -80510894.3333\n",
      "Epoch 1134/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -201568.5885 - g_loss: -77459878.9167\n",
      "Epoch 1135/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 94108.0927 - g_loss: -75308487.8333\n",
      "Epoch 1136/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -470245.1436 - g_loss: -79772207.5000\n",
      "Epoch 1137/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -31074.7961 - g_loss: -76472692.3333\n",
      "Epoch 1138/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 12609.0338 - g_loss: -70947968.0833\n",
      "Epoch 1139/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 261712.4992 - g_loss: -69555235.0000\n",
      "Epoch 1140/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 29308.8395 - g_loss: -83864056.5000\n",
      "Epoch 1141/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 369479.1674 - g_loss: -73221677.3333\n",
      "Epoch 1142/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -815877.9432 - g_loss: -94531833.1667\n",
      "Epoch 1143/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -67886.5479 - g_loss: -89616259.0000\n",
      "Epoch 1144/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -31796.5192 - g_loss: -84514813.0000\n",
      "Epoch 1145/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -208983.9291 - g_loss: -91774742.3333\n",
      "Epoch 1146/2000\n",
      "47/47 [==============================] - 15s 330ms/step - d_loss: 435296.3369 - g_loss: -84408638.0000\n",
      "Epoch 1147/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 208358.6278 - g_loss: -85907722.1667\n",
      "Epoch 1148/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 1255.9879 - g_loss: -84767978.1667\n",
      "Epoch 1149/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 143241.6906 - g_loss: -67432045.4167\n",
      "Epoch 1150/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -70997.7676 - g_loss: -65068434.3333\n",
      "Epoch 1151/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 53580.3303 - g_loss: -79774948.2500\n",
      "Epoch 1152/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 325287.4590 - g_loss: -83423319.8333\n",
      "Epoch 1153/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1672.8283 - g_loss: -74181234.6667\n",
      "Epoch 1154/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -279858.2418 - g_loss: -79584127.6667\n",
      "Epoch 1155/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -144891.4968 - g_loss: -71042204.3333\n",
      "Epoch 1156/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -224748.6953 - g_loss: -78799494.5000\n",
      "Epoch 1157/2000\n",
      "47/47 [==============================] - 16s 334ms/step - d_loss: -208965.6377 - g_loss: -72528390.2500\n",
      "Epoch 1158/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -173729.2728 - g_loss: -69558267.8333\n",
      "Epoch 1159/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 186699.5241 - g_loss: -65118370.4167\n",
      "Epoch 1160/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 77762.2527 - g_loss: -49956464.0000\n",
      "Epoch 1161/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -147671.7613 - g_loss: -54995843.5000\n",
      "Epoch 1162/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -2189.4416 - g_loss: -58830943.2500\n",
      "Epoch 1163/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 66557.2904 - g_loss: -66407128.2500\n",
      "Epoch 1164/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -42534.5333 - g_loss: -57466616.2500\n",
      "Epoch 1165/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -136733.3177 - g_loss: -49028537.6667\n",
      "Epoch 1166/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 55607.2272 - g_loss: -57248977.8333\n",
      "Epoch 1167/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -57112.5781 - g_loss: -46748284.5000\n",
      "Epoch 1168/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 60067.8090 - g_loss: -39070474.4167\n",
      "Epoch 1169/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 115250.1167 - g_loss: -27537046.2917\n",
      "Epoch 1170/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 38326.0192 - g_loss: -27578985.2083\n",
      "Epoch 1171/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -54948.3362 - g_loss: -25281474.6667\n",
      "Epoch 1172/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -56421.3486 - g_loss: -25096014.5000\n",
      "Epoch 1173/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -23858.2850 - g_loss: -25509315.8750\n",
      "Epoch 1174/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -81379.3508 - g_loss: -26764312.6667\n",
      "Epoch 1175/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -43948.7313 - g_loss: -20560497.0833\n",
      "Epoch 1176/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 55749.0170 - g_loss: -23390015.9583\n",
      "Epoch 1177/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 26698.0664 - g_loss: -23589387.7500\n",
      "Epoch 1178/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 7513.7236 - g_loss: -18072767.5208\n",
      "Epoch 1179/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -58788.6963 - g_loss: -21367535.3125\n",
      "Epoch 1180/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 44734.8024 - g_loss: -30837481.3333\n",
      "Epoch 1181/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 46884.7207 - g_loss: -26795583.9583\n",
      "Epoch 1182/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 63613.2671 - g_loss: -25119968.6250\n",
      "Epoch 1183/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -33955.1501 - g_loss: -28954873.8333\n",
      "Epoch 1184/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -101951.2984 - g_loss: -28305736.3750\n",
      "Epoch 1185/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -48624.0726 - g_loss: -35626540.8333\n",
      "Epoch 1186/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -178338.3162 - g_loss: -53436017.4167\n",
      "Epoch 1187/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 3411.3730 - g_loss: -70413914.3333\n",
      "Epoch 1188/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -331675.4056 - g_loss: -76971087.2500\n",
      "Epoch 1189/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -160109.7885 - g_loss: -87476074.1667\n",
      "Epoch 1190/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -310051.6073 - g_loss: -90748972.5000\n",
      "Epoch 1191/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -103110.8062 - g_loss: -96666728.1667\n",
      "Epoch 1192/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -98198.8320 - g_loss: -111675777.3333\n",
      "Epoch 1193/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 173357.6252 - g_loss: -119046568.8333\n",
      "Epoch 1194/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -304088.0060 - g_loss: -114804547.8333\n",
      "Epoch 1195/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 376868.1713 - g_loss: -119573367.8333\n",
      "Epoch 1196/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -237683.0098 - g_loss: -127470528.1667\n",
      "Epoch 1197/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -755013.2913 - g_loss: -126134830.0000\n",
      "Epoch 1198/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -357652.5999 - g_loss: -126232080.1667\n",
      "Epoch 1199/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -124524.2754 - g_loss: -147909085.6667\n",
      "Epoch 1200/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -136839.0824 - g_loss: -127872096.8333\n",
      "Epoch 1201/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 51668.4851 - g_loss: -119282618.1667\n",
      "Epoch 1202/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 35994.7244 - g_loss: -107714366.8333\n",
      "Epoch 1203/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 268343.5996 - g_loss: -92465092.0000\n",
      "Epoch 1204/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 157927.8937 - g_loss: -49532697.5417\n",
      "Epoch 1205/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 88173.9894 - g_loss: -34881242.2083\n",
      "Epoch 1206/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -37445.5461 - g_loss: -32510368.5417\n",
      "Epoch 1207/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -225109.8459 - g_loss: -46216519.2500\n",
      "Epoch 1208/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 178000.0635 - g_loss: -56713333.7500\n",
      "Epoch 1209/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 48768.3692 - g_loss: -50003075.4167\n",
      "Epoch 1210/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 171502.4743 - g_loss: -52631421.4167\n",
      "Epoch 1211/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -18312.8545 - g_loss: -46622469.0000\n",
      "Epoch 1212/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 36897.3393 - g_loss: -38895432.9583\n",
      "Epoch 1213/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -54855.7046 - g_loss: -41769749.5000\n",
      "Epoch 1214/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -111568.2129 - g_loss: -38686348.1667\n",
      "Epoch 1215/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -10913.4063 - g_loss: -41175084.4167\n",
      "Epoch 1216/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -17753.0163 - g_loss: -41249216.7500\n",
      "Epoch 1217/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -161204.2683 - g_loss: -44213570.0833\n",
      "Epoch 1218/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -11941.7514 - g_loss: -50730609.5000\n",
      "Epoch 1219/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -90270.3147 - g_loss: -52395807.7500\n",
      "Epoch 1220/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 44509.3795 - g_loss: -48802167.2500\n",
      "Epoch 1221/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 47298.6724 - g_loss: -49790290.5833\n",
      "Epoch 1222/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 146952.1635 - g_loss: -46800179.3333\n",
      "Epoch 1223/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -34409.1053 - g_loss: -48504353.0833\n",
      "Epoch 1224/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -1429.3986 - g_loss: -33120356.1250\n",
      "Epoch 1225/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -27456.8948 - g_loss: -23468700.2917\n",
      "Epoch 1226/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 60968.2115 - g_loss: -21142995.2708\n",
      "Epoch 1227/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 31099.6906 - g_loss: -18354873.4583\n",
      "Epoch 1228/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 25461.4255 - g_loss: -9318080.0104\n",
      "Epoch 1229/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -71098.1393 - g_loss: -11964153.0833\n",
      "Epoch 1230/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -29743.2190 - g_loss: -12925755.7708\n",
      "Epoch 1231/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -60073.6228 - g_loss: -10969486.4688\n",
      "Epoch 1232/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 56411.4585 - g_loss: -25395470.3125\n",
      "Epoch 1233/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -40778.8622 - g_loss: -29961056.6250\n",
      "Epoch 1234/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -21794.8362 - g_loss: -24193726.7500\n",
      "Epoch 1235/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -18270.5279 - g_loss: -15862854.5833\n",
      "Epoch 1236/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -64947.8630 - g_loss: -11667192.0833\n",
      "Epoch 1237/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 26306.0107 - g_loss: -12367841.1875\n",
      "Epoch 1238/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -448.3687 - g_loss: -8853904.6042\n",
      "Epoch 1239/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 16895.8716 - g_loss: -11472965.8229\n",
      "Epoch 1240/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 85593.6162 - g_loss: -21674896.5833\n",
      "Epoch 1241/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 51349.4201 - g_loss: -16705579.7083\n",
      "Epoch 1242/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 34841.2047 - g_loss: -14840343.1042\n",
      "Epoch 1243/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -20184.8928 - g_loss: -15345248.0833\n",
      "Epoch 1244/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 43253.5114 - g_loss: -13252595.1250\n",
      "Epoch 1245/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 53845.5011 - g_loss: -21669259.7083\n",
      "Epoch 1246/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -36640.3535 - g_loss: -22812355.3750\n",
      "Epoch 1247/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -18770.5380 - g_loss: -37532951.3750\n",
      "Epoch 1248/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 126084.7792 - g_loss: -33775890.8333\n",
      "Epoch 1249/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -57078.6677 - g_loss: -38918729.9167\n",
      "Epoch 1250/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -237969.0006 - g_loss: -31828563.0833\n",
      "Epoch 1251/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 92524.8876 - g_loss: -31055993.7917\n",
      "Epoch 1252/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -75093.0541 - g_loss: -35937608.4583\n",
      "Epoch 1253/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -71538.3067 - g_loss: -44481306.5833\n",
      "Epoch 1254/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 163317.7008 - g_loss: -31547195.1667\n",
      "Epoch 1255/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -217207.3424 - g_loss: -36758769.5833\n",
      "Epoch 1256/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 105851.7255 - g_loss: -35357983.2500\n",
      "Epoch 1257/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -75013.0588 - g_loss: -34122274.8333\n",
      "Epoch 1258/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -135379.3990 - g_loss: -42439257.2500\n",
      "Epoch 1259/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 291424.7619 - g_loss: -52651663.2500\n",
      "Epoch 1260/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 108351.9653 - g_loss: -43292129.5833\n",
      "Epoch 1261/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 292927.0123 - g_loss: -33742042.2083\n",
      "Epoch 1262/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -60421.0589 - g_loss: -37038182.5417\n",
      "Epoch 1263/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -103341.8528 - g_loss: -33936039.7917\n",
      "Epoch 1264/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 114634.4793 - g_loss: -46497849.2500\n",
      "Epoch 1265/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: -54808.2451 - g_loss: -43912104.5000\n",
      "Epoch 1266/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -58611.7128 - g_loss: -41723585.9167\n",
      "Epoch 1267/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 153516.9155 - g_loss: -33540148.7917\n",
      "Epoch 1268/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -69515.9849 - g_loss: -29032530.4583\n",
      "Epoch 1269/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 67146.1960 - g_loss: -32388162.0833\n",
      "Epoch 1270/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -41268.8185 - g_loss: -41201166.9167\n",
      "Epoch 1271/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -105199.4972 - g_loss: -39162577.7083\n",
      "Epoch 1272/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 115290.2315 - g_loss: -35264136.4583\n",
      "Epoch 1273/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: 87261.1329 - g_loss: -27826569.3750\n",
      "Epoch 1274/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -69823.4408 - g_loss: -16348412.5417\n",
      "Epoch 1275/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 41007.8432 - g_loss: -13920405.8333\n",
      "Epoch 1276/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -23687.3349 - g_loss: -12185218.8333\n",
      "Epoch 1277/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -48834.2751 - g_loss: -10985914.8750\n",
      "Epoch 1278/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 18978.8280 - g_loss: -9190137.4688\n",
      "Epoch 1279/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -18360.9763 - g_loss: -8147855.0833\n",
      "Epoch 1280/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -51416.9677 - g_loss: -11321377.4062\n",
      "Epoch 1281/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -32772.1514 - g_loss: -13444799.9375\n",
      "Epoch 1282/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -22533.4275 - g_loss: -16240782.6042\n",
      "Epoch 1283/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -46256.5443 - g_loss: -8188082.4479\n",
      "Epoch 1284/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -49545.0553 - g_loss: -9957646.1042\n",
      "Epoch 1285/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 13780.2246 - g_loss: -3718854.6042\n",
      "Epoch 1286/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 29697.9566 - g_loss: -7214451.2188\n",
      "Epoch 1287/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -9743.7296 - g_loss: -5735966.1042\n",
      "Epoch 1288/2000\n",
      "47/47 [==============================] - 15s 330ms/step - d_loss: -22771.2693 - g_loss: -2772488.3099\n",
      "Epoch 1289/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 28665.4074 - g_loss: -8263635.7500\n",
      "Epoch 1290/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -29145.9285 - g_loss: -3940732.1562\n",
      "Epoch 1291/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 10285.3396 - g_loss: -10991526.9896\n",
      "Epoch 1292/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -189445.0506 - g_loss: -26286619.0000\n",
      "Epoch 1293/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 74509.9593 - g_loss: -43066410.4167\n",
      "Epoch 1294/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 313744.1545 - g_loss: -53041037.8333\n",
      "Epoch 1295/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -181255.7772 - g_loss: -60776063.7500\n",
      "Epoch 1296/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -110511.0293 - g_loss: -69079626.5833\n",
      "Epoch 1297/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 183269.2119 - g_loss: -80094103.1667\n",
      "Epoch 1298/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 111062.7625 - g_loss: -72019514.2500\n",
      "Epoch 1299/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -181198.5630 - g_loss: -74574556.2500\n",
      "Epoch 1300/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -75275.2712 - g_loss: -65279236.0000\n",
      "Epoch 1301/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 29517.4204 - g_loss: -80120710.6667\n",
      "Epoch 1302/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 37602.5304 - g_loss: -92329817.0000\n",
      "Epoch 1303/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 708327.4465 - g_loss: -101452082.6667\n",
      "Epoch 1304/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 647584.2760 - g_loss: -117737710.6667\n",
      "Epoch 1305/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -45862.7174 - g_loss: -139634182.0000\n",
      "Epoch 1306/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -140663.1958 - g_loss: -156575637.6667\n",
      "Epoch 1307/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -111580.3101 - g_loss: -184845711.0000\n",
      "Epoch 1308/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -989886.4722 - g_loss: -210791466.6667\n",
      "Epoch 1309/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -931416.6797 - g_loss: -217998748.6667\n",
      "Epoch 1310/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 270090.0146 - g_loss: -245890698.3333\n",
      "Epoch 1311/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 4312.2129 - g_loss: -296337725.6667\n",
      "Epoch 1312/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 511627.9442 - g_loss: -292536923.3333\n",
      "Epoch 1313/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 1511317.8211 - g_loss: -272782212.6667\n",
      "Epoch 1314/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -604336.3711 - g_loss: -256480927.3333\n",
      "Epoch 1315/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 145414.1367 - g_loss: -249378378.6667\n",
      "Epoch 1316/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -418675.7795 - g_loss: -222906107.0000\n",
      "Epoch 1317/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -919305.2231 - g_loss: -239745893.3333\n",
      "Epoch 1318/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 316504.9781 - g_loss: -258770580.0000\n",
      "Epoch 1319/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1004684.3939 - g_loss: -320521718.6667\n",
      "Epoch 1320/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1320241.1660 - g_loss: -326488166.6667\n",
      "Epoch 1321/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 1050376.5098 - g_loss: -255043658.6667\n",
      "Epoch 1322/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -912030.9580 - g_loss: -235811365.0000\n",
      "Epoch 1323/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 159876.6979 - g_loss: -207028971.3333\n",
      "Epoch 1324/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 255985.9173 - g_loss: -246469176.0000\n",
      "Epoch 1325/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 134555.1829 - g_loss: -251370271.6667\n",
      "Epoch 1326/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1539559.5966 - g_loss: -282343043.6667\n",
      "Epoch 1327/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 610130.0088 - g_loss: -298760076.6667\n",
      "Epoch 1328/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -1637399.4891 - g_loss: -272607027.6667\n",
      "Epoch 1329/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 2247194.7303 - g_loss: -247631904.0000\n",
      "Epoch 1330/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -416312.2968 - g_loss: -213212305.3333\n",
      "Epoch 1331/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 1112227.0837 - g_loss: -226675128.6667\n",
      "Epoch 1332/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 548577.2306 - g_loss: -214052114.0000\n",
      "Epoch 1333/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 260263.9199 - g_loss: -191891211.0000\n",
      "Epoch 1334/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -1183914.9629 - g_loss: -183736932.0000\n",
      "Epoch 1335/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 173668.3374 - g_loss: -223604779.3333\n",
      "Epoch 1336/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -814464.5433 - g_loss: -285858003.3333\n",
      "Epoch 1337/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 2368186.1178 - g_loss: -316413806.0000\n",
      "Epoch 1338/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -243696.3561 - g_loss: -271927453.3333\n",
      "Epoch 1339/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -180444.1916 - g_loss: -251098526.3333\n",
      "Epoch 1340/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 2785459.0120 - g_loss: -266339646.3333\n",
      "Epoch 1341/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -37790.0345 - g_loss: -240184339.3333\n",
      "Epoch 1342/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -499286.5036 - g_loss: -256933858.3333\n",
      "Epoch 1343/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1226192.8171 - g_loss: -274653249.3333\n",
      "Epoch 1344/2000\n",
      "47/47 [==============================] - 23s 493ms/step - d_loss: 366980.5698 - g_loss: -328993714.0000\n",
      "Epoch 1345/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -331405.9337 - g_loss: -264242382.3333\n",
      "Epoch 1346/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 208934.8665 - g_loss: -245234217.0000\n",
      "Epoch 1347/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 5953.8112 - g_loss: -241938471.6667\n",
      "Epoch 1348/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -389527.1719 - g_loss: -267150143.3333\n",
      "Epoch 1349/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 90603.0094 - g_loss: -295720367.0000\n",
      "Epoch 1350/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -418536.8617 - g_loss: -280810444.0000\n",
      "Epoch 1351/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 442165.0661 - g_loss: -282511589.3333\n",
      "Epoch 1352/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -370285.0697 - g_loss: -265365441.6667\n",
      "Epoch 1353/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1845210.4967 - g_loss: -248090085.0000\n",
      "Epoch 1354/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 1635789.7605 - g_loss: -261470528.0000\n",
      "Epoch 1355/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -482180.2370 - g_loss: -236005076.3333\n",
      "Epoch 1356/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 223293.3232 - g_loss: -220015192.0000\n",
      "Epoch 1357/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -700609.3376 - g_loss: -213859406.0000\n",
      "Epoch 1358/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 735358.2663 - g_loss: -207613194.6667\n",
      "Epoch 1359/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: 1671827.8617 - g_loss: -197281809.3333\n",
      "Epoch 1360/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -178656.2175 - g_loss: -155417208.3333\n",
      "Epoch 1361/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -39816.0761 - g_loss: -124655953.5000\n",
      "Epoch 1362/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1021984.8710 - g_loss: -143490195.6667\n",
      "Epoch 1363/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -48920.8303 - g_loss: -162013885.3333\n",
      "Epoch 1364/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -712473.0614 - g_loss: -220545485.3333\n",
      "Epoch 1365/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 100124.9453 - g_loss: -275018878.3333\n",
      "Epoch 1366/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -244531.3521 - g_loss: -199568522.6667\n",
      "Epoch 1367/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -112510.9232 - g_loss: -177272521.3333\n",
      "Epoch 1368/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -261088.2259 - g_loss: -189978714.0000\n",
      "Epoch 1369/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 920617.3581 - g_loss: -186914213.0000\n",
      "Epoch 1370/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 250180.7861 - g_loss: -133788600.5000\n",
      "Epoch 1371/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 647711.7042 - g_loss: -142689097.6667\n",
      "Epoch 1372/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -219524.2225 - g_loss: -162462921.0000\n",
      "Epoch 1373/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -79569.2632 - g_loss: -147344286.5000\n",
      "Epoch 1374/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 433747.1514 - g_loss: -152736145.6667\n",
      "Epoch 1375/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 431324.8494 - g_loss: -142066992.5000\n",
      "Epoch 1376/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1503143.9274 - g_loss: -169373335.0000\n",
      "Epoch 1377/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 1082541.2021 - g_loss: -184534580.6667\n",
      "Epoch 1378/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -347009.8221 - g_loss: -148850757.3333\n",
      "Epoch 1379/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -202363.2910 - g_loss: -154118604.0000\n",
      "Epoch 1380/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 228533.7895 - g_loss: -137618183.5000\n",
      "Epoch 1381/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -628097.4546 - g_loss: -123559692.5000\n",
      "Epoch 1382/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -255967.9950 - g_loss: -105326918.0000\n",
      "Epoch 1383/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -77729.2318 - g_loss: -126476193.0000\n",
      "Epoch 1384/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -53421.5039 - g_loss: -163404677.0000\n",
      "Epoch 1385/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -191941.5221 - g_loss: -157205539.0000\n",
      "Epoch 1386/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 200840.3529 - g_loss: -126643829.6667\n",
      "Epoch 1387/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -25131.0880 - g_loss: -141311350.8333\n",
      "Epoch 1388/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -29021.8525 - g_loss: -128912730.3333\n",
      "Epoch 1389/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -330534.2062 - g_loss: -134886193.1667\n",
      "Epoch 1390/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 182147.3169 - g_loss: -165643295.3333\n",
      "Epoch 1391/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 45440.0446 - g_loss: -165705521.3333\n",
      "Epoch 1392/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -413436.3011 - g_loss: -168117014.6667\n",
      "Epoch 1393/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 828360.9548 - g_loss: -173179562.6667\n",
      "Epoch 1394/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1310641.7640 - g_loss: -179019855.0000\n",
      "Epoch 1395/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -40005.1667 - g_loss: -204408289.6667\n",
      "Epoch 1396/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -794368.6733 - g_loss: -171056646.6667\n",
      "Epoch 1397/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -408983.7194 - g_loss: -159663603.6667\n",
      "Epoch 1398/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1108494.3675 - g_loss: -165784408.0000\n",
      "Epoch 1399/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 448213.0088 - g_loss: -145802787.0000\n",
      "Epoch 1400/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -89027.0537 - g_loss: -155803856.6667\n",
      "Epoch 1401/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -553749.3376 - g_loss: -151463939.5000\n",
      "Epoch 1402/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -535697.1910 - g_loss: -173198362.0000\n",
      "Epoch 1403/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 879583.1559 - g_loss: -172899112.3333\n",
      "Epoch 1404/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1018622.9320 - g_loss: -159754448.0000\n",
      "Epoch 1405/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 637177.5977 - g_loss: -145983334.6667\n",
      "Epoch 1406/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -723098.5348 - g_loss: -130731417.6667\n",
      "Epoch 1407/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 166789.6589 - g_loss: -129389947.6667\n",
      "Epoch 1408/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 160537.8024 - g_loss: -132955836.5000\n",
      "Epoch 1409/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -574788.3542 - g_loss: -150440026.3333\n",
      "Epoch 1410/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 2429822.1120 - g_loss: -172719904.6667\n",
      "Epoch 1411/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -477625.2487 - g_loss: -178845701.0000\n",
      "Epoch 1412/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -391620.8045 - g_loss: -217215907.6667\n",
      "Epoch 1413/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 254175.8307 - g_loss: -238950339.0000\n",
      "Epoch 1414/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 493292.6736 - g_loss: -271231878.0000\n",
      "Epoch 1415/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 3632.2461 - g_loss: -313079439.6667\n",
      "Epoch 1416/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1821837.0299 - g_loss: -358272750.0000\n",
      "Epoch 1417/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2934059.7148 - g_loss: -344099526.0000\n",
      "Epoch 1418/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1731792.7699 - g_loss: -308568410.0000\n",
      "Epoch 1419/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1472103.3988 - g_loss: -347056949.3333\n",
      "Epoch 1420/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 290879.1961 - g_loss: -414340076.6667\n",
      "Epoch 1421/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -2227881.1660 - g_loss: -455074056.6667\n",
      "Epoch 1422/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -2121293.6100 - g_loss: -491737020.0000\n",
      "Epoch 1423/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 1992311.7351 - g_loss: -550500617.3333\n",
      "Epoch 1424/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 2575767.9922 - g_loss: -537905294.0000\n",
      "Epoch 1425/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 1223345.9292 - g_loss: -495717858.6667\n",
      "Epoch 1426/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1838256.7936 - g_loss: -564989164.6667\n",
      "Epoch 1427/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 1520853.9642 - g_loss: -541638890.6667\n",
      "Epoch 1428/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1228646.1576 - g_loss: -532316751.3333\n",
      "Epoch 1429/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 2556100.1240 - g_loss: -498790164.0000\n",
      "Epoch 1430/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -2891012.9984 - g_loss: -493165796.6667\n",
      "Epoch 1431/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -217448.0426 - g_loss: -468455493.3333\n",
      "Epoch 1432/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -865328.4538 - g_loss: -502240446.6667\n",
      "Epoch 1433/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 818030.2217 - g_loss: -524049938.0000\n",
      "Epoch 1434/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -3351609.1406 - g_loss: -550305012.6667\n",
      "Epoch 1435/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1321943.9010 - g_loss: -558487732.6667\n",
      "Epoch 1436/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -2193658.0190 - g_loss: -632183262.6667\n",
      "Epoch 1437/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1305182.1226 - g_loss: -674257332.0000\n",
      "Epoch 1438/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -3319317.2699 - g_loss: -681922029.3333\n",
      "Epoch 1439/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 289872.2578 - g_loss: -756975478.6667\n",
      "Epoch 1440/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -2556955.9517 - g_loss: -699100864.0000\n",
      "Epoch 1441/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -1881719.5244 - g_loss: -736764028.0000\n",
      "Epoch 1442/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 2297435.0326 - g_loss: -741346642.6667\n",
      "Epoch 1443/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -567047.0321 - g_loss: -750919292.0000\n",
      "Epoch 1444/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1326576.7552 - g_loss: -690066681.3333\n",
      "Epoch 1445/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1852410.9460 - g_loss: -667100518.6667\n",
      "Epoch 1446/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1296600.6647 - g_loss: -609410108.0000\n",
      "Epoch 1447/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -812452.0775 - g_loss: -569089544.0000\n",
      "Epoch 1448/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -530276.9740 - g_loss: -538035178.6667\n",
      "Epoch 1449/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 2639607.8438 - g_loss: -520861268.6667\n",
      "Epoch 1450/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 1510675.1758 - g_loss: -496889176.0000\n",
      "Epoch 1451/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 657927.7279 - g_loss: -467077954.6667\n",
      "Epoch 1452/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 583634.9648 - g_loss: -532876914.0000\n",
      "Epoch 1453/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1633912.5863 - g_loss: -597709608.0000\n",
      "Epoch 1454/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -4832678.4631 - g_loss: -561916335.3333\n",
      "Epoch 1455/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -983249.9544 - g_loss: -582019407.3333\n",
      "Epoch 1456/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 516859.6367 - g_loss: -636720094.6667\n",
      "Epoch 1457/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 296208.0078 - g_loss: -695000700.0000\n",
      "Epoch 1458/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 460833.7585 - g_loss: -832337444.0000\n",
      "Epoch 1459/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 928685.4870 - g_loss: -909144045.3333\n",
      "Epoch 1460/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -3341005.7448 - g_loss: -892248145.3333\n",
      "Epoch 1461/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 4825460.9767 - g_loss: -915094530.6667\n",
      "Epoch 1462/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 1599407.0695 - g_loss: -865044434.6667\n",
      "Epoch 1463/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1775743.3569 - g_loss: -760231020.0000\n",
      "Epoch 1464/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 4487507.1233 - g_loss: -727195484.0000\n",
      "Epoch 1465/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -2260690.6251 - g_loss: -692055929.3333\n",
      "Epoch 1466/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 464132.5098 - g_loss: -709470990.6667\n",
      "Epoch 1467/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: 3828349.6185 - g_loss: -638793182.6667\n",
      "Epoch 1468/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -759907.4323 - g_loss: -619790921.3333\n",
      "Epoch 1469/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -2603371.7708 - g_loss: -551069980.6667\n",
      "Epoch 1470/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 2036844.8102 - g_loss: -519464222.6667\n",
      "Epoch 1471/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -747455.8138 - g_loss: -458385766.6667\n",
      "Epoch 1472/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -1806241.0059 - g_loss: -477761097.3333\n",
      "Epoch 1473/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1898414.1927 - g_loss: -426758728.0000\n",
      "Epoch 1474/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -976367.7715 - g_loss: -413030059.3333\n",
      "Epoch 1475/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -362780.2214 - g_loss: -427625272.0000\n",
      "Epoch 1476/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 442605.3770 - g_loss: -406142580.6667\n",
      "Epoch 1477/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 37401.1657 - g_loss: -396279198.0000\n",
      "Epoch 1478/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1471607.2077 - g_loss: -416691400.0000\n",
      "Epoch 1479/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -6144422.8750 - g_loss: -510609692.0000\n",
      "Epoch 1480/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 257133.3444 - g_loss: -534148440.0000\n",
      "Epoch 1481/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -3861590.6491 - g_loss: -433034060.6667\n",
      "Epoch 1482/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -285739.6536 - g_loss: -409691611.3333\n",
      "Epoch 1483/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -2035379.7500 - g_loss: -423834360.6667\n",
      "Epoch 1484/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 2643051.2965 - g_loss: -430058702.0000\n",
      "Epoch 1485/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 727302.1797 - g_loss: -431180510.0000\n",
      "Epoch 1486/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -1451183.9609 - g_loss: -438986384.6667\n",
      "Epoch 1487/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 941706.9930 - g_loss: -515427751.3333\n",
      "Epoch 1488/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 3784767.5651 - g_loss: -487183302.0000\n",
      "Epoch 1489/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -1059260.5433 - g_loss: -475522397.3333\n",
      "Epoch 1490/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1647563.2012 - g_loss: -474137433.3333\n",
      "Epoch 1491/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -4496578.2891 - g_loss: -448520468.6667\n",
      "Epoch 1492/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1015447.4388 - g_loss: -423541272.6667\n",
      "Epoch 1493/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -268118.7471 - g_loss: -434275456.6667\n",
      "Epoch 1494/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -2724674.2552 - g_loss: -482106270.0000\n",
      "Epoch 1495/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1379372.5749 - g_loss: -440705427.3333\n",
      "Epoch 1496/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -683230.6495 - g_loss: -471978118.0000\n",
      "Epoch 1497/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -2264883.1839 - g_loss: -488451279.3333\n",
      "Epoch 1498/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1425389.6445 - g_loss: -488196592.6667\n",
      "Epoch 1499/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 2497591.2087 - g_loss: -467810825.3333\n",
      "Epoch 1500/2000\n",
      "47/47 [==============================] - 15s 330ms/step - d_loss: 317951.0269 - g_loss: -432852310.0000\n",
      "Epoch 1501/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 1439404.5189 - g_loss: -401013872.6667\n",
      "Epoch 1502/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 1495023.8451 - g_loss: -362125533.3333\n",
      "Epoch 1503/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 1761091.4531 - g_loss: -355083811.3333\n",
      "Epoch 1504/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2154107.9479 - g_loss: -350163066.6667\n",
      "Epoch 1505/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -2700330.0964 - g_loss: -351981358.0000\n",
      "Epoch 1506/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -110996.5104 - g_loss: -332047962.0000\n",
      "Epoch 1507/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 6584.6120 - g_loss: -349040956.6667\n",
      "Epoch 1508/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1022940.0964 - g_loss: -336527948.0000\n",
      "Epoch 1509/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 358287.3066 - g_loss: -389708170.6667\n",
      "Epoch 1510/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 533301.6064 - g_loss: -377666743.3333\n",
      "Epoch 1511/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -2237497.2148 - g_loss: -420297710.0000\n",
      "Epoch 1512/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 345058.4827 - g_loss: -393657498.0000\n",
      "Epoch 1513/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1654857.0192 - g_loss: -417220814.6667\n",
      "Epoch 1514/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -807372.3359 - g_loss: -414540227.3333\n",
      "Epoch 1515/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1895615.3451 - g_loss: -438657484.0000\n",
      "Epoch 1516/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 75880.9826 - g_loss: -445681959.3333\n",
      "Epoch 1517/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1322920.2227 - g_loss: -441560596.6667\n",
      "Epoch 1518/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 1939070.7087 - g_loss: -416023794.6667\n",
      "Epoch 1519/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 915429.0798 - g_loss: -443119589.3333\n",
      "Epoch 1520/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 3942958.8678 - g_loss: -500569985.3333\n",
      "Epoch 1521/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 2569392.6699 - g_loss: -529373474.6667\n",
      "Epoch 1522/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1938117.1458 - g_loss: -527070497.3333\n",
      "Epoch 1523/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 458819.4706 - g_loss: -497809010.0000\n",
      "Epoch 1524/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 2104681.4857 - g_loss: -462984251.3333\n",
      "Epoch 1525/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 428467.6751 - g_loss: -475000560.0000\n",
      "Epoch 1526/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -686015.9974 - g_loss: -487748056.0000\n",
      "Epoch 1527/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 814922.9935 - g_loss: -546763108.0000\n",
      "Epoch 1528/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -2145964.6875 - g_loss: -582595972.6667\n",
      "Epoch 1529/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 2124024.8305 - g_loss: -633543034.6667\n",
      "Epoch 1530/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -1719322.0784 - g_loss: -650276968.0000\n",
      "Epoch 1531/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1222146.1309 - g_loss: -657708984.0000\n",
      "Epoch 1532/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 3405062.2345 - g_loss: -686043356.0000\n",
      "Epoch 1533/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 3294944.4962 - g_loss: -622429933.3333\n",
      "Epoch 1534/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1029537.5016 - g_loss: -702233924.0000\n",
      "Epoch 1535/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -1431189.1667 - g_loss: -782069916.0000\n",
      "Epoch 1536/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 2060241.4837 - g_loss: -855464709.3333\n",
      "Epoch 1537/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 3618656.8379 - g_loss: -908608378.6667\n",
      "Epoch 1538/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 5454476.9688 - g_loss: -969125726.6667\n",
      "Epoch 1539/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1153152.1051 - g_loss: -1055649162.6667\n",
      "Epoch 1540/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 6558007.8053 - g_loss: -1044364826.6667\n",
      "Epoch 1541/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 508623.9076 - g_loss: -1108106261.3333\n",
      "Epoch 1542/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 4774511.6211 - g_loss: -1215034018.6667\n",
      "Epoch 1543/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 8396445.9511 - g_loss: -1314369896.0000\n",
      "Epoch 1544/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -2838544.5130 - g_loss: -1247880066.6667\n",
      "Epoch 1545/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 2577094.4688 - g_loss: -1250070634.6667\n",
      "Epoch 1546/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -4131431.9219 - g_loss: -1174185980.0000\n",
      "Epoch 1547/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -10109688.7734 - g_loss: -1171915610.6667\n",
      "Epoch 1548/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 2657405.3776 - g_loss: -1196797268.0000\n",
      "Epoch 1549/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -10759837.9629 - g_loss: -1133749778.6667\n",
      "Epoch 1550/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 3336377.5169 - g_loss: -1166826664.0000\n",
      "Epoch 1551/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -12193881.6016 - g_loss: -1271512493.3333\n",
      "Epoch 1552/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -151039.3854 - g_loss: -1382820573.3333\n",
      "Epoch 1553/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1934438.5986 - g_loss: -1460883189.3333\n",
      "Epoch 1554/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 5783680.2917 - g_loss: -1556460946.6667\n",
      "Epoch 1555/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 13996615.4740 - g_loss: -1524062336.0000\n",
      "Epoch 1556/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -2917098.3665 - g_loss: -1539674794.6667\n",
      "Epoch 1557/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -10654606.2552 - g_loss: -1752778989.3333\n",
      "Epoch 1558/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1764203.0990 - g_loss: -1856557800.0000\n",
      "Epoch 1559/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 6704094.8464 - g_loss: -1803262920.0000\n",
      "Epoch 1560/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -4062267.5004 - g_loss: -1753340810.6667\n",
      "Epoch 1561/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 4628340.7812 - g_loss: -1868359834.6667\n",
      "Epoch 1562/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -13484936.7240 - g_loss: -1870639125.3333\n",
      "Epoch 1563/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 730961.4792 - g_loss: -1814125541.3333\n",
      "Epoch 1564/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 8601799.0052 - g_loss: -1815249109.3333\n",
      "Epoch 1565/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -5522854.7422 - g_loss: -1727480680.0000\n",
      "Epoch 1566/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -5051799.0625 - g_loss: -1790500858.6667\n",
      "Epoch 1567/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -2287066.4062 - g_loss: -1777731181.3333\n",
      "Epoch 1568/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -151778.6589 - g_loss: -1771824104.0000\n",
      "Epoch 1569/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -1729214.4844 - g_loss: -1694503661.3333\n",
      "Epoch 1570/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -7385394.5938 - g_loss: -1704726901.3333\n",
      "Epoch 1571/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -517753.2448 - g_loss: -1787913304.0000\n",
      "Epoch 1572/2000\n",
      "47/47 [==============================] - 16s 332ms/step - d_loss: 5458976.1250 - g_loss: -1823578288.0000\n",
      "Epoch 1573/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -6159226.5365 - g_loss: -1846811816.0000\n",
      "Epoch 1574/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 4188113.9583 - g_loss: -1716628586.6667\n",
      "Epoch 1575/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 14363256.0638 - g_loss: -1596142032.0000\n",
      "Epoch 1576/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -244064.5391 - g_loss: -1528260346.6667\n",
      "Epoch 1577/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -5286328.4076 - g_loss: -1513433061.3333\n",
      "Epoch 1578/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -6909410.3779 - g_loss: -1445150701.3333\n",
      "Epoch 1579/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 31812.5807 - g_loss: -1321761602.6667\n",
      "Epoch 1580/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -2523088.2292 - g_loss: -1318482064.0000\n",
      "Epoch 1581/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -6540534.4271 - g_loss: -1338705760.0000\n",
      "Epoch 1582/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 3779690.5208 - g_loss: -1379087858.6667\n",
      "Epoch 1583/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -10974419.8991 - g_loss: -1497265165.3333\n",
      "Epoch 1584/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 835101.3545 - g_loss: -1472499442.6667\n",
      "Epoch 1585/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 3083715.0052 - g_loss: -1401489680.0000\n",
      "Epoch 1586/2000\n",
      "47/47 [==============================] - 15s 315ms/step - d_loss: -5665648.3750 - g_loss: -1456332888.0000\n",
      "Epoch 1587/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 6633555.2168 - g_loss: -1577345938.6667\n",
      "Epoch 1588/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 11049640.8242 - g_loss: -1591425328.0000\n",
      "Epoch 1589/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1384549.9427 - g_loss: -1549487645.3333\n",
      "Epoch 1590/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2313750.4639 - g_loss: -1543665472.0000\n",
      "Epoch 1591/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 13317804.0156 - g_loss: -1588606520.0000\n",
      "Epoch 1592/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 12121816.3333 - g_loss: -1553004962.6667\n",
      "Epoch 1593/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 5093972.8568 - g_loss: -1564516917.3333\n",
      "Epoch 1594/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1152789.9622 - g_loss: -1508836584.0000\n",
      "Epoch 1595/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 804168.2233 - g_loss: -1505180533.3333\n",
      "Epoch 1596/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 283834.5885 - g_loss: -1605915096.0000\n",
      "Epoch 1597/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2906953.7201 - g_loss: -1511369725.3333\n",
      "Epoch 1598/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 2028865.0078 - g_loss: -1354814893.3333\n",
      "Epoch 1599/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 5716952.2500 - g_loss: -1485866512.0000\n",
      "Epoch 1600/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 4071160.5026 - g_loss: -1595578906.6667\n",
      "Epoch 1601/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -744108.9167 - g_loss: -1477296112.0000\n",
      "Epoch 1602/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 9335102.9792 - g_loss: -1434239933.3333\n",
      "Epoch 1603/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 7158076.8620 - g_loss: -1483579488.0000\n",
      "Epoch 1604/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -1347422.6912 - g_loss: -1423034032.0000\n",
      "Epoch 1605/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 4711737.4010 - g_loss: -1454160850.6667\n",
      "Epoch 1606/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -2146066.4167 - g_loss: -1516888976.0000\n",
      "Epoch 1607/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -11140699.2526 - g_loss: -1587594994.6667\n",
      "Epoch 1608/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 2249938.5625 - g_loss: -1515169968.0000\n",
      "Epoch 1609/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 12563673.1709 - g_loss: -1445010328.0000\n",
      "Epoch 1610/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -2812213.4557 - g_loss: -1479451096.0000\n",
      "Epoch 1611/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 12094396.5638 - g_loss: -1480589906.6667\n",
      "Epoch 1612/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 9908637.0391 - g_loss: -1403346365.3333\n",
      "Epoch 1613/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -11922216.7292 - g_loss: -1658696778.6667\n",
      "Epoch 1614/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -8025969.2344 - g_loss: -1752352552.0000\n",
      "Epoch 1615/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 2609085.3411 - g_loss: -1926366277.3333\n",
      "Epoch 1616/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 3252527.0833 - g_loss: -1781950282.6667\n",
      "Epoch 1617/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -12080597.7500 - g_loss: -1918693040.0000\n",
      "Epoch 1618/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 1527663.6094 - g_loss: -1803637346.6667\n",
      "Epoch 1619/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 7231860.2212 - g_loss: -1942619768.0000\n",
      "Epoch 1620/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 6509718.1432 - g_loss: -1965465989.3333\n",
      "Epoch 1621/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: 4142907.8854 - g_loss: -1928281882.6667\n",
      "Epoch 1622/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 4309832.9441 - g_loss: -1975504722.6667\n",
      "Epoch 1623/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -6908765.8490 - g_loss: -2180125685.3333\n",
      "Epoch 1624/2000\n",
      "47/47 [==============================] - 16s 333ms/step - d_loss: -3730509.4840 - g_loss: -2285477658.6667\n",
      "Epoch 1625/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 8662055.7708 - g_loss: -2392612458.6667\n",
      "Epoch 1626/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 16491418.4062 - g_loss: -2330438917.3333\n",
      "Epoch 1627/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -7051000.8464 - g_loss: -2366609978.6667\n",
      "Epoch 1628/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 5543391.0326 - g_loss: -2416842314.6667\n",
      "Epoch 1629/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -3970378.6146 - g_loss: -2488894416.0000\n",
      "Epoch 1630/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 3324849.8177 - g_loss: -2663521610.6667\n",
      "Epoch 1631/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 4087481.5107 - g_loss: -2672975056.0000\n",
      "Epoch 1632/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -1615953.6136 - g_loss: -2771632698.6667\n",
      "Epoch 1633/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 1545450.9805 - g_loss: -2803643274.6667\n",
      "Epoch 1634/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 14582780.8802 - g_loss: -2736065066.6667\n",
      "Epoch 1635/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 11651488.0039 - g_loss: -2796655866.6667\n",
      "Epoch 1636/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -12765224.7829 - g_loss: -2709908373.3333\n",
      "Epoch 1637/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 15165714.3672 - g_loss: -2747451034.6667\n",
      "Epoch 1638/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 2384117.9167 - g_loss: -2939042784.0000\n",
      "Epoch 1639/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -5964319.1042 - g_loss: -3029546144.0000\n",
      "Epoch 1640/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 12992158.7292 - g_loss: -3070128784.0000\n",
      "Epoch 1641/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 19115921.8880 - g_loss: -3011542186.6667\n",
      "Epoch 1642/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 19447920.8385 - g_loss: -2995997493.3333\n",
      "Epoch 1643/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 25890471.2812 - g_loss: -3163525845.3333\n",
      "Epoch 1644/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -22159192.1693 - g_loss: -3403328714.6667\n",
      "Epoch 1645/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 798963.4115 - g_loss: -3303434645.3333\n",
      "Epoch 1646/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 21935711.7500 - g_loss: -3228968448.0000\n",
      "Epoch 1647/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1952484.0599 - g_loss: -3146835253.3333\n",
      "Epoch 1648/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -11169884.8542 - g_loss: -3101922805.3333\n",
      "Epoch 1649/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -2451769.5417 - g_loss: -3057785141.3333\n",
      "Epoch 1650/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -14448275.8294 - g_loss: -2910853653.3333\n",
      "Epoch 1651/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 4836338.2865 - g_loss: -2771570112.0000\n",
      "Epoch 1652/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -4075749.1719 - g_loss: -2737875050.6667\n",
      "Epoch 1653/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 661367.7917 - g_loss: -2852429290.6667\n",
      "Epoch 1654/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -9221295.1536 - g_loss: -2742366389.3333\n",
      "Epoch 1655/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: 2204508.5938 - g_loss: -2690535920.0000\n",
      "Epoch 1656/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -9716847.1771 - g_loss: -2665415040.0000\n",
      "Epoch 1657/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 9042705.7031 - g_loss: -2848473493.3333\n",
      "Epoch 1658/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -4738175.2447 - g_loss: -3095842389.3333\n",
      "Epoch 1659/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -2790057.2012 - g_loss: -3474774160.0000\n",
      "Epoch 1660/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -8086526.4896 - g_loss: -3466083509.3333\n",
      "Epoch 1661/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 3328731.9323 - g_loss: -3417968592.0000\n",
      "Epoch 1662/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 11271087.6458 - g_loss: -3263671280.0000\n",
      "Epoch 1663/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -30046838.2708 - g_loss: -3173669322.6667\n",
      "Epoch 1664/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 772146.0729 - g_loss: -3123053130.6667\n",
      "Epoch 1665/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 1938079.0547 - g_loss: -3165287082.6667\n",
      "Epoch 1666/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 14183242.4089 - g_loss: -2909974053.3333\n",
      "Epoch 1667/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 2024634.0000 - g_loss: -3171909712.0000\n",
      "Epoch 1668/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -13503425.8802 - g_loss: -3322022234.6667\n",
      "Epoch 1669/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 16249302.6354 - g_loss: -3079166960.0000\n",
      "Epoch 1670/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 2238197.6068 - g_loss: -2878477450.6667\n",
      "Epoch 1671/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -6020936.5417 - g_loss: -2896052160.0000\n",
      "Epoch 1672/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -26668910.2695 - g_loss: -3080054960.0000\n",
      "Epoch 1673/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -13935685.5078 - g_loss: -3308542085.3333\n",
      "Epoch 1674/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -16763730.8750 - g_loss: -3458915802.6667\n",
      "Epoch 1675/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 20689788.5885 - g_loss: -3463113952.0000\n",
      "Epoch 1676/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 6900756.5312 - g_loss: -3285988186.6667\n",
      "Epoch 1677/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 29523472.5625 - g_loss: -3294049098.6667\n",
      "Epoch 1678/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -11723347.6927 - g_loss: -3141700085.3333\n",
      "Epoch 1679/2000\n",
      "47/47 [==============================] - 15s 329ms/step - d_loss: 12438399.9422 - g_loss: -3215315658.6667\n",
      "Epoch 1680/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -24424238.7031 - g_loss: -3544703290.6667\n",
      "Epoch 1681/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -6922584.3880 - g_loss: -3958745994.6667\n",
      "Epoch 1682/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -2407895.6146 - g_loss: -3950005557.3333\n",
      "Epoch 1683/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -41201067.3125 - g_loss: -3908752154.6667\n",
      "Epoch 1684/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -21515190.9362 - g_loss: -3977609344.0000\n",
      "Epoch 1685/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 11463522.6979 - g_loss: -3879706026.6667\n",
      "Epoch 1686/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -23628724.8789 - g_loss: -3801609413.3333\n",
      "Epoch 1687/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 9299402.5781 - g_loss: -3702868912.0000\n",
      "Epoch 1688/2000\n",
      "47/47 [==============================] - 16s 332ms/step - d_loss: 10257043.8542 - g_loss: -3608252298.6667\n",
      "Epoch 1689/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 2577713.4141 - g_loss: -3506570186.6667\n",
      "Epoch 1690/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 11801447.5573 - g_loss: -3398814352.0000\n",
      "Epoch 1691/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -26006181.1875 - g_loss: -3509781216.0000\n",
      "Epoch 1692/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -9257687.7734 - g_loss: -3204719850.6667\n",
      "Epoch 1693/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -12590700.9036 - g_loss: -3240498309.3333\n",
      "Epoch 1694/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -2712600.2969 - g_loss: -3331114064.0000\n",
      "Epoch 1695/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -4517826.8125 - g_loss: -3492148154.6667\n",
      "Epoch 1696/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -4483808.6250 - g_loss: -3850261808.0000\n",
      "Epoch 1697/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -8951443.8333 - g_loss: -3922594906.6667\n",
      "Epoch 1698/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 2473325.2643 - g_loss: -3891471354.6667\n",
      "Epoch 1699/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -7144160.1562 - g_loss: -3835122256.0000\n",
      "Epoch 1700/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 17598670.8333 - g_loss: -3734635514.6667\n",
      "Epoch 1701/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -4121050.1146 - g_loss: -3861615738.6667\n",
      "Epoch 1702/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 15923921.8542 - g_loss: -3824518512.0000\n",
      "Epoch 1703/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -3527880.4089 - g_loss: -3796258837.3333\n",
      "Epoch 1704/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 24328963.3691 - g_loss: -3628374117.3333\n",
      "Epoch 1705/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 12605528.3177 - g_loss: -3779825861.3333\n",
      "Epoch 1706/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 21643505.0990 - g_loss: -3430917877.3333\n",
      "Epoch 1707/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 332662.7839 - g_loss: -3211223418.6667\n",
      "Epoch 1708/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -180500.5000 - g_loss: -3170212405.3333\n",
      "Epoch 1709/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 10397105.2188 - g_loss: -3200623813.3333\n",
      "Epoch 1710/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -4292623.7474 - g_loss: -2891253381.3333\n",
      "Epoch 1711/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -4914145.7292 - g_loss: -2885767008.0000\n",
      "Epoch 1712/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -1946106.8021 - g_loss: -2889743509.3333\n",
      "Epoch 1713/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 6383995.0208 - g_loss: -2946621904.0000\n",
      "Epoch 1714/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -27405741.3854 - g_loss: -3107590298.6667\n",
      "Epoch 1715/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 11062908.2083 - g_loss: -3137324304.0000\n",
      "Epoch 1716/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 5549878.6667 - g_loss: -3098854586.6667\n",
      "Epoch 1717/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 14397971.7292 - g_loss: -3281201632.0000\n",
      "Epoch 1718/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 19138912.0182 - g_loss: -3191034229.3333\n",
      "Epoch 1719/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 10432882.2318 - g_loss: -3293338624.0000\n",
      "Epoch 1720/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -26211636.9375 - g_loss: -3558768832.0000\n",
      "Epoch 1721/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 9126965.4531 - g_loss: -3692637541.3333\n",
      "Epoch 1722/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 21446385.6458 - g_loss: -3421004533.3333\n",
      "Epoch 1723/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 16684105.0970 - g_loss: -3208242773.3333\n",
      "Epoch 1724/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1382680.7005 - g_loss: -2953952634.6667\n",
      "Epoch 1725/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 14921416.1621 - g_loss: -2955708325.3333\n",
      "Epoch 1726/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -14391487.4427 - g_loss: -2898362133.3333\n",
      "Epoch 1727/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 14398841.6667 - g_loss: -2901252458.6667\n",
      "Epoch 1728/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -331334.0182 - g_loss: -2819493610.6667\n",
      "Epoch 1729/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 8208723.5680 - g_loss: -2664497909.3333\n",
      "Epoch 1730/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 248339.4896 - g_loss: -2263803346.6667\n",
      "Epoch 1731/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2710561.3776 - g_loss: -2214656874.6667\n",
      "Epoch 1732/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -11410594.7708 - g_loss: -2123205602.6667\n",
      "Epoch 1733/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -2022365.3691 - g_loss: -2167161885.3333\n",
      "Epoch 1734/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -10170156.0182 - g_loss: -2314037098.6667\n",
      "Epoch 1735/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -17306853.0977 - g_loss: -2557520250.6667\n",
      "Epoch 1736/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -17862895.8594 - g_loss: -2670903877.3333\n",
      "Epoch 1737/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 8529162.1663 - g_loss: -2746492597.3333\n",
      "Epoch 1738/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 282941.4349 - g_loss: -2648898224.0000\n",
      "Epoch 1739/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -6023658.5365 - g_loss: -2567801189.3333\n",
      "Epoch 1740/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -11550002.1615 - g_loss: -2601516576.0000\n",
      "Epoch 1741/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 3235516.7865 - g_loss: -2473297376.0000\n",
      "Epoch 1742/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 2807749.3698 - g_loss: -2288353530.6667\n",
      "Epoch 1743/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -1462188.1771 - g_loss: -2268046032.0000\n",
      "Epoch 1744/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 306227.2523 - g_loss: -2412545450.6667\n",
      "Epoch 1745/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 23647744.9062 - g_loss: -2410572256.0000\n",
      "Epoch 1746/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 9725699.4401 - g_loss: -2362647104.0000\n",
      "Epoch 1747/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -17750026.6680 - g_loss: -2231144869.3333\n",
      "Epoch 1748/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 19889928.6859 - g_loss: -2143040298.6667\n",
      "Epoch 1749/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -14176291.9329 - g_loss: -2163212973.3333\n",
      "Epoch 1750/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 9234098.8854 - g_loss: -2211233232.0000\n",
      "Epoch 1751/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 13815297.6120 - g_loss: -2132455834.6667\n",
      "Epoch 1752/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 13047645.1400 - g_loss: -2105534184.0000\n",
      "Epoch 1753/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -9972320.2591 - g_loss: -2137086141.3333\n",
      "Epoch 1754/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -15635893.8060 - g_loss: -2293333597.3333\n",
      "Epoch 1755/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 12629915.6299 - g_loss: -2503198586.6667\n",
      "Epoch 1756/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -25008860.5052 - g_loss: -2582309120.0000\n",
      "Epoch 1757/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -400842.5885 - g_loss: -2608573450.6667\n",
      "Epoch 1758/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -13925548.7422 - g_loss: -2398382106.6667\n",
      "Epoch 1759/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -17598864.0104 - g_loss: -2460240394.6667\n",
      "Epoch 1760/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -9382416.5156 - g_loss: -2762356112.0000\n",
      "Epoch 1761/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: 15266103.5000 - g_loss: -2905019962.6667\n",
      "Epoch 1762/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -4193064.3542 - g_loss: -2880598997.3333\n",
      "Epoch 1763/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -2551961.2370 - g_loss: -2978227904.0000\n",
      "Epoch 1764/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 3631709.6751 - g_loss: -3283401013.3333\n",
      "Epoch 1765/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -5199040.4531 - g_loss: -3255664810.6667\n",
      "Epoch 1766/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 12625780.7630 - g_loss: -3475199450.6667\n",
      "Epoch 1767/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 933392.2344 - g_loss: -3423319872.0000\n",
      "Epoch 1768/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 7201714.7292 - g_loss: -3142090272.0000\n",
      "Epoch 1769/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -62337.6953 - g_loss: -3100089269.3333\n",
      "Epoch 1770/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 16948963.8346 - g_loss: -2869857840.0000\n",
      "Epoch 1771/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 1791653.3438 - g_loss: -2754075664.0000\n",
      "Epoch 1772/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -7821552.8906 - g_loss: -2597038165.3333\n",
      "Epoch 1773/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 21373595.3542 - g_loss: -2576883989.3333\n",
      "Epoch 1774/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -91706.5651 - g_loss: -2645613189.3333\n",
      "Epoch 1775/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -6604586.6146 - g_loss: -2824677872.0000\n",
      "Epoch 1776/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -641906.1510 - g_loss: -2643948618.6667\n",
      "Epoch 1777/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 2314837.5472 - g_loss: -2637275253.3333\n",
      "Epoch 1778/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 2827644.7982 - g_loss: -2490148298.6667\n",
      "Epoch 1779/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -6749695.7604 - g_loss: -2289607568.0000\n",
      "Epoch 1780/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 10602575.0417 - g_loss: -2042044178.6667\n",
      "Epoch 1781/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -3372890.4102 - g_loss: -1903319586.6667\n",
      "Epoch 1782/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -8821992.0052 - g_loss: -1840886885.3333\n",
      "Epoch 1783/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -11282892.8164 - g_loss: -1763821296.0000\n",
      "Epoch 1784/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -4825108.4219 - g_loss: -1628250866.6667\n",
      "Epoch 1785/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 5289140.8203 - g_loss: -1659388957.3333\n",
      "Epoch 1786/2000\n",
      "47/47 [==============================] - 15s 315ms/step - d_loss: -3329082.7760 - g_loss: -1575354818.6667\n",
      "Epoch 1787/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -741897.9323 - g_loss: -1537999698.6667\n",
      "Epoch 1788/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 10432503.2839 - g_loss: -1487203224.0000\n",
      "Epoch 1789/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -9935580.5339 - g_loss: -1497084802.6667\n",
      "Epoch 1790/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -5330860.7253 - g_loss: -1679476554.6667\n",
      "Epoch 1791/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 2382142.8672 - g_loss: -1736710666.6667\n",
      "Epoch 1792/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -16760193.9531 - g_loss: -1760443509.3333\n",
      "Epoch 1793/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 6850346.2995 - g_loss: -1717093061.3333\n",
      "Epoch 1794/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 972339.7604 - g_loss: -1708380949.3333\n",
      "Epoch 1795/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -18011315.8021 - g_loss: -1832349058.6667\n",
      "Epoch 1796/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 6715048.8542 - g_loss: -1847402978.6667\n",
      "Epoch 1797/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 13300084.7474 - g_loss: -1834713296.0000\n",
      "Epoch 1798/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -1320777.5840 - g_loss: -1687635581.3333\n",
      "Epoch 1799/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 16947523.8281 - g_loss: -1522985066.6667\n",
      "Epoch 1800/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 5835145.6849 - g_loss: -1474459034.6667\n",
      "Epoch 1801/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -11167627.5340 - g_loss: -1546773786.6667\n",
      "Epoch 1802/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 14740283.5833 - g_loss: -1552527336.0000\n",
      "Epoch 1803/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 2385606.7669 - g_loss: -1542614973.3333\n",
      "Epoch 1804/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 4820787.0781 - g_loss: -1452360389.3333\n",
      "Epoch 1805/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -8208660.2031 - g_loss: -1610124202.6667\n",
      "Epoch 1806/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -2487771.9857 - g_loss: -1664194840.0000\n",
      "Epoch 1807/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 4698628.5000 - g_loss: -1602823789.3333\n",
      "Epoch 1808/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 11702354.8242 - g_loss: -1545807184.0000\n",
      "Epoch 1809/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -15611908.9974 - g_loss: -1462828581.3333\n",
      "Epoch 1810/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 3705671.8672 - g_loss: -1610414602.6667\n",
      "Epoch 1811/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 10748950.2617 - g_loss: -1625689722.6667\n",
      "Epoch 1812/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -1896204.5938 - g_loss: -1803368592.0000\n",
      "Epoch 1813/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 19782289.2969 - g_loss: -1812301997.3333\n",
      "Epoch 1814/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 13025704.5182 - g_loss: -1787724056.0000\n",
      "Epoch 1815/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 5554859.0260 - g_loss: -1865625986.6667\n",
      "Epoch 1816/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -8386644.2969 - g_loss: -1876773760.0000\n",
      "Epoch 1817/2000\n",
      "47/47 [==============================] - 16s 333ms/step - d_loss: 1119509.1693 - g_loss: -1678928069.3333\n",
      "Epoch 1818/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 12163458.5729 - g_loss: -1545934962.6667\n",
      "Epoch 1819/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 5558739.2590 - g_loss: -1599162568.0000\n",
      "Epoch 1820/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 6636225.7083 - g_loss: -1680530181.3333\n",
      "Epoch 1821/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -10397051.0157 - g_loss: -1708784720.0000\n",
      "Epoch 1822/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -3271217.3958 - g_loss: -1756728048.0000\n",
      "Epoch 1823/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 2174147.0625 - g_loss: -1804743893.3333\n",
      "Epoch 1824/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2250154.9954 - g_loss: -1840846717.3333\n",
      "Epoch 1825/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 12128267.3750 - g_loss: -1717851784.0000\n",
      "Epoch 1826/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -11747857.1758 - g_loss: -1687728762.6667\n",
      "Epoch 1827/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -1634346.9062 - g_loss: -1591262728.0000\n",
      "Epoch 1828/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 9430559.3564 - g_loss: -1599979424.0000\n",
      "Epoch 1829/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 5946126.4128 - g_loss: -1503475253.3333\n",
      "Epoch 1830/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -11133468.8542 - g_loss: -1625166290.6667\n",
      "Epoch 1831/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -5638709.1094 - g_loss: -1710015909.3333\n",
      "Epoch 1832/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -6167630.7708 - g_loss: -1679346650.6667\n",
      "Epoch 1833/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -4457593.0632 - g_loss: -1609136546.6667\n",
      "Epoch 1834/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -5002304.4609 - g_loss: -1574573445.3333\n",
      "Epoch 1835/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 12914375.2917 - g_loss: -1665975778.6667\n",
      "Epoch 1836/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 4162782.4479 - g_loss: -1596644717.3333\n",
      "Epoch 1837/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 7221931.6510 - g_loss: -1684468541.3333\n",
      "Epoch 1838/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -10499316.5495 - g_loss: -1863140410.6667\n",
      "Epoch 1839/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 6509025.6719 - g_loss: -2065204013.3333\n",
      "Epoch 1840/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -2887241.2812 - g_loss: -2095444226.6667\n",
      "Epoch 1841/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 3696021.6120 - g_loss: -2102048952.0000\n",
      "Epoch 1842/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -2246370.9948 - g_loss: -2073606176.0000\n",
      "Epoch 1843/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -2711768.2604 - g_loss: -2071374632.0000\n",
      "Epoch 1844/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 8603512.1745 - g_loss: -1975505573.3333\n",
      "Epoch 1845/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 2099073.5365 - g_loss: -1719652184.0000\n",
      "Epoch 1846/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 6316645.6292 - g_loss: -1628106112.0000\n",
      "Epoch 1847/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -16431374.6589 - g_loss: -1641165432.0000\n",
      "Epoch 1848/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 4949477.8203 - g_loss: -1691748544.0000\n",
      "Epoch 1849/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 4646543.2526 - g_loss: -1566835250.6667\n",
      "Epoch 1850/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 5858660.3359 - g_loss: -1588344605.3333\n",
      "Epoch 1851/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -5318697.3171 - g_loss: -1696973890.6667\n",
      "Epoch 1852/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 7459281.0729 - g_loss: -1683967080.0000\n",
      "Epoch 1853/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 6216757.9701 - g_loss: -1765570800.0000\n",
      "Epoch 1854/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -11289574.3086 - g_loss: -1827593096.0000\n",
      "Epoch 1855/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 8343345.2500 - g_loss: -1819350237.3333\n",
      "Epoch 1856/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 4226056.2083 - g_loss: -1716335093.3333\n",
      "Epoch 1857/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -4281535.4870 - g_loss: -1814781000.0000\n",
      "Epoch 1858/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 4952.9925 - g_loss: -1880339682.6667\n",
      "Epoch 1859/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1959344.0104 - g_loss: -1842399658.6667\n",
      "Epoch 1860/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -6240820.5794 - g_loss: -1842132000.0000\n",
      "Epoch 1861/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 5795465.8099 - g_loss: -1858396568.0000\n",
      "Epoch 1862/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -1317232.5957 - g_loss: -1822629029.3333\n",
      "Epoch 1863/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 2271657.1250 - g_loss: -1756267341.3333\n",
      "Epoch 1864/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 1769546.2041 - g_loss: -1797432157.3333\n",
      "Epoch 1865/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -9898325.8880 - g_loss: -1889547056.0000\n",
      "Epoch 1866/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -271071.0365 - g_loss: -2010621136.0000\n",
      "Epoch 1867/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 4742093.0208 - g_loss: -1916918194.6667\n",
      "Epoch 1868/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 437951.9062 - g_loss: -1810035421.3333\n",
      "Epoch 1869/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 9418198.9010 - g_loss: -1895648704.0000\n",
      "Epoch 1870/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 12563912.6680 - g_loss: -2055991120.0000\n",
      "Epoch 1871/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 4655911.2604 - g_loss: -2056716125.3333\n",
      "Epoch 1872/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 11648875.8281 - g_loss: -2152167141.3333\n",
      "Epoch 1873/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -2676261.2240 - g_loss: -2252316394.6667\n",
      "Epoch 1874/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 2484262.4167 - g_loss: -2379818341.3333\n",
      "Epoch 1875/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1370998.0807 - g_loss: -2145351528.0000\n",
      "Epoch 1876/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -11181958.8854 - g_loss: -2205408072.0000\n",
      "Epoch 1877/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 1917419.1463 - g_loss: -2302428901.3333\n",
      "Epoch 1878/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -2266027.8229 - g_loss: -2314953333.3333\n",
      "Epoch 1879/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -579959.6927 - g_loss: -2463981061.3333\n",
      "Epoch 1880/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -3759578.9583 - g_loss: -2587547648.0000\n",
      "Epoch 1881/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 1738593.2920 - g_loss: -2586478378.6667\n",
      "Epoch 1882/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -5539560.0495 - g_loss: -2497799317.3333\n",
      "Epoch 1883/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -1731653.3880 - g_loss: -2315161925.3333\n",
      "Epoch 1884/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: -6867480.4327 - g_loss: -2393566784.0000\n",
      "Epoch 1885/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 10873840.3854 - g_loss: -2339289440.0000\n",
      "Epoch 1886/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -3141897.8568 - g_loss: -2428588768.0000\n",
      "Epoch 1887/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -6152377.9850 - g_loss: -2363329594.6667\n",
      "Epoch 1888/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 4762931.3802 - g_loss: -2220798938.6667\n",
      "Epoch 1889/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 6013005.7461 - g_loss: -2202716290.6667\n",
      "Epoch 1890/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 8950462.6953 - g_loss: -2036019541.3333\n",
      "Epoch 1891/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 9448032.5736 - g_loss: -1908686376.0000\n",
      "Epoch 1892/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 5757046.2826 - g_loss: -1796339042.6667\n",
      "Epoch 1893/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 8102523.1374 - g_loss: -1873736525.3333\n",
      "Epoch 1894/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -836436.2904 - g_loss: -1940293962.6667\n",
      "Epoch 1895/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -7515278.8738 - g_loss: -1982374930.6667\n",
      "Epoch 1896/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 7564117.7760 - g_loss: -2057963386.6667\n",
      "Epoch 1897/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -6400406.6940 - g_loss: -2273224640.0000\n",
      "Epoch 1898/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: -1918458.1250 - g_loss: -2261532917.3333\n",
      "Epoch 1899/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2395940.5312 - g_loss: -2287167824.0000\n",
      "Epoch 1900/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -5041586.8073 - g_loss: -2547857088.0000\n",
      "Epoch 1901/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -3583860.3958 - g_loss: -2775628906.6667\n",
      "Epoch 1902/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 7192033.2096 - g_loss: -2845435712.0000\n",
      "Epoch 1903/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 5116708.7083 - g_loss: -2958859274.6667\n",
      "Epoch 1904/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 4237417.2318 - g_loss: -2948568458.6667\n",
      "Epoch 1905/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 5373980.1458 - g_loss: -3021044784.0000\n",
      "Epoch 1906/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 15850234.9427 - g_loss: -2994950160.0000\n",
      "Epoch 1907/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 9093059.9154 - g_loss: -2925169370.6667\n",
      "Epoch 1908/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 5575354.9154 - g_loss: -2662662389.3333\n",
      "Epoch 1909/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 6383495.7891 - g_loss: -2637681082.6667\n",
      "Epoch 1910/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -1868613.1875 - g_loss: -2676925866.6667\n",
      "Epoch 1911/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -6489430.2018 - g_loss: -2910877690.6667\n",
      "Epoch 1912/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 14506399.7760 - g_loss: -3215432656.0000\n",
      "Epoch 1913/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: -5726398.4889 - g_loss: -3300372666.6667\n",
      "Epoch 1914/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -14807688.2708 - g_loss: -3281334682.6667\n",
      "Epoch 1915/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -25054867.1979 - g_loss: -3210877856.0000\n",
      "Epoch 1916/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 18388866.7259 - g_loss: -3289042405.3333\n",
      "Epoch 1917/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 18328253.9479 - g_loss: -3279518826.6667\n",
      "Epoch 1918/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 972046.6562 - g_loss: -3098052288.0000\n",
      "Epoch 1919/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 11928970.1667 - g_loss: -3011885840.0000\n",
      "Epoch 1920/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 18501781.0531 - g_loss: -2803775760.0000\n",
      "Epoch 1921/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -1879655.9609 - g_loss: -2830040026.6667\n",
      "Epoch 1922/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 12261022.6762 - g_loss: -3119561125.3333\n",
      "Epoch 1923/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 7886056.4335 - g_loss: -2943963429.3333\n",
      "Epoch 1924/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 1208838.6380 - g_loss: -2968272954.6667\n",
      "Epoch 1925/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 5797842.4818 - g_loss: -2894604901.3333\n",
      "Epoch 1926/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -11285549.8490 - g_loss: -2893218021.3333\n",
      "Epoch 1927/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -4746530.4557 - g_loss: -3015261130.6667\n",
      "Epoch 1928/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 7543270.3151 - g_loss: -2666101882.6667\n",
      "Epoch 1929/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -4647063.6302 - g_loss: -2481913802.6667\n",
      "Epoch 1930/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -473093.1016 - g_loss: -2416558778.6667\n",
      "Epoch 1931/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: 4516886.6533 - g_loss: -2316361888.0000\n",
      "Epoch 1932/2000\n",
      "47/47 [==============================] - 16s 330ms/step - d_loss: -5683877.4766 - g_loss: -2260279498.6667\n",
      "Epoch 1933/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -6024788.0391 - g_loss: -2332607237.3333\n",
      "Epoch 1934/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 5694165.5703 - g_loss: -2103981021.3333\n",
      "Epoch 1935/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -9987711.6512 - g_loss: -2030550176.0000\n",
      "Epoch 1936/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -132261.9661 - g_loss: -2201232578.6667\n",
      "Epoch 1937/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -6294002.5286 - g_loss: -2265424858.6667\n",
      "Epoch 1938/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -33456.8438 - g_loss: -2338602922.6667\n",
      "Epoch 1939/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 2375023.8151 - g_loss: -2250927125.3333\n",
      "Epoch 1940/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 5557440.3125 - g_loss: -2153757509.3333\n",
      "Epoch 1941/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 9769720.1068 - g_loss: -1987551122.6667\n",
      "Epoch 1942/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 3404262.9870 - g_loss: -1955373488.0000\n",
      "Epoch 1943/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: -3803550.5072 - g_loss: -1999497448.0000\n",
      "Epoch 1944/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1889526.4993 - g_loss: -1920042984.0000\n",
      "Epoch 1945/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 776846.2578 - g_loss: -1838422562.6667\n",
      "Epoch 1946/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 12426986.6458 - g_loss: -1718166346.6667\n",
      "Epoch 1947/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 5858252.5241 - g_loss: -1587960608.0000\n",
      "Epoch 1948/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 5736348.7760 - g_loss: -1592698976.0000\n",
      "Epoch 1949/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2747428.1621 - g_loss: -1685851533.3333\n",
      "Epoch 1950/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 5057273.7135 - g_loss: -1673072738.6667\n",
      "Epoch 1951/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 1674656.3646 - g_loss: -1797727149.3333\n",
      "Epoch 1952/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1999016.2526 - g_loss: -2037795656.0000\n",
      "Epoch 1953/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 10733576.3750 - g_loss: -2001296389.3333\n",
      "Epoch 1954/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 6707229.6771 - g_loss: -2064532749.3333\n",
      "Epoch 1955/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -5808385.9531 - g_loss: -2120866898.6667\n",
      "Epoch 1956/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -8357882.2500 - g_loss: -2268747090.6667\n",
      "Epoch 1957/2000\n",
      "47/47 [==============================] - 15s 328ms/step - d_loss: 8888523.1927 - g_loss: -2383892218.6667\n",
      "Epoch 1958/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: 5452346.2985 - g_loss: -2328759722.6667\n",
      "Epoch 1959/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -10108483.5612 - g_loss: -2294027824.0000\n",
      "Epoch 1960/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -2205187.1641 - g_loss: -2663722357.3333\n",
      "Epoch 1961/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -979851.9785 - g_loss: -2780573632.0000\n",
      "Epoch 1962/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: 14202540.4232 - g_loss: -2902294880.0000\n",
      "Epoch 1963/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1247320.1849 - g_loss: -2883987136.0000\n",
      "Epoch 1964/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -18412237.6715 - g_loss: -3025122229.3333\n",
      "Epoch 1965/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -13770837.5365 - g_loss: -3184168192.0000\n",
      "Epoch 1966/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2444234.0521 - g_loss: -3213704949.3333\n",
      "Epoch 1967/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -17511555.8385 - g_loss: -3121695312.0000\n",
      "Epoch 1968/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -17764234.9336 - g_loss: -3440753290.6667\n",
      "Epoch 1969/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: 1090375.7878 - g_loss: -3903906944.0000\n",
      "Epoch 1970/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: -26838173.3659 - g_loss: -4055068021.3333\n",
      "Epoch 1971/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -3753453.5521 - g_loss: -4171190890.6667\n",
      "Epoch 1972/2000\n",
      "47/47 [==============================] - 15s 324ms/step - d_loss: -3925650.3932 - g_loss: -4236437280.0000\n",
      "Epoch 1973/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -29047213.4167 - g_loss: -4194281493.3333\n",
      "Epoch 1974/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 8192403.2891 - g_loss: -3885252762.6667\n",
      "Epoch 1975/2000\n",
      "47/47 [==============================] - 15s 315ms/step - d_loss: -21724048.9102 - g_loss: -3518783968.0000\n",
      "Epoch 1976/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -15773634.5573 - g_loss: -3472964496.0000\n",
      "Epoch 1977/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -20392041.4583 - g_loss: -3582452672.0000\n",
      "Epoch 1978/2000\n",
      "47/47 [==============================] - 15s 327ms/step - d_loss: -11687283.5312 - g_loss: -3689279578.6667\n",
      "Epoch 1979/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 36089184.9167 - g_loss: -3724421850.6667\n",
      "Epoch 1980/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -35950135.5573 - g_loss: -3751215882.6667\n",
      "Epoch 1981/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: -25546285.5208 - g_loss: -3994003941.3333\n",
      "Epoch 1982/2000\n",
      "47/47 [==============================] - 15s 317ms/step - d_loss: -9427152.1979 - g_loss: -4266279600.0000\n",
      "Epoch 1983/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -21802873.1667 - g_loss: -4772729536.0000\n",
      "Epoch 1984/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 1286073.5990 - g_loss: -5074345717.3333\n",
      "Epoch 1985/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -30768853.8233 - g_loss: -5493228885.3333\n",
      "Epoch 1986/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: 14097362.7396 - g_loss: -5973343573.3333\n",
      "Epoch 1987/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -4323101.3021 - g_loss: -6252864960.0000\n",
      "Epoch 1988/2000\n",
      "47/47 [==============================] - 15s 326ms/step - d_loss: -32873453.2135 - g_loss: -6255057653.3333\n",
      "Epoch 1989/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: -13452168.9583 - g_loss: -6225730538.6667\n",
      "Epoch 1990/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -28448405.1771 - g_loss: -6127846282.6667\n",
      "Epoch 1991/2000\n",
      "47/47 [==============================] - 15s 321ms/step - d_loss: -16957988.8346 - g_loss: -5845337898.6667\n",
      "Epoch 1992/2000\n",
      "47/47 [==============================] - 15s 316ms/step - d_loss: 12735589.6250 - g_loss: -5774829706.6667\n",
      "Epoch 1993/2000\n",
      "47/47 [==============================] - 15s 318ms/step - d_loss: 3867041.7500 - g_loss: -5669777504.0000\n",
      "Epoch 1994/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: -6631695.2604 - g_loss: -5533917344.0000\n",
      "Epoch 1995/2000\n",
      "47/47 [==============================] - 15s 323ms/step - d_loss: -764861.7344 - g_loss: -5528202880.0000\n",
      "Epoch 1996/2000\n",
      "47/47 [==============================] - 15s 320ms/step - d_loss: 8398987.2604 - g_loss: -5593391648.0000\n",
      "Epoch 1997/2000\n",
      "47/47 [==============================] - 26s 556ms/step - d_loss: -4372640.4583 - g_loss: -5612211029.3333\n",
      "Epoch 1998/2000\n",
      "47/47 [==============================] - 15s 325ms/step - d_loss: 52390572.6979 - g_loss: -5455882741.3333\n",
      "Epoch 1999/2000\n",
      "47/47 [==============================] - 15s 322ms/step - d_loss: 2703017.1667 - g_loss: -5401963562.6667\n",
      "Epoch 2000/2000\n",
      "47/47 [==============================] - 15s 319ms/step - d_loss: 8486598.4089 - g_loss: -5402482634.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f71c019e240>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training the model.\n",
    "wgan.fit(train_ds, batch_size=BATCH_SIZE, epochs=2000, callbacks=[cbk, tb_cbk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Examples using learned Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 11.5, 11.5, -0.5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALEAAACLCAYAAAAuyHgOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJ3ElEQVR4nO2dbYxdRRnHf8/dtrt92Za2oLT2DbSUghH8oMQXkGjVmohVkReJKChVUKOJ2IovSUFbEjUGQzBKxNgUEXwLGqt+UIRio4IfLAExxVpbV6AEli5923Z3744fziweTu/dme1tt/fB/y852T3nPDPzzMz/zJ17njvnWAgBITxTO94OCNEqErFwj0Qs3CMRC/dIxMI9ErFwz4tCxGZ2hZltPt5+iONDUsRmtsPM+s1sX2m7ZTycGy/M7K1mdq+Z7TWzXjPbYmafM7Ou4+1bFTNbb2Zrj0G+7zSzB81sf2yDO8xs3hjS32dmVx1Ff7Lzyx2JLwghTCttn2zBv7bCzC4Cfgr8EFgYQpgNXALMA+aPsy8TxqGMjgbH3kdR/28CJwJnAoeAzWY281j71DIhhFE3YAewrMm5bwM/K+1/FbgHMGAmsBF4Gtgd/59Xsr0PWAv8EdgH/BKYDdwB7AH+Aiwq2QfgU8B24Bng60AtnrsC2FyyPR34LfAssBW4uIn/BvQA1ybaoAZcB/wT6AV+DMyK5xZF3z4E/Dv69sUxpv1ITHt/PP4TYBfwHHA/cGY8/lFgEBgYabN4fGlszz7gb8C7SuWvj/30a2B/tS9jG+wEVjeo8yPAl+P+9cAPSudHfJ8ArAPqwMHo1y0ZfTbm/Jr2T4singI8FkV0bnR0Xjw3G7gw2nTHjvl5RcTbgJcDM4BHY17LYkU2AN+viPheYBawINpeVRUxMJVCmFfGfF4d/Tqjgf+nx3wXJdrg08CfKUbnTuBW4M5K438XmAycRTGKLR1D2g3R78nx+Idjm3VSjI5bKqJcW9qfGNvxC8Ak4M3AXmBJyf454A0Uwuxq0ganNKj3DcCfUqIr9edVlfSj9dmY82tVxPsorvKRbWXp/DkUI95O4P2j5HM2sLsi4vKI9Q3gN6X9CyqdF4Dlpf2PA/c0EPElwB8qZd8KrGng0xtjvl2lY3fFOh4ALo/H/g68pWQzh2JEnFBq/PKnzIPApWNIe+oo7XZCtJnRRMTnUozatdKxO4HrS/YbRsn/sDYonbsa+EeLIm7WZ2POr9mWOwd7dwjhd41OhBAeMLPtwEsoPioBMLMpwE3AcoqpBUC3mXWEEOpx/6lSVv0N9qdViusp/b8TmNvApYXAOWbWVzo2Abi9gW1v/DsH+Fesz6XR/83AyPxxIXC3mQ2X0taBl5b2d5X+P1DyPSft8/WKc9Z1wEXAScBIuhMpRtQqc4GeEEI5/53Ayxrl34Bn4t/n26DEnNL5IyWnz1qi5VtsZvYJio+9J4DVpVPXAkuAc0II04HzRpK0UFz5i9aCWGaVHmBTCOGE0jYthHBNA9utwOPAexPl9gDvqOTZFUJ4PMPnnLTlnxJeBqygmFbNoBih4H/tVv3Z4RPAfDMr9+WCWK9G+VfZCvyH4qJ5npjfhRTfcaCYT08pmZxcyadZGc367EjzO4yWRGxmp1F8OfsAcDmw2szOjqe7KUbTPjObBaxppazIKjObaWbzKeaaP2pgsxE4zcwuN7OJcXuNmS2tGsbR61pgjZmtjHmbmS3mhSPld4B1ZrYw1vskM1uR6fNY03ZTzKl7KTr5xsr5p4BTS/sPUIz8q2Ndz6eYit2V41woPrs/C3zJzC4zsy4zOxm4DZhO8WkKsAU4z8wWmNkM4PMJv0Zo1mdHml/DSuTMifsp5sUj290UH9EPAteVbK8BHqYYmedSzGv2UUzoP8Yocx6Ki2F9aX8ZsK0yvxr5pttLMYfuqM6J4/4S4FcUd0Z6gd8DZ49Sx+XApuhrL/BXYBUwNZ6vAZ+hGLX2UtxpuLHRXK5atyNIOw34RbTdCXww2rwinl9MIYA+4hdliltimyimG48C7ynlt57SHHqUNlhBcUdoP8V3nDuB+RWbb8VytwErK/35utjPu4GbU312JPk12ywmaHvMLACLQwjbjrcvIo/x6rMXRdhZ/H8jEQv3uJlOCNEMjcTCPRKxcM8x/9VULvGbbOtMmpgua9phP+Q6jNA/nLTh0FDaZlrmODFcT9scyMknoxlzwk0Z2YQQWglcHTU0Egv3SMTCPRKxcI9ELNwjEQv3SMTCPRKxcI9ELNzTNr+dsNqEDEcyAgI51bGMe/Rt0i5j5igFMnJQsEOIo4RELNwjEQv3SMTCPRKxcI9ELNwjEQv3SMTCPe2zsuOk1ydtwsCWdEZ79qVtxjOQUcuMB+T4NCGnuzJWpAxlLdvIKKs90Egs3CMRC/dIxMI9ErFwj0Qs3CMRC/dIxMI9ErFwT9sEOyb1V995cjgDh9I38kNn+jFWDGasEBnKsMkh57FSkLfaZCjjsVk541JOIKMt1mzkoZFYuEciFu6RiIV7JGLhHolYuEciFu6RiIV7JGLhnrYJdgwMNHqB/AsJs9KBjNrBdEAgDKbv5Hd0pK/v+nBG8MXymriW84iurnT9hwczVnbYpLTNQE5gpT3QSCzcIxEL90jEwj0SsXCPRCzcIxEL90jEwj0SsXBP2wQ7wilnJW0mXTk/nc/DfUmboa2PpW0e6knaEDICAsN5QYOMEAX0D6ZtMoI0DA7klOYGjcTCPRKxcI9ELNwjEQv3SMTCPRKxcI9ELNwjEQv3tE2w47LBPUmbZ5aen7TpvvpNSZvpTz+ZtLn9vgeSNuEr30va1J/anrQBsMGMR0t1pVdk1OrpfOqdGc+o2n8wbdMmaCQW7pGIhXskYuEeiVi4RyIW7pGIhXskYuEeiVi4RyIW7mmbiN1rz3hV0ubieWl3uw9uS9p0Tl+UtFn1tguSNm+/4bakTU/mI82yXk9z4FDSpF7LeYFN5stwnKCRWLhHIhbukYiFeyRi4R6JWLhHIhbukYiFeyRi4R4LOW9cHwc6pnckHQkL0tfcI1PTAZETvnZT0ubJDc8mbZZt7Eja7O9bk7QBsp7ZVrd0eTlPdatlvMBmaG9/0iaEkBFZOfZoJBbukYiFeyRi4R6JWLhHIhbukYiFeyRi4R6JWLinbYIdVksHO7LWP+Rclhk3+5mZUVZ/xutinst6pQxYRtwgp6s6089ry1khQoYuFOwQ4ighEQv3SMTCPRKxcI9ELNwjEQv3SMTCPRKxcE/7BDvM0o5MzAkIZFyXIR2AsFp6FUXOConhel48oIOM8qakV61MHkoHMvb0p1eRhEMDaRsFO4Q4OkjEwj0SsXCPRCzcIxEL90jEwj0SsXCPRCzc0zbv7KB7ctKkdih9b324I+O6fGVn0iQ8tDtpU89598XwYNoGqM9K199mpoMrg1NnJ23Cjl1ph/r8jG9+PBWiCRKxcI9ELNwjEQv3SMTCPRKxcI9ELNwjEQv3tM3KDiGOFI3Ewj0SsXCPRCzcIxEL90jEwj0SsXDPfwGRjt9ZroJwtgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load generator\n",
    "'''\n",
    "generator.compile(optimizer=Adam(lr=0.0008), # per Foster, 2017 RMSprop(lr=0.0008)\n",
    "                          loss=binary_crossentropy,\n",
    "                          metrics=['accuracy'])\n",
    "generator = tf.keras.models.load_model('/data/output/models/dwarfganWGANGPR02/generator-2021-04-04_025322.h5')\n",
    "'''\n",
    "# generate new example of learned representation in latent space\n",
    "try:\n",
    "    generator\n",
    "except NameError:\n",
    "    #get latest generator model save file\n",
    "    folder = pathlib.Path(f'{out_model_dir}/{model_name}')\n",
    "    saves = list(folder.glob('generator*'))\n",
    "    latest = max(saves, key=os.path.getctime)\n",
    "    #load latest generator save file\n",
    "    generator = tf.keras.models.load_model(latest)\n",
    "        \n",
    "noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "tiles = load_tiles()\n",
    "tiles = tiles[0].reshape((1,12,10,256)) #resphae into same shape as noise input\n",
    "res = np.array(generator.predict({'Generator_Input':noise,'Tiles_Input':tiles})).astype('uint8')\n",
    "\n",
    "#Rescale\n",
    "res = res.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Visualize result\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(res)\n",
    "plt.title(f'Example Generator Output')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
