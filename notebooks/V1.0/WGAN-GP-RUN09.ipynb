{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DwarfGAN - Deep Learning based Map Design for Dwarf Fortress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "loosely based on example coded provided in Foster, 2019 see:\n",
    "\n",
    "basic GAN\n",
    "\"G:\\Dev\\DataScience\\GDL_code\\models\\GAN.py\"\n",
    "\n",
    "Wasserstein GAN\n",
    "\"G:\\Dev\\DataScience\\GDL_code\\models\\WGAN.py\"\n",
    "\n",
    "Wasserstein GAN with Gradient Penatly\n",
    "\"G:\\Dev\\DataScience\\GDL_code\\models\\WGANGP.py\"\n",
    "\n",
    "\"\"\"\n",
    "# imports\n",
    "from keras.layers import Add, Concatenate, Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D, LayerNormalization\n",
    "from keras.layers.experimental import preprocessing\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.losses import binary_crossentropy, Loss\n",
    "from keras import metrics\n",
    "from functools import partial\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import random\n",
    "\n",
    "#!pip install boto3\n",
    "import boto3 as b3 \n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "############### CONFIG ###################\n",
    "\n",
    "# model name\n",
    "model_name = 'dwarfganWGANGPR09'\n",
    "# folder path to input files (map images)\n",
    "fpath = r'/data2/input'\n",
    "# folder path to tensorboard output\n",
    "tboard_dir = '/data2/output/tensorboard'\n",
    "# folder path for saved model output\n",
    "out_model_dir = '/data2/output/models'\n",
    "# folder for images to be saved during training\n",
    "out_img_dir = '/data2/output/images'\n",
    "# frequency of checkpoint saves (images, model weights) in epochs\n",
    "CHECKPOINT = 50\n",
    "# use skip connections (additive)?\n",
    "SKIP_ADD = False\n",
    "SKIP_CONCAT = False\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 1000 \n",
    "#BATCH_PER_EPOCH = 20\n",
    "# pre-processed (cropped) images are 1024x1024. We will later resize the images to 256x256 due to memory restrictions.\n",
    "IMAGE_SIZE = (12,12)\n",
    "BATCH_SIZE = 128\n",
    "CRITIC_FACTOR = 5 # number of times the critic is trained more often than the generator. Recommended = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "RELU_SLOPE_C = 0.2\n",
    "RELU_SLOPE_G = 0.2\n",
    "DROPOUT_C = 0.3\n",
    "MOMENTUM_G = 0.9\n",
    "CRIT_LR = 0.0003 # Adjusted learning rates according to two time-scale update rule (TTUR), see Heusel et al., 2017\n",
    "GEN_LR = 0.0001\n",
    "\n",
    "# NOTE: all extracted map PNGs have been saved on a separate virtual disk mapped to '/data' of the virtual machine in use\n",
    "data_dir = pathlib.Path(fpath + '/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Train / Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map images sourced from the DFMA come in a variety of dimensions. In order to create sample images with constant dimensions, as required by tensors, the 100k input samples were run through a python script to randomly crop 10 1024 x 1024 areas per picture. Of those cropped (sub-)images, only the ones which contain structures were retained. This was achieved by filtering out image crops which only contained two or less different colors. With that, the logic mainly filterd out crops which only contained black. This process resulted in 700'000+ (sub-)image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12068 cropped image samples available\n"
     ]
    }
   ],
   "source": [
    "# use pre-processed (cropped) 128 x 128 images\n",
    "data_dir = pathlib.Path(fpath + '/ascii_crops_12/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "print(f'There are {str(len(imgs))} cropped image samples available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random sample input image:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAIAAADZF8uwAAAAMUlEQVR4nGNgoAJIwCqag0v5fmrYicd8kkA8Gn8fA0M2qogZaQZmE1aCF2C3bh8WMQDzaAUVvKH48QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=12x12 at 0x7FDB900F74A8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show example sample image (cropped to 128x128)\n",
    "print('A random sample input image:')\n",
    "PIL.Image.open(imgs[random.randint(0,len(imgs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12068 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# creating keras datasets for training and validation - refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "dataset_train = tf.keras.preprocessing.image_dataset_from_directory(  fpath+'/ascii_crops_12',\n",
    "                                                                      image_size=IMAGE_SIZE, \n",
    "                                                                      batch_size=BATCH_SIZE, \n",
    "                                                                      labels=[1.] * len(imgs), # setting all labels to 1.0 (for 'real') as float32\n",
    "                                                                      #label_mode=None, # yields float32 type labels\n",
    "                                                                      seed=1234 #,\n",
    "                                                                   )\n",
    "\n",
    "# refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = dataset_train.cache().prefetch(buffer_size=BATCH_SIZE)\n",
    "#val_ds = dataset_val.cache().prefetch(buffer_size=BATCH_SIZE)\n",
    "#val_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAINING = 12068 # = 20% of total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Random Sample from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check random images from prepared batches\n",
    "plt.figure(figsize=(5, 5))\n",
    "for images, labels in train_ds.take(1): # take one batch. Here batch_size = 128 examples per batch\n",
    "    for i in range(9): # show first 9 images of batch\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(f'img {i}')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATED Model Definition\n",
    "\n",
    "Generally the following changes have been implemented to the architectures:\n",
    "\n",
    "### RUN09\n",
    "\n",
    "- instead of generating full maps, this model focuses on learning to recreate single tiles which represent the different objects/elements in the game world\n",
    "- implemented secondary input for both critic and generator which introduces the single tiles cropped from the tileset files (12x10) as a 12x10x256 vector (256 different 12x10 tiles) \n",
    "\n",
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discriminator_model():\n",
    "\n",
    "    # DISCRIMINATOR\n",
    "    # set input variables to variable width + height. Will be cropped in preprocessing [CURRENTLY FIXED TO 256x256]\n",
    "    input_dim = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "\n",
    "    # Input\n",
    "    d_input = Input(shape=input_dim, name='Discriminator_Input')\n",
    "    \n",
    "    \n",
    "\n",
    "    # ---- REMOVED FOR 256x256 NETWORK ----------\n",
    "    # Keras-based preprocessing. Alternative: RandomCrop()\n",
    "    # use smart_resizing?\n",
    "    #x = tf.keras.preprocessing.image.smart_resize(d_input, (1024, 1024))\n",
    "    #x = preprocessing.Resizing(width=512, \n",
    "    #                           height=512, \n",
    "    #                           name='Preprocessing_Resize'\n",
    "    #                          )(d_input) # Resize to 512 x 512 images\n",
    "\n",
    "    #we crop the images to 12x10 to match the tile dimensions\n",
    "    x = preprocessing.RandomCrop(height=12, \n",
    "                                width=10, \n",
    "                                name = 'Preprocessing_RandomCrop'\n",
    "                               )(d_input)\n",
    "\n",
    "    x = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale'\n",
    "                               )(x) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    # START TILES INPUT\n",
    "    tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "    \n",
    "    tiles = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale_Tiles'\n",
    "                               )(tiles_input) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    \n",
    "    #tiles = Conv2D(filters=256, kernel_size=(1,1), strides=1)(tiles)\n",
    "    \n",
    "    x = Concatenate()([x, tiles])\n",
    "    # END TILES INPUT\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 0\n",
    "    x = Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = (3,3), \n",
    "            strides = 1,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_0'\n",
    "    )(x)\n",
    "    \n",
    "    # Activation 0 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_0')(x)\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 1\n",
    "    x = Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_1'\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 1\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 1 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_1')(x)\n",
    "\n",
    "    # Dropout 1\n",
    "    x = Dropout(rate = DROPOUT_C)(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Conv2D Layer 2\n",
    "    x = Conv2D(\n",
    "            filters = 256,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            name = 'Discriminator_Conv2D_Layer_3',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 2\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 2 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_3')(x)\n",
    "\n",
    "\n",
    "    # OUTPUT\n",
    "    x = Flatten()(x)\n",
    "    #x = Dropout(DROPOUT_C)(x)\n",
    "    \n",
    "    d_output = Dense(1, \n",
    "                     #activation='sigmoid', \n",
    "                     kernel_initializer = RandomNormal(mean=0, stddev=0.02) # random initialization of weights with normal distribution around 0 with small SD\n",
    "                    )(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Discriminator Model intialization\n",
    "    discriminator = Model([d_input, tiles_input], d_output, name='Discriminator')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Discriminator_Input (InputLayer [(None, 12, 12, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_RandomCrop (Rando (None, 12, 10, 3)    0           Discriminator_Input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Tiles_Input (InputLayer)        [(None, 12, 10, 256) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_Rescale (Rescalin (None, 12, 10, 3)    0           Preprocessing_RandomCrop[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Preprocessing_Rescale_Tiles (Re (None, 12, 10, 256)  0           Tiles_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 12, 10, 259)  0           Preprocessing_Rescale[0][0]      \n",
      "                                                                 Preprocessing_Rescale_Tiles[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_0 (C (None, 12, 10, 64)   149248      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_0 (LeakyReLU)        (None, 12, 10, 64)   0           Discriminator_Conv2D_Layer_0[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_1 (C (None, 6, 5, 128)    73856       Activation_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_1 (LeakyReLU)        (None, 6, 5, 128)    0           Discriminator_Conv2D_Layer_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 6, 5, 128)    0           Activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_3 (C (None, 3, 3, 256)    295168      dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Activation_3 (LeakyReLU)        (None, 3, 3, 256)    0           Discriminator_Conv2D_Layer_3[0][0\n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 2304)         0           Activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 1)            2305        flatten_22[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 520,577\n",
      "Trainable params: 520,577\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = discriminator_model()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    # GENERATOR\n",
    "\n",
    "    # set input variable dimensions. Here we will start out with a vector of length 100 for each sample (sampled from a normal distribution, representing the learned latent space)\n",
    "    input_dim = (LATENT_DIM)\n",
    "\n",
    "    # Input\n",
    "    g_input = Input(shape=input_dim, name='Generator_Input')\n",
    "\n",
    "    # Dense Layer 1\n",
    "    x = Dense(np.prod([3,3,512]), kernel_initializer = RandomNormal(mean=0., stddev=0.02), \n",
    "              use_bias=False)(g_input) # use_bias=False see https://keras.io/examples/generative/wgan_gp/\n",
    "\n",
    "    # Batch Norm Layer 1\n",
    "    x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    \n",
    "    # Activation Layer 1\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Reshape into 3D tensor\n",
    "    x = Reshape((3,3,512))(x)\n",
    "\n",
    "    # Upsampling Layer 1 + Conv2D Layer1\n",
    "    x = Conv2DTranspose(filters=512, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=256, kernel_size=(3,3), padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=(3,3), padding='same', strides=(1,1), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    # START TILES\n",
    "    tiles_input = Input(shape=(12,10,256), name='Tiles_Input')\n",
    "    tiles = preprocessing.Resizing(width=12, height=12)(tiles_input)\n",
    "    \n",
    "    x = Concatenate()([x, tiles])\n",
    "    \n",
    "    # END TILES\n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "    \n",
    "    # reduce output dimensions via 1x1 convolution\n",
    "    x = Conv2D(filters=3, kernel_size=(1,1), padding='same', strides=(1,1),\n",
    "               kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "        \n",
    "    # tanh activation layer to scale values to [-1:1]\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    # Batch Norm Layer 7\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "\n",
    "    # Output - Rescale Values back to [0:255] since the discriminator will automatically rescale back down to [-1:1] as part of the pre-processing pipeline\n",
    "    g_output = (255 / 2) * (x + 1) \n",
    "\n",
    "\n",
    "    # Generator Model initialization\n",
    "    generator = Model([g_input, tiles_input], g_output, name='Generator')\n",
    "    \n",
    "    \n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Generator_Input (InputLayer)    [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 4608)         589824      Generator_Input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4608)         18432       dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, 4608)         0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 3, 3, 512)    0           leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_39 (Conv2DTran (None, 6, 6, 512)    2359296     reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, 6, 6, 512)    1024        conv2d_transpose_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, 6, 6, 512)    0           layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_40 (Conv2DTran (None, 12, 12, 256)  1179648     leaky_re_lu_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, 12, 12, 256)  512         conv2d_transpose_40[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, 12, 12, 256)  0           layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Tiles_Input (InputLayer)        [(None, 12, 10, 256) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_41 (Conv2DTran (None, 12, 12, 128)  294912      leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "resizing (Resizing)             (None, 12, 12, 256)  0           Tiles_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 12, 12, 384)  0           conv2d_transpose_41[0][0]        \n",
      "                                                                 resizing[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, 12, 12, 384)  768         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, 12, 12, 384)  0           layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 12, 12, 3)    1152        leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 12, 12, 3)    0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_15 (TensorFlo [(None, 12, 12, 3)]  0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_15 (TensorFlowO [(None, 12, 12, 3)]  0           tf_op_layer_AddV2_15[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 4,445,568\n",
      "Trainable params: 4,436,352\n",
      "Non-trainable params: 9,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen = generator_model()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP (Full) Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we compile the models, we need to implement a custom loss function which uses the Wasserstein distance and a gradient penalty term in order to ensure 1 Lipschitz constraints are followed. A WGAN with GP further involves a slightly more complicated training process which trains the critic (discriminator without sigmoid activation function) by feeding three different kinds of images:\n",
    "\n",
    "1. real images (i.e. available samples)\n",
    "2. 'fake' images (i.e. constructed by the generator)\n",
    "3. random interpolations between real and fake images (i.e. random samples interpolated from values between the fake and real images)\n",
    "\n",
    "The full training process of a critic is depicted below (source: Foster, 2019, p. 122):\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"wgan_gp_critic_training.png\"></img>\n",
    "    <i>Computational Graph for one Discriminator Training Epoch. (Source: Foster, 2019, p.122)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below roughly follows the OOP-based framework set by keras see https://keras.io/examples/generative/wgan_gp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope(): \n",
    "\n",
    "    critic = discriminator_model()\n",
    "    generator = generator_model()\n",
    "\n",
    "    class WGANGP(keras.Model):\n",
    "        def __init__(\n",
    "            self,\n",
    "            critic,\n",
    "            generator,\n",
    "            latent_dim,\n",
    "            tensorboard_callback,\n",
    "            critic_extra_steps=5,\n",
    "            gp_weight=10.0\n",
    "        ):\n",
    "            super(WGANGP, self).__init__()\n",
    "            self.critic = critic\n",
    "            self.generator = generator\n",
    "            self.latent_dim = latent_dim\n",
    "            self.tensorboard_callback = tensorboard_callback\n",
    "            self.d_steps = critic_extra_steps\n",
    "            self.gp_weight = gp_weight\n",
    "            \n",
    "\n",
    "        def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "            super(WGANGP, self).compile()\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.g_optimizer = g_optimizer\n",
    "            self.d_loss_fn = d_loss_fn\n",
    "            self.g_loss_fn = g_loss_fn\n",
    "\n",
    "        def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "            \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "            This loss is calculated on an interpolated image\n",
    "            and added to the discriminator loss.\n",
    "            \"\"\"\n",
    "            # Get the interpolated image\n",
    "            alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "            diff = fake_images - real_images\n",
    "            interpolated = real_images + alpha * diff\n",
    "\n",
    "            with tf.GradientTape() as gp_tape:\n",
    "                gp_tape.watch(interpolated)\n",
    "                # 1. Get the discriminator output for this interpolated image.\n",
    "                pred = self.critic(interpolated, training=True)\n",
    "\n",
    "            # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "            grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "            # 3. Calculate the norm of the gradients.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "            gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "            return gp\n",
    "\n",
    "        def train_step(self, real_images):\n",
    "            #checking whether we handed a tuple of (numpy) data to .fit().\n",
    "            #if not, the data must be a tf.data.Dataset generator that yields batches of datasets (data, labels)\n",
    "            if isinstance(real_images, tuple):\n",
    "                real_images = real_images[0]\n",
    "\n",
    "            # Get the batch size\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "            # For each batch, we are going to perform the\n",
    "            # following steps as laid out in the original paper:\n",
    "            # 1. Train the generator and get the generator loss\n",
    "            # 2. Train the discriminator and get the discriminator loss\n",
    "            # 3. Calculate the gradient penalty\n",
    "            # 4. Multiply this gradient penalty with a constant weight factor = self.discriminator_extra_steps = 5 (default value)\n",
    "            # 5. Add the gradient penalty to the discriminator loss\n",
    "            # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "            # Train the discriminator first. The original paper recommends training\n",
    "            # the discriminator for `x` more steps (typically 5) as compared to generator\n",
    "            \n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.critic)\n",
    "            \n",
    "            for i in range(self.d_steps):\n",
    "                # Get the latent vector\n",
    "                random_latent_vectors = tf.random.normal(\n",
    "                    shape=(batch_size, self.latent_dim)\n",
    "                )\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Generate fake images from the latent vector\n",
    "                    fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                    # Get the logits for the fake images\n",
    "                    fake_logits = self.critic(fake_images, training=True)\n",
    "                    # Get the logits for the real images\n",
    "                    real_logits = self.critic(real_images, training=True)\n",
    "\n",
    "                    # Calculate the discriminator loss using the fake and real image logits\n",
    "                    d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                    # Calculate the gradient penalty\n",
    "                    gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                    # Add the gradient penalty to the original discriminator loss\n",
    "                    d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "                # Get the gradients w.r.t the discriminator loss\n",
    "                d_gradient = tape.gradient(d_loss, self.critic.trainable_variables)\n",
    "                # Update the weights of the discriminator using the discriminator optimizer\n",
    "                self.d_optimizer.apply_gradients(\n",
    "                    zip(d_gradient, self.critic.trainable_variables)\n",
    "                )\n",
    "\n",
    "            # Train the generator\n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.generator)\n",
    "            \n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images using the generator\n",
    "                generated_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the discriminator logits for fake images\n",
    "                gen_img_logits = self.critic(generated_images, training=True)\n",
    "                # Calculate the generator loss\n",
    "                g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "            # Get the gradients w.r.t the generator loss\n",
    "            gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "            # Update the weights of the generator using the generator optimizer\n",
    "            self.g_optimizer.apply_gradients(\n",
    "                zip(gen_gradient, self.generator.trainable_variables)\n",
    "            )\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "        \n",
    "    class GANMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, num_img=5, latent_dim=128):\n",
    "            self.num_img = num_img\n",
    "            self.latent_dim = latent_dim\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None): #on_epoch_end(self, epoch, logs=None):\n",
    "            '''\n",
    "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "            generated_images = self.model.generator(random_latent_vectors)\n",
    "            #generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "            for i in range(self.num_img):\n",
    "                img = generated_images[i].numpy()\n",
    "                img = keras.preprocessing.image.array_to_img(img)\n",
    "                img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "            '''\n",
    "            \n",
    "            # Sample generator output for num_img images\n",
    "            noise = np.random.normal(0, 1, (self.num_img, self.latent_dim))\n",
    "            gen_imgs = generator.predict(noise)\n",
    "            gen_imgs = gen_imgs.astype('uint8')\n",
    "\n",
    "            #!!!NOT NECESSARY ANYMORE AS IMPLEMENTED AS PART OF THE MODEL!!!\n",
    "            #gen_imgs = 0.5 * (gen_imgs + 1)  #scale back to [0:1]\n",
    "            gen_imgs = gen_imgs.reshape((self.num_img, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "            # save n example images\n",
    "            for i in range(self.num_img):\n",
    "                plt.figure(figsize=(5, 5))\n",
    "                plt.imshow(gen_imgs[i])\n",
    "                #plt.title(f'Example Generator Output')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # adjust path based on whether execution is local or on linux VM\n",
    "                if pathlib.Path(f'{out_img_dir}/{model_name}').exists():\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    #mkdir\n",
    "                    os.mkdir(f'{out_img_dir}/{model_name}')\n",
    "                    #save\n",
    "                    plt.imsave(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png', gen_imgs[i])\n",
    "                    plt.close()\n",
    "                    \n",
    "            # save corresponding model\n",
    "            now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "            \n",
    "            if pathlib.Path(f'{out_model_dir}/{model_name}').exists():\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5')        \n",
    "            else:\n",
    "                #make dir\n",
    "                os.mkdir(f'{out_model_dir}/{model_name}')\n",
    "                #write\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5') \n",
    "        \n",
    "        \n",
    "    # Instantiate the optimizer for both networks\n",
    "    # (learning_rate=0.0002, beta_1=0.5 are recommended) as per Radford et al. 2016 pp. 3-4\n",
    "    generator_optimizer = Adam(\n",
    "        learning_rate=GEN_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "    critic_optimizer = Adam(\n",
    "        learning_rate=CRIT_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def critic_loss(real_img, fake_img):\n",
    "        real_loss = tf.reduce_mean(real_img)\n",
    "        fake_loss = tf.reduce_mean(fake_img)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(fake_img):\n",
    "        return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "    # Instantiate the custome `GANMonitor` Keras callback.\n",
    "    cbk = GANMonitor(num_img=5, latent_dim=LATENT_DIM)\n",
    "    \n",
    "    # Instantiate the tensorboard tf.keras callback\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "    tb_cbk = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = f'{tboard_dir}/{model_name}_{now}', \n",
    "        write_graph = False,\n",
    "        write_images = True,\n",
    "        histogram_freq = 1) \n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGANGP(\n",
    "        critic=critic,\n",
    "        generator=generator,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        tensorboard_callback=tb_cbk,\n",
    "        critic_extra_steps=CRITIC_FACTOR,\n",
    "        gp_weight=GRADIENT_PENALTY_WEIGHT\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "        d_optimizer=critic_optimizer,\n",
    "        g_optimizer=generator_optimizer,\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=critic_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 2/95 [..............................] - ETA: 1:11 - d_loss: 8.9651 - g_loss: -0.4414WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1294s vs `on_train_batch_end` time: 1.4030s). Check your callbacks.\n",
      "95/95 [==============================] - 15s 160ms/step - d_loss: -603.7732 - g_loss: -1456.9865\n",
      "Epoch 2/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -6491.2614 - g_loss: -9004.1944\n",
      "Epoch 3/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -11582.4803 - g_loss: -19229.6956\n",
      "Epoch 4/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -6118.8382 - g_loss: 7313.8805\n",
      "Epoch 5/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -11596.3526 - g_loss: 14340.0960\n",
      "Epoch 6/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -16540.1746 - g_loss: 13806.3162\n",
      "Epoch 7/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -720.7288 - g_loss: 21400.2575\n",
      "Epoch 8/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1332.3416 - g_loss: 12407.2219\n",
      "Epoch 9/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1803.4370 - g_loss: 10175.9906\n",
      "Epoch 10/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1917.3207 - g_loss: 10867.2177\n",
      "Epoch 11/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1805.5527 - g_loss: 5268.0465\n",
      "Epoch 12/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1822.2638 - g_loss: 2530.6025\n",
      "Epoch 13/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1557.6894 - g_loss: -777.5029\n",
      "Epoch 14/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1639.9877 - g_loss: -5021.6154\n",
      "Epoch 15/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1628.6148 - g_loss: -1302.6778\n",
      "Epoch 16/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1543.0073 - g_loss: 3998.8738\n",
      "Epoch 17/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1457.4566 - g_loss: 8479.5954\n",
      "Epoch 18/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1306.3164 - g_loss: 9450.7485\n",
      "Epoch 19/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1272.4629 - g_loss: 15064.0312\n",
      "Epoch 20/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1294.2648 - g_loss: 15775.1122\n",
      "Epoch 21/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1300.4971 - g_loss: 15515.9019\n",
      "Epoch 22/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1289.6059 - g_loss: 16439.3791\n",
      "Epoch 23/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1281.6198 - g_loss: 18315.1522\n",
      "Epoch 24/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1165.0969 - g_loss: 18821.9167\n",
      "Epoch 25/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1156.6060 - g_loss: 20737.1839\n",
      "Epoch 26/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1350.3475 - g_loss: 24595.9263\n",
      "Epoch 27/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1240.2541 - g_loss: 26176.7607\n",
      "Epoch 28/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1301.2135 - g_loss: 29034.1569\n",
      "Epoch 29/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1328.6176 - g_loss: 28428.6759\n",
      "Epoch 30/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1290.4770 - g_loss: 27814.3075\n",
      "Epoch 31/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1253.7769 - g_loss: 31067.7577\n",
      "Epoch 32/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1182.0175 - g_loss: 30444.4960\n",
      "Epoch 33/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1370.7260 - g_loss: 31937.0578\n",
      "Epoch 34/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1308.5427 - g_loss: 32891.8725\n",
      "Epoch 35/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1320.2617 - g_loss: 32697.5818\n",
      "Epoch 36/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1283.7510 - g_loss: 34311.1653\n",
      "Epoch 37/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1423.6006 - g_loss: 32985.7911\n",
      "Epoch 38/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1293.3505 - g_loss: 31215.3446\n",
      "Epoch 39/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1371.7795 - g_loss: 30237.0016\n",
      "Epoch 40/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1382.2025 - g_loss: 30347.4801\n",
      "Epoch 41/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1354.7258 - g_loss: 27922.1411\n",
      "Epoch 42/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1211.9027 - g_loss: 28926.7461\n",
      "Epoch 43/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1321.7356 - g_loss: 29315.0444\n",
      "Epoch 44/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1428.9902 - g_loss: 28596.3958\n",
      "Epoch 45/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1394.2186 - g_loss: 29682.0867\n",
      "Epoch 46/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1247.2917 - g_loss: 29288.7954\n",
      "Epoch 47/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1257.1203 - g_loss: 29224.0476\n",
      "Epoch 48/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1345.9988 - g_loss: 29555.0182\n",
      "Epoch 49/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1340.8307 - g_loss: 30264.7429\n",
      "Epoch 50/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1396.6812 - g_loss: 31086.0783\n",
      "Epoch 51/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1333.2860 - g_loss: 30840.4799\n",
      "Epoch 52/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1308.0366 - g_loss: 32115.8206\n",
      "Epoch 53/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -1310.1851 - g_loss: 31898.7484\n",
      "Epoch 54/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1286.2721 - g_loss: 31253.5020\n",
      "Epoch 55/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1284.8522 - g_loss: 30356.0110\n",
      "Epoch 56/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1156.5891 - g_loss: 29719.4299\n",
      "Epoch 57/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1277.8545 - g_loss: 28118.3433\n",
      "Epoch 58/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1272.4248 - g_loss: 27980.7916\n",
      "Epoch 59/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1207.8534 - g_loss: 25578.2289\n",
      "Epoch 60/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1253.0274 - g_loss: 26213.0093\n",
      "Epoch 61/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1153.1878 - g_loss: 27071.6943\n",
      "Epoch 62/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1145.7805 - g_loss: 26862.9460\n",
      "Epoch 63/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1352.9582 - g_loss: 27627.5811\n",
      "Epoch 64/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1126.8510 - g_loss: 27943.7450\n",
      "Epoch 65/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1248.6133 - g_loss: 29033.9377\n",
      "Epoch 66/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1143.4093 - g_loss: 27853.1856\n",
      "Epoch 67/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1180.8531 - g_loss: 27248.0673\n",
      "Epoch 68/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1228.8577 - g_loss: 26559.9415\n",
      "Epoch 69/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1227.6165 - g_loss: 25166.7347\n",
      "Epoch 70/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1230.0729 - g_loss: 26300.2905\n",
      "Epoch 71/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1230.8312 - g_loss: 25812.8630\n",
      "Epoch 72/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1233.7790 - g_loss: 25659.2032\n",
      "Epoch 73/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -1160.6935 - g_loss: 24877.9559\n",
      "Epoch 74/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1288.8033 - g_loss: 23485.0643\n",
      "Epoch 75/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1273.0648 - g_loss: 24215.8414\n",
      "Epoch 76/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1321.7825 - g_loss: 25442.7393\n",
      "Epoch 77/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1245.7149 - g_loss: 25712.8549\n",
      "Epoch 78/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1223.1872 - g_loss: 26671.5088\n",
      "Epoch 79/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1240.8733 - g_loss: 26649.8106\n",
      "Epoch 80/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1267.0805 - g_loss: 26515.3973\n",
      "Epoch 81/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1264.8842 - g_loss: 26616.8735\n",
      "Epoch 82/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1269.7114 - g_loss: 26965.3708\n",
      "Epoch 83/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1295.4055 - g_loss: 27807.4065\n",
      "Epoch 84/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1153.7285 - g_loss: 28102.3856\n",
      "Epoch 85/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1179.7783 - g_loss: 27766.1260\n",
      "Epoch 86/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1292.4930 - g_loss: 27718.1568\n",
      "Epoch 87/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1184.6023 - g_loss: 26848.9541\n",
      "Epoch 88/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1312.6981 - g_loss: 26399.6957\n",
      "Epoch 89/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1127.1837 - g_loss: 26255.3397\n",
      "Epoch 90/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1220.8078 - g_loss: 26507.1310\n",
      "Epoch 91/2000\n",
      "95/95 [==============================] - 12s 123ms/step - d_loss: -1172.0999 - g_loss: 26633.7185\n",
      "Epoch 92/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1220.0215 - g_loss: 26408.6665\n",
      "Epoch 93/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1264.4458 - g_loss: 26244.4685\n",
      "Epoch 94/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1322.2827 - g_loss: 26160.2522\n",
      "Epoch 95/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1189.5821 - g_loss: 27247.6608\n",
      "Epoch 96/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1107.3445 - g_loss: 26940.7227\n",
      "Epoch 97/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1168.9956 - g_loss: 26855.2031\n",
      "Epoch 98/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1182.5696 - g_loss: 26410.9081\n",
      "Epoch 99/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1258.1419 - g_loss: 26005.6967\n",
      "Epoch 100/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1222.7777 - g_loss: 25520.4091\n",
      "Epoch 101/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1163.8159 - g_loss: 25416.8576\n",
      "Epoch 102/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1163.6401 - g_loss: 24441.7470\n",
      "Epoch 103/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1107.6446 - g_loss: 24908.5208\n",
      "Epoch 104/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1130.5054 - g_loss: 25102.4952\n",
      "Epoch 105/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1135.1628 - g_loss: 25049.5067\n",
      "Epoch 106/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1185.3105 - g_loss: 25139.5005\n",
      "Epoch 107/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1205.1231 - g_loss: 23715.8753\n",
      "Epoch 108/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1108.1750 - g_loss: 24552.2667\n",
      "Epoch 109/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1092.2090 - g_loss: 23936.0781\n",
      "Epoch 110/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1063.7051 - g_loss: 23366.9866\n",
      "Epoch 111/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1208.2138 - g_loss: 23432.7257\n",
      "Epoch 112/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1150.1077 - g_loss: 22016.9786\n",
      "Epoch 113/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1197.5221 - g_loss: 23843.1347\n",
      "Epoch 114/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1136.1363 - g_loss: 24588.6793\n",
      "Epoch 115/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1242.6576 - g_loss: 24913.3033\n",
      "Epoch 116/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1084.3148 - g_loss: 24231.9645\n",
      "Epoch 117/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1163.7128 - g_loss: 22956.6183\n",
      "Epoch 118/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1190.2782 - g_loss: 23152.4777\n",
      "Epoch 119/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1196.4363 - g_loss: 22238.6465\n",
      "Epoch 120/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1203.1800 - g_loss: 23011.9686\n",
      "Epoch 121/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1143.7463 - g_loss: 24347.0303\n",
      "Epoch 122/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1152.0945 - g_loss: 24981.3063\n",
      "Epoch 123/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1190.0639 - g_loss: 23828.6615\n",
      "Epoch 124/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1286.4549 - g_loss: 23582.0174\n",
      "Epoch 125/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1212.1777 - g_loss: 24094.3737\n",
      "Epoch 126/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1204.2552 - g_loss: 23784.3902\n",
      "Epoch 127/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1216.0229 - g_loss: 24635.7709\n",
      "Epoch 128/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1204.7471 - g_loss: 23519.3246\n",
      "Epoch 129/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1135.8214 - g_loss: 22896.7901\n",
      "Epoch 130/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1047.4895 - g_loss: 21569.7923\n",
      "Epoch 131/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1120.8171 - g_loss: 23515.9814\n",
      "Epoch 132/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1120.9859 - g_loss: 24949.6911\n",
      "Epoch 133/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1175.1096 - g_loss: 22252.3118\n",
      "Epoch 134/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1158.7870 - g_loss: 22298.3560\n",
      "Epoch 135/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1110.4008 - g_loss: 23495.1214\n",
      "Epoch 136/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1210.6726 - g_loss: 23175.9327\n",
      "Epoch 137/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1103.4756 - g_loss: 22922.1137\n",
      "Epoch 138/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1163.1988 - g_loss: 23064.8832\n",
      "Epoch 139/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1142.1323 - g_loss: 23261.4835\n",
      "Epoch 140/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1220.4435 - g_loss: 22471.8754\n",
      "Epoch 141/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1121.7189 - g_loss: 21626.5433\n",
      "Epoch 142/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1160.5814 - g_loss: 23472.5914\n",
      "Epoch 143/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1141.0666 - g_loss: 23647.9614\n",
      "Epoch 144/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1044.4768 - g_loss: 22561.0282\n",
      "Epoch 145/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1186.8437 - g_loss: 22683.5319\n",
      "Epoch 146/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1130.3205 - g_loss: 23155.5011\n",
      "Epoch 147/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1125.2692 - g_loss: 22539.9520\n",
      "Epoch 148/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1219.6384 - g_loss: 22783.3998\n",
      "Epoch 149/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1228.4303 - g_loss: 23754.3431\n",
      "Epoch 150/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1070.3787 - g_loss: 24186.7213\n",
      "Epoch 151/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1226.9285 - g_loss: 23161.5234\n",
      "Epoch 152/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1205.8708 - g_loss: 22576.9156\n",
      "Epoch 153/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1177.0792 - g_loss: 20912.9942\n",
      "Epoch 154/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1063.6093 - g_loss: 19911.8800\n",
      "Epoch 155/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -1042.9572 - g_loss: 19756.8675\n",
      "Epoch 156/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1127.7187 - g_loss: 20072.5105\n",
      "Epoch 157/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1158.3641 - g_loss: 22123.0792\n",
      "Epoch 158/2000\n",
      "95/95 [==============================] - 12s 123ms/step - d_loss: -1057.7691 - g_loss: 20097.8152\n",
      "Epoch 159/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1116.4301 - g_loss: 20526.7026\n",
      "Epoch 160/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1176.3061 - g_loss: 21066.7151\n",
      "Epoch 161/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1184.0112 - g_loss: 20930.0337\n",
      "Epoch 162/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1081.6187 - g_loss: 21412.2701\n",
      "Epoch 163/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1136.1214 - g_loss: 21588.9945\n",
      "Epoch 164/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1082.4815 - g_loss: 21210.5055\n",
      "Epoch 165/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1110.8152 - g_loss: 22078.8517\n",
      "Epoch 166/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1114.2712 - g_loss: 22424.1540\n",
      "Epoch 167/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1215.0595 - g_loss: 20634.8368\n",
      "Epoch 168/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1044.4337 - g_loss: 22260.0052\n",
      "Epoch 169/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1004.6483 - g_loss: 22846.5022\n",
      "Epoch 170/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -989.8884 - g_loss: 23105.0634\n",
      "Epoch 171/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1123.3560 - g_loss: 23291.7598\n",
      "Epoch 172/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1102.7503 - g_loss: 22506.5725\n",
      "Epoch 173/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -1032.4573 - g_loss: 21974.7720\n",
      "Epoch 174/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1099.0334 - g_loss: 21945.0691\n",
      "Epoch 175/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1111.0854 - g_loss: 21838.7022\n",
      "Epoch 176/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1058.5807 - g_loss: 21626.9286\n",
      "Epoch 177/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1191.8467 - g_loss: 21664.0530\n",
      "Epoch 178/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1004.3833 - g_loss: 20960.2562\n",
      "Epoch 179/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1111.6311 - g_loss: 20062.8880\n",
      "Epoch 180/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1094.4634 - g_loss: 21262.1361\n",
      "Epoch 181/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1067.3881 - g_loss: 22089.6303\n",
      "Epoch 182/2000\n",
      "95/95 [==============================] - 13s 132ms/step - d_loss: -1097.3928 - g_loss: 21975.5032\n",
      "Epoch 183/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1254.0012 - g_loss: 21434.0667\n",
      "Epoch 184/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1015.7364 - g_loss: 21220.8683\n",
      "Epoch 185/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1045.2503 - g_loss: 19517.7089\n",
      "Epoch 186/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1096.6906 - g_loss: 19388.9999\n",
      "Epoch 187/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1005.8921 - g_loss: 19971.3604\n",
      "Epoch 188/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1037.5860 - g_loss: 19907.9004\n",
      "Epoch 189/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1068.0851 - g_loss: 21042.8790\n",
      "Epoch 190/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1022.7219 - g_loss: 21505.4530\n",
      "Epoch 191/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1077.2504 - g_loss: 20997.0623\n",
      "Epoch 192/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1053.2551 - g_loss: 20454.7048\n",
      "Epoch 193/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -1035.1645 - g_loss: 21107.6971\n",
      "Epoch 194/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -927.0520 - g_loss: 20912.9849\n",
      "Epoch 195/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1081.2386 - g_loss: 20197.2556\n",
      "Epoch 196/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -970.3807 - g_loss: 21736.3366\n",
      "Epoch 197/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -924.5259 - g_loss: 20302.7251\n",
      "Epoch 198/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -1095.7086 - g_loss: 18896.0455\n",
      "Epoch 199/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -953.3571 - g_loss: 18525.0492\n",
      "Epoch 200/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -869.7439 - g_loss: 20459.7405\n",
      "Epoch 201/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -997.7605 - g_loss: 20771.1776\n",
      "Epoch 202/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -949.0308 - g_loss: 20152.5120\n",
      "Epoch 203/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1039.0105 - g_loss: 21290.2702\n",
      "Epoch 204/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1054.1299 - g_loss: 21331.1611\n",
      "Epoch 205/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1066.4133 - g_loss: 20648.8310\n",
      "Epoch 206/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1081.2573 - g_loss: 20671.5570\n",
      "Epoch 207/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -946.8957 - g_loss: 21846.5876\n",
      "Epoch 208/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -931.7912 - g_loss: 21822.0259\n",
      "Epoch 209/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -924.4987 - g_loss: 20985.4878\n",
      "Epoch 210/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1048.8214 - g_loss: 20578.4885\n",
      "Epoch 211/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1070.0954 - g_loss: 20932.0638\n",
      "Epoch 212/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -1017.4987 - g_loss: 20582.3590\n",
      "Epoch 213/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -926.3292 - g_loss: 20543.0888\n",
      "Epoch 214/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -918.3883 - g_loss: 20801.3137\n",
      "Epoch 215/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1022.5557 - g_loss: 20682.9358\n",
      "Epoch 216/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -942.1529 - g_loss: 20543.7142\n",
      "Epoch 217/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1029.9819 - g_loss: 20587.9299\n",
      "Epoch 218/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1020.7879 - g_loss: 20643.5100\n",
      "Epoch 219/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -927.5099 - g_loss: 20810.7604\n",
      "Epoch 220/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1055.2084 - g_loss: 20100.6746\n",
      "Epoch 221/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -811.1212 - g_loss: 20697.9507\n",
      "Epoch 222/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -901.5635 - g_loss: 20183.2309\n",
      "Epoch 223/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1035.7077 - g_loss: 21166.3560\n",
      "Epoch 224/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -938.0418 - g_loss: 21864.9784\n",
      "Epoch 225/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1005.1382 - g_loss: 20740.8252\n",
      "Epoch 226/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -927.4958 - g_loss: 22515.5312\n",
      "Epoch 227/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -862.8170 - g_loss: 21447.9521\n",
      "Epoch 228/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -980.2685 - g_loss: 22109.8969\n",
      "Epoch 229/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -983.1886 - g_loss: 21762.9642\n",
      "Epoch 230/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -932.5329 - g_loss: 22629.0564\n",
      "Epoch 231/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1008.1016 - g_loss: 21788.0209\n",
      "Epoch 232/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -950.7525 - g_loss: 20140.4933\n",
      "Epoch 233/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -954.5366 - g_loss: 21334.1513\n",
      "Epoch 234/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -975.3986 - g_loss: 22904.6108\n",
      "Epoch 235/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1055.1604 - g_loss: 22773.0509\n",
      "Epoch 236/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -886.7688 - g_loss: 22226.5929\n",
      "Epoch 237/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -948.0378 - g_loss: 22914.2095\n",
      "Epoch 238/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -963.1832 - g_loss: 21919.7025\n",
      "Epoch 239/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -997.6964 - g_loss: 22403.2201\n",
      "Epoch 240/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -952.0239 - g_loss: 22873.2363\n",
      "Epoch 241/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -949.4859 - g_loss: 23171.6751\n",
      "Epoch 242/2000\n",
      "95/95 [==============================] - 14s 146ms/step - d_loss: -1006.4401 - g_loss: 22739.1765\n",
      "Epoch 243/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -1023.8831 - g_loss: 23119.8676\n",
      "Epoch 244/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -999.0712 - g_loss: 23638.9103\n",
      "Epoch 245/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -929.6013 - g_loss: 22471.4847\n",
      "Epoch 246/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -912.0025 - g_loss: 21623.4309\n",
      "Epoch 247/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -997.1511 - g_loss: 21642.8557\n",
      "Epoch 248/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -962.6785 - g_loss: 23230.6493\n",
      "Epoch 249/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -910.7643 - g_loss: 21810.4850\n",
      "Epoch 250/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -782.7774 - g_loss: 23121.3167\n",
      "Epoch 251/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -958.3418 - g_loss: 24217.0098\n",
      "Epoch 252/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -949.1889 - g_loss: 22834.6044\n",
      "Epoch 253/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -887.5793 - g_loss: 22859.5999\n",
      "Epoch 254/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -919.5941 - g_loss: 23858.6655\n",
      "Epoch 255/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1033.4487 - g_loss: 24359.6417\n",
      "Epoch 256/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -926.4568 - g_loss: 23738.8864\n",
      "Epoch 257/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -889.1843 - g_loss: 23173.2800\n",
      "Epoch 258/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -1044.7940 - g_loss: 22575.7197\n",
      "Epoch 259/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -902.9152 - g_loss: 22166.2613\n",
      "Epoch 260/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -914.4184 - g_loss: 21418.4399\n",
      "Epoch 261/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -953.2907 - g_loss: 21813.8554\n",
      "Epoch 262/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -869.6015 - g_loss: 21852.5648\n",
      "Epoch 263/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -910.3789 - g_loss: 23573.7806\n",
      "Epoch 264/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -905.2083 - g_loss: 23325.2184\n",
      "Epoch 265/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -861.7049 - g_loss: 23257.3800\n",
      "Epoch 266/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -894.8320 - g_loss: 22349.6374\n",
      "Epoch 267/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -989.0860 - g_loss: 21929.6462\n",
      "Epoch 268/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -1094.7289 - g_loss: 22596.3695\n",
      "Epoch 269/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -828.6813 - g_loss: 23290.9984\n",
      "Epoch 270/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -842.7408 - g_loss: 22547.2250\n",
      "Epoch 271/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -981.9381 - g_loss: 22854.4035\n",
      "Epoch 272/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -952.0608 - g_loss: 21314.8926\n",
      "Epoch 273/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -931.3415 - g_loss: 21389.5203\n",
      "Epoch 274/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -938.8753 - g_loss: 21978.5203\n",
      "Epoch 275/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -1009.6961 - g_loss: 24379.5292\n",
      "Epoch 276/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -909.5713 - g_loss: 24849.8026\n",
      "Epoch 277/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -996.2090 - g_loss: 24975.7536\n",
      "Epoch 278/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -920.3611 - g_loss: 23618.6474\n",
      "Epoch 279/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -908.2398 - g_loss: 25146.1175\n",
      "Epoch 280/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -871.6937 - g_loss: 24535.1695\n",
      "Epoch 281/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -841.4689 - g_loss: 25043.3927\n",
      "Epoch 282/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -829.4713 - g_loss: 25248.8347\n",
      "Epoch 283/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -903.3698 - g_loss: 26104.3719\n",
      "Epoch 284/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -906.3860 - g_loss: 25694.9087\n",
      "Epoch 285/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -950.6656 - g_loss: 25253.3293\n",
      "Epoch 286/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -914.6367 - g_loss: 25143.9472\n",
      "Epoch 287/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -878.2797 - g_loss: 25330.3895\n",
      "Epoch 288/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -918.6810 - g_loss: 24843.3090\n",
      "Epoch 289/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -902.6196 - g_loss: 24186.8341\n",
      "Epoch 290/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -870.9353 - g_loss: 25250.5438\n",
      "Epoch 291/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -899.7695 - g_loss: 24972.8551\n",
      "Epoch 292/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -852.3868 - g_loss: 24757.5381\n",
      "Epoch 293/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -912.4015 - g_loss: 24335.7450\n",
      "Epoch 294/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -913.8669 - g_loss: 23117.1325\n",
      "Epoch 295/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -853.6416 - g_loss: 22354.0813\n",
      "Epoch 296/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -895.0492 - g_loss: 21697.5363\n",
      "Epoch 297/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -889.1440 - g_loss: 20289.8137\n",
      "Epoch 298/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -838.6507 - g_loss: 20836.1538\n",
      "Epoch 299/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -798.3020 - g_loss: 20623.3029\n",
      "Epoch 300/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -758.4101 - g_loss: 21917.2772\n",
      "Epoch 301/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -870.4148 - g_loss: 22457.5587\n",
      "Epoch 302/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -789.4294 - g_loss: 22570.5019\n",
      "Epoch 303/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -811.0241 - g_loss: 22709.4848\n",
      "Epoch 304/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -826.9384 - g_loss: 22930.7900\n",
      "Epoch 305/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -814.7496 - g_loss: 22256.1065\n",
      "Epoch 306/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -835.2384 - g_loss: 21677.7140\n",
      "Epoch 307/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -912.0362 - g_loss: 21225.2069\n",
      "Epoch 308/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -877.1511 - g_loss: 21717.3800\n",
      "Epoch 309/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -823.9081 - g_loss: 21116.0704\n",
      "Epoch 310/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -876.2344 - g_loss: 21615.7556\n",
      "Epoch 311/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -870.0991 - g_loss: 21118.2829\n",
      "Epoch 312/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -775.3553 - g_loss: 18956.7827\n",
      "Epoch 313/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -791.1763 - g_loss: 19351.4141\n",
      "Epoch 314/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -826.0465 - g_loss: 19711.1566\n",
      "Epoch 315/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -846.9101 - g_loss: 19574.8255\n",
      "Epoch 316/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -866.5843 - g_loss: 19380.6862\n",
      "Epoch 317/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -899.9476 - g_loss: 18716.1346\n",
      "Epoch 318/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -886.3465 - g_loss: 18501.2485\n",
      "Epoch 319/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -846.9436 - g_loss: 18984.9752\n",
      "Epoch 320/2000\n",
      "95/95 [==============================] - 14s 149ms/step - d_loss: -831.4186 - g_loss: 18744.0945\n",
      "Epoch 321/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -825.7151 - g_loss: 19607.0548\n",
      "Epoch 322/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -863.4350 - g_loss: 20081.8580\n",
      "Epoch 323/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -796.5028 - g_loss: 20551.1916\n",
      "Epoch 324/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -775.6992 - g_loss: 20327.2997\n",
      "Epoch 325/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -803.5904 - g_loss: 20027.1198\n",
      "Epoch 326/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -902.6030 - g_loss: 19562.5522\n",
      "Epoch 327/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -780.8821 - g_loss: 18576.6852\n",
      "Epoch 328/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -801.2209 - g_loss: 19893.8418\n",
      "Epoch 329/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -894.5026 - g_loss: 21687.1587\n",
      "Epoch 330/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -829.7360 - g_loss: 21841.3255\n",
      "Epoch 331/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -853.1228 - g_loss: 20385.0395\n",
      "Epoch 332/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -794.9252 - g_loss: 20611.2140\n",
      "Epoch 333/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -774.1630 - g_loss: 19883.9037\n",
      "Epoch 334/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -781.0720 - g_loss: 21151.9674\n",
      "Epoch 335/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -847.4656 - g_loss: 21237.6506\n",
      "Epoch 336/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -859.2691 - g_loss: 21497.0747\n",
      "Epoch 337/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -896.0807 - g_loss: 20456.6834\n",
      "Epoch 338/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -788.7465 - g_loss: 21176.8354\n",
      "Epoch 339/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -806.8918 - g_loss: 21541.6673\n",
      "Epoch 340/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -748.1210 - g_loss: 21334.4908\n",
      "Epoch 341/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -852.2989 - g_loss: 20228.7309\n",
      "Epoch 342/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -930.2657 - g_loss: 20523.2480\n",
      "Epoch 343/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -942.2161 - g_loss: 20137.2599\n",
      "Epoch 344/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -748.0737 - g_loss: 20426.1286\n",
      "Epoch 345/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -745.2999 - g_loss: 21647.2318\n",
      "Epoch 346/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -730.7389 - g_loss: 21658.1335\n",
      "Epoch 347/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -826.1326 - g_loss: 20630.8123\n",
      "Epoch 348/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -834.4285 - g_loss: 20343.2077\n",
      "Epoch 349/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -706.7059 - g_loss: 20096.0560\n",
      "Epoch 350/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -936.9335 - g_loss: 21485.9101\n",
      "Epoch 351/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -889.8020 - g_loss: 22874.4611\n",
      "Epoch 352/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -882.8703 - g_loss: 22669.5573\n",
      "Epoch 353/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -769.9822 - g_loss: 20930.7934\n",
      "Epoch 354/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -866.6034 - g_loss: 21763.9516\n",
      "Epoch 355/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -763.6528 - g_loss: 19077.4250\n",
      "Epoch 356/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -794.0685 - g_loss: 20139.7625\n",
      "Epoch 357/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -805.9534 - g_loss: 19321.2265\n",
      "Epoch 358/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -807.1053 - g_loss: 19022.1424\n",
      "Epoch 359/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -780.7471 - g_loss: 19719.7222\n",
      "Epoch 360/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -763.6658 - g_loss: 19863.6345\n",
      "Epoch 361/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -846.9230 - g_loss: 20629.6844\n",
      "Epoch 362/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -749.2166 - g_loss: 21293.9739\n",
      "Epoch 363/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -796.2090 - g_loss: 20676.4558\n",
      "Epoch 364/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -798.0245 - g_loss: 20023.9275\n",
      "Epoch 365/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -762.7605 - g_loss: 19489.3225\n",
      "Epoch 366/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -676.2996 - g_loss: 18538.9942\n",
      "Epoch 367/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -776.6144 - g_loss: 18856.6491\n",
      "Epoch 368/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -727.3209 - g_loss: 19359.4470\n",
      "Epoch 369/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -754.8106 - g_loss: 19947.2203\n",
      "Epoch 370/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -893.7675 - g_loss: 19626.3012\n",
      "Epoch 371/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -768.7415 - g_loss: 19167.5765\n",
      "Epoch 372/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -803.5476 - g_loss: 20120.2580\n",
      "Epoch 373/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -800.3973 - g_loss: 19529.3568\n",
      "Epoch 374/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -743.9649 - g_loss: 19127.5094\n",
      "Epoch 375/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -852.9213 - g_loss: 18869.2726\n",
      "Epoch 376/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -767.2049 - g_loss: 18382.8659\n",
      "Epoch 377/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -785.5556 - g_loss: 18733.7326\n",
      "Epoch 378/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -680.7071 - g_loss: 18934.2935\n",
      "Epoch 379/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -747.7033 - g_loss: 18877.4075\n",
      "Epoch 380/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -821.2393 - g_loss: 20359.9021\n",
      "Epoch 381/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -707.6054 - g_loss: 20693.0842\n",
      "Epoch 382/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -764.6856 - g_loss: 18800.8319\n",
      "Epoch 383/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -828.5234 - g_loss: 19470.9310\n",
      "Epoch 384/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -796.6791 - g_loss: 20076.2883\n",
      "Epoch 385/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -796.2620 - g_loss: 20118.3700\n",
      "Epoch 386/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -722.2770 - g_loss: 20111.9069\n",
      "Epoch 387/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -729.9198 - g_loss: 21134.3987\n",
      "Epoch 388/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -790.6259 - g_loss: 21302.2357\n",
      "Epoch 389/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -780.1975 - g_loss: 21558.7568\n",
      "Epoch 390/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -799.4103 - g_loss: 21319.2778\n",
      "Epoch 391/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -706.5369 - g_loss: 20710.5885\n",
      "Epoch 392/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -753.6411 - g_loss: 20848.2352\n",
      "Epoch 393/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -774.6091 - g_loss: 21318.0390\n",
      "Epoch 394/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -777.4313 - g_loss: 22114.6580\n",
      "Epoch 395/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -696.2108 - g_loss: 21905.6039\n",
      "Epoch 396/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -738.6603 - g_loss: 21346.6108\n",
      "Epoch 397/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -687.9810 - g_loss: 21070.7780\n",
      "Epoch 398/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -749.4040 - g_loss: 20382.9181\n",
      "Epoch 399/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -827.9145 - g_loss: 21217.4096\n",
      "Epoch 400/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -793.6373 - g_loss: 21048.8714\n",
      "Epoch 401/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -810.2643 - g_loss: 18933.7320\n",
      "Epoch 402/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -760.8714 - g_loss: 18723.6120\n",
      "Epoch 403/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -716.5249 - g_loss: 19387.5689\n",
      "Epoch 404/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -670.9445 - g_loss: 20036.9642\n",
      "Epoch 405/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -731.0870 - g_loss: 21151.9025\n",
      "Epoch 406/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -700.1509 - g_loss: 20709.0125\n",
      "Epoch 407/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -819.2407 - g_loss: 20537.7094\n",
      "Epoch 408/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -796.1717 - g_loss: 20814.7384\n",
      "Epoch 409/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -674.5551 - g_loss: 20953.5399\n",
      "Epoch 410/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -651.5602 - g_loss: 21750.0235\n",
      "Epoch 411/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -770.1991 - g_loss: 20989.4190\n",
      "Epoch 412/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -810.0154 - g_loss: 22552.8901\n",
      "Epoch 413/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -725.8674 - g_loss: 21960.2943\n",
      "Epoch 414/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -686.1236 - g_loss: 21786.8621\n",
      "Epoch 415/2000\n",
      "95/95 [==============================] - 15s 154ms/step - d_loss: -761.9213 - g_loss: 20700.0149\n",
      "Epoch 416/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -710.2216 - g_loss: 21277.4423\n",
      "Epoch 417/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -763.0205 - g_loss: 20104.3582\n",
      "Epoch 418/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -877.4861 - g_loss: 20220.1529\n",
      "Epoch 419/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -705.9610 - g_loss: 19339.8493\n",
      "Epoch 420/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -667.7563 - g_loss: 21529.7122\n",
      "Epoch 421/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -776.1890 - g_loss: 20862.1981\n",
      "Epoch 422/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -758.8033 - g_loss: 20234.1470\n",
      "Epoch 423/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -759.1352 - g_loss: 19700.2177\n",
      "Epoch 424/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -858.6316 - g_loss: 21193.1906\n",
      "Epoch 425/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -781.8696 - g_loss: 22245.3703\n",
      "Epoch 426/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -785.5180 - g_loss: 21566.2186\n",
      "Epoch 427/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -774.3613 - g_loss: 20585.4952\n",
      "Epoch 428/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -803.4219 - g_loss: 20433.8902\n",
      "Epoch 429/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -874.3613 - g_loss: 19983.4523\n",
      "Epoch 430/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -805.9039 - g_loss: 21403.8176\n",
      "Epoch 431/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -843.5278 - g_loss: 22285.5704\n",
      "Epoch 432/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -758.7924 - g_loss: 22291.2529\n",
      "Epoch 433/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -645.3184 - g_loss: 21824.7435\n",
      "Epoch 434/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -800.7666 - g_loss: 21448.1187\n",
      "Epoch 435/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -791.2597 - g_loss: 21627.4656\n",
      "Epoch 436/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -725.7514 - g_loss: 21673.8506\n",
      "Epoch 437/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -753.3014 - g_loss: 21363.8245\n",
      "Epoch 438/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -692.9279 - g_loss: 20720.1894\n",
      "Epoch 439/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -631.8157 - g_loss: 20583.0258\n",
      "Epoch 440/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -800.4343 - g_loss: 21381.8641\n",
      "Epoch 441/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -792.6532 - g_loss: 21600.9626\n",
      "Epoch 442/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -719.8396 - g_loss: 20570.4304\n",
      "Epoch 443/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -762.0992 - g_loss: 21293.5159\n",
      "Epoch 444/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -663.5863 - g_loss: 22376.5123\n",
      "Epoch 445/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -827.6685 - g_loss: 21472.2644\n",
      "Epoch 446/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -776.9637 - g_loss: 20741.1089\n",
      "Epoch 447/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -684.5637 - g_loss: 21848.1743\n",
      "Epoch 448/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -673.7738 - g_loss: 21276.2677\n",
      "Epoch 449/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -699.8247 - g_loss: 21475.2965\n",
      "Epoch 450/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -865.7027 - g_loss: 21072.3857\n",
      "Epoch 451/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -771.8755 - g_loss: 20253.0203\n",
      "Epoch 452/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -829.4586 - g_loss: 19428.4636\n",
      "Epoch 453/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -755.1123 - g_loss: 19873.5515\n",
      "Epoch 454/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -653.6718 - g_loss: 20812.4055\n",
      "Epoch 455/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -744.2649 - g_loss: 21147.5809\n",
      "Epoch 456/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -718.3020 - g_loss: 21133.0632\n",
      "Epoch 457/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -820.7488 - g_loss: 20588.7627\n",
      "Epoch 458/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -762.4400 - g_loss: 21730.4671\n",
      "Epoch 459/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -678.0687 - g_loss: 21213.6649\n",
      "Epoch 460/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -679.8552 - g_loss: 20488.0517\n",
      "Epoch 461/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -668.4006 - g_loss: 20293.7987\n",
      "Epoch 462/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -745.7513 - g_loss: 20582.7089\n",
      "Epoch 463/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -694.4510 - g_loss: 20623.5468\n",
      "Epoch 464/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -708.4221 - g_loss: 21700.1932\n",
      "Epoch 465/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -790.7516 - g_loss: 23071.1047\n",
      "Epoch 466/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -750.9215 - g_loss: 22340.10020s - d_loss: -740.9031 - g_loss: 22357.851\n",
      "Epoch 467/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -660.8504 - g_loss: 21759.8988\n",
      "Epoch 468/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -719.7313 - g_loss: 20880.6998\n",
      "Epoch 469/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -820.1569 - g_loss: 20873.4251\n",
      "Epoch 470/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -754.4155 - g_loss: 21521.7457\n",
      "Epoch 471/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -745.1656 - g_loss: 22231.0089\n",
      "Epoch 472/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -694.7165 - g_loss: 20315.7887\n",
      "Epoch 473/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -736.8778 - g_loss: 20805.3037\n",
      "Epoch 474/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -734.6213 - g_loss: 20940.5424\n",
      "Epoch 475/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -707.5229 - g_loss: 22096.7862\n",
      "Epoch 476/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -751.4921 - g_loss: 21512.2651\n",
      "Epoch 477/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -798.2647 - g_loss: 21534.5167\n",
      "Epoch 478/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -751.8177 - g_loss: 21176.1407\n",
      "Epoch 479/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -821.5634 - g_loss: 22016.4473\n",
      "Epoch 480/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -696.5241 - g_loss: 22918.7064\n",
      "Epoch 481/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -759.7158 - g_loss: 22016.3728\n",
      "Epoch 482/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -662.3346 - g_loss: 22348.0581\n",
      "Epoch 483/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -690.7833 - g_loss: 21432.0466\n",
      "Epoch 484/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -719.8389 - g_loss: 22225.8895\n",
      "Epoch 485/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -701.1345 - g_loss: 23233.6023\n",
      "Epoch 486/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -735.0879 - g_loss: 23218.7155\n",
      "Epoch 487/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -738.3390 - g_loss: 22347.0711\n",
      "Epoch 488/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -706.7615 - g_loss: 21597.5427\n",
      "Epoch 489/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -696.5562 - g_loss: 20852.0718\n",
      "Epoch 490/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -671.0407 - g_loss: 21397.0875\n",
      "Epoch 491/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -700.4328 - g_loss: 22370.2344\n",
      "Epoch 492/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -737.5615 - g_loss: 23591.9330\n",
      "Epoch 493/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -712.0703 - g_loss: 23629.2223\n",
      "Epoch 494/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -701.4919 - g_loss: 22899.0368\n",
      "Epoch 495/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -673.8280 - g_loss: 24193.5040\n",
      "Epoch 496/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -676.6249 - g_loss: 23819.4235\n",
      "Epoch 497/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -643.0483 - g_loss: 23805.3662\n",
      "Epoch 498/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -726.5851 - g_loss: 24730.0801\n",
      "Epoch 499/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -723.2733 - g_loss: 24640.1168\n",
      "Epoch 500/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -622.9417 - g_loss: 24234.5749\n",
      "Epoch 501/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -645.6555 - g_loss: 23584.6830\n",
      "Epoch 502/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -686.0637 - g_loss: 23243.9511\n",
      "Epoch 503/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -624.6924 - g_loss: 23096.1102\n",
      "Epoch 504/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -717.0471 - g_loss: 24093.4799\n",
      "Epoch 505/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -662.7544 - g_loss: 23060.2361\n",
      "Epoch 506/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -714.9000 - g_loss: 22071.6028\n",
      "Epoch 507/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -705.8223 - g_loss: 22338.8278\n",
      "Epoch 508/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -736.7790 - g_loss: 23030.7543\n",
      "Epoch 509/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -673.3522 - g_loss: 23753.0590\n",
      "Epoch 510/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -733.3639 - g_loss: 24008.1557\n",
      "Epoch 511/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -774.6662 - g_loss: 23284.1580\n",
      "Epoch 512/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -748.7586 - g_loss: 23858.6798\n",
      "Epoch 513/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -579.7235 - g_loss: 22208.0188\n",
      "Epoch 514/2000\n",
      "95/95 [==============================] - 12s 131ms/step - d_loss: -760.1252 - g_loss: 19973.8651\n",
      "Epoch 515/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -728.3957 - g_loss: 20564.5835\n",
      "Epoch 516/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -738.9362 - g_loss: 21015.0697\n",
      "Epoch 517/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -790.9102 - g_loss: 20907.6374\n",
      "Epoch 518/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -696.3359 - g_loss: 20910.0109\n",
      "Epoch 519/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -759.5541 - g_loss: 20266.1010\n",
      "Epoch 520/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -705.6007 - g_loss: 19886.3796\n",
      "Epoch 521/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -804.7458 - g_loss: 20117.4529\n",
      "Epoch 522/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -603.1211 - g_loss: 21296.1497\n",
      "Epoch 523/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -716.3740 - g_loss: 22187.0374\n",
      "Epoch 524/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -692.9883 - g_loss: 20301.2443\n",
      "Epoch 525/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -674.8845 - g_loss: 21276.4386\n",
      "Epoch 526/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -713.2752 - g_loss: 21485.9536\n",
      "Epoch 527/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -717.1878 - g_loss: 21837.7228\n",
      "Epoch 528/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -646.5719 - g_loss: 21130.0074\n",
      "Epoch 529/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -624.3731 - g_loss: 20829.7075\n",
      "Epoch 530/2000\n",
      "95/95 [==============================] - 16s 167ms/step - d_loss: -697.1672 - g_loss: 20312.1299\n",
      "Epoch 531/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -758.7512 - g_loss: 19631.7341\n",
      "Epoch 532/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -637.9840 - g_loss: 19369.9895\n",
      "Epoch 533/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -673.6219 - g_loss: 19828.6466\n",
      "Epoch 534/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -711.3468 - g_loss: 20299.5589\n",
      "Epoch 535/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -657.0472 - g_loss: 20019.8082\n",
      "Epoch 536/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -687.4746 - g_loss: 20031.2065\n",
      "Epoch 537/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -650.9318 - g_loss: 19139.1774\n",
      "Epoch 538/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -699.8655 - g_loss: 19545.4135\n",
      "Epoch 539/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -768.9635 - g_loss: 19466.7484\n",
      "Epoch 540/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -675.1206 - g_loss: 20249.8302\n",
      "Epoch 541/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -686.4376 - g_loss: 19924.5699\n",
      "Epoch 542/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -714.9968 - g_loss: 20151.3941\n",
      "Epoch 543/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -734.9557 - g_loss: 20490.9050\n",
      "Epoch 544/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -610.7457 - g_loss: 20054.5836\n",
      "Epoch 545/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -741.3520 - g_loss: 20047.6474\n",
      "Epoch 546/2000\n",
      "95/95 [==============================] - 12s 123ms/step - d_loss: -728.1007 - g_loss: 19492.7730\n",
      "Epoch 547/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -701.5337 - g_loss: 19103.4498\n",
      "Epoch 548/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -730.8254 - g_loss: 19173.6197\n",
      "Epoch 549/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -614.0122 - g_loss: 18431.2378\n",
      "Epoch 550/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -698.2645 - g_loss: 18772.2081\n",
      "Epoch 551/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -729.5937 - g_loss: 18546.9180\n",
      "Epoch 552/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -618.9627 - g_loss: 19270.7464\n",
      "Epoch 553/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -683.8607 - g_loss: 19282.5764\n",
      "Epoch 554/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -715.9266 - g_loss: 19314.0826\n",
      "Epoch 555/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -675.5377 - g_loss: 20412.0246\n",
      "Epoch 556/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -621.5700 - g_loss: 20322.5689\n",
      "Epoch 557/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -623.7583 - g_loss: 19319.5577\n",
      "Epoch 558/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -675.5590 - g_loss: 19991.5802\n",
      "Epoch 559/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -756.6090 - g_loss: 20092.5530\n",
      "Epoch 560/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -698.9351 - g_loss: 20391.3550\n",
      "Epoch 561/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -773.6765 - g_loss: 21104.9591\n",
      "Epoch 562/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -709.6697 - g_loss: 19724.2624\n",
      "Epoch 563/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -645.4124 - g_loss: 20072.8599\n",
      "Epoch 564/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -675.9567 - g_loss: 20818.8813\n",
      "Epoch 565/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -751.2873 - g_loss: 20878.7917\n",
      "Epoch 566/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -734.9024 - g_loss: 20422.7648\n",
      "Epoch 567/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -690.7805 - g_loss: 20688.0851\n",
      "Epoch 568/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -597.7021 - g_loss: 19608.6260\n",
      "Epoch 569/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -711.9550 - g_loss: 19162.5438\n",
      "Epoch 570/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -697.8515 - g_loss: 18997.5978\n",
      "Epoch 571/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -732.5885 - g_loss: 19180.2550\n",
      "Epoch 572/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -698.0933 - g_loss: 18927.4647\n",
      "Epoch 573/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -686.5060 - g_loss: 18232.5113\n",
      "Epoch 574/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -743.2122 - g_loss: 18635.3222\n",
      "Epoch 575/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -686.0637 - g_loss: 19529.9925\n",
      "Epoch 576/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -670.0448 - g_loss: 17829.0924\n",
      "Epoch 577/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -674.1250 - g_loss: 17865.0383\n",
      "Epoch 578/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -662.3432 - g_loss: 17354.8617\n",
      "Epoch 579/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -751.7949 - g_loss: 17422.4401\n",
      "Epoch 580/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -641.2587 - g_loss: 18403.3968\n",
      "Epoch 581/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -677.7737 - g_loss: 17703.9992\n",
      "Epoch 582/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -555.5177 - g_loss: 16503.8373\n",
      "Epoch 583/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -596.0752 - g_loss: 15858.0608\n",
      "Epoch 584/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -635.3517 - g_loss: 16133.4855\n",
      "Epoch 585/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -688.6287 - g_loss: 16587.3888\n",
      "Epoch 586/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -645.4781 - g_loss: 16256.0338\n",
      "Epoch 587/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -757.2450 - g_loss: 16468.7881\n",
      "Epoch 588/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -746.7388 - g_loss: 16814.6987\n",
      "Epoch 589/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -681.6965 - g_loss: 15996.3931\n",
      "Epoch 590/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -631.2617 - g_loss: 15275.6346\n",
      "Epoch 591/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -727.5391 - g_loss: 16841.7520\n",
      "Epoch 592/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -689.1837 - g_loss: 16890.0476\n",
      "Epoch 593/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -654.2461 - g_loss: 16936.7669\n",
      "Epoch 594/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -684.6203 - g_loss: 17377.4667\n",
      "Epoch 595/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -713.3746 - g_loss: 17399.1414\n",
      "Epoch 596/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -755.2853 - g_loss: 17809.6539\n",
      "Epoch 597/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -664.9744 - g_loss: 17572.7235\n",
      "Epoch 598/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -664.2375 - g_loss: 17046.2458\n",
      "Epoch 599/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -645.8198 - g_loss: 17257.7794\n",
      "Epoch 600/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -632.3698 - g_loss: 17165.8285\n",
      "Epoch 601/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -628.7243 - g_loss: 18126.7775\n",
      "Epoch 602/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -577.1429 - g_loss: 17369.0940\n",
      "Epoch 603/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -638.7015 - g_loss: 17350.2092\n",
      "Epoch 604/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -662.8444 - g_loss: 17924.3807\n",
      "Epoch 605/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -644.0332 - g_loss: 18154.7727\n",
      "Epoch 606/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -718.2737 - g_loss: 17687.8932\n",
      "Epoch 607/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -671.5935 - g_loss: 17292.8858\n",
      "Epoch 608/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -667.1357 - g_loss: 18319.8044\n",
      "Epoch 609/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -691.0677 - g_loss: 18437.1712\n",
      "Epoch 610/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -724.1197 - g_loss: 18480.2306\n",
      "Epoch 611/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -752.6474 - g_loss: 18889.8765\n",
      "Epoch 612/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -650.2326 - g_loss: 17711.6716\n",
      "Epoch 613/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -727.4823 - g_loss: 17305.2219\n",
      "Epoch 614/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -641.9323 - g_loss: 17010.2967\n",
      "Epoch 615/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -670.7423 - g_loss: 17644.7006\n",
      "Epoch 616/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -753.3418 - g_loss: 18653.6844\n",
      "Epoch 617/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -529.3700 - g_loss: 18027.9777\n",
      "Epoch 618/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -600.2636 - g_loss: 17963.0084\n",
      "Epoch 619/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -624.0011 - g_loss: 17854.1518\n",
      "Epoch 620/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -656.2038 - g_loss: 18483.3849\n",
      "Epoch 621/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -623.6284 - g_loss: 18699.8127\n",
      "Epoch 622/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -698.7206 - g_loss: 18700.5082\n",
      "Epoch 623/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -631.9348 - g_loss: 18907.0545\n",
      "Epoch 624/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -720.0832 - g_loss: 18030.8220\n",
      "Epoch 625/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -643.8171 - g_loss: 18486.0819\n",
      "Epoch 626/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -604.6478 - g_loss: 19343.3663\n",
      "Epoch 627/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -742.5508 - g_loss: 19170.9067\n",
      "Epoch 628/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -642.5287 - g_loss: 18523.9886\n",
      "Epoch 629/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -725.0068 - g_loss: 18819.1248\n",
      "Epoch 630/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -697.2347 - g_loss: 17989.4802\n",
      "Epoch 631/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -673.2172 - g_loss: 17295.5823\n",
      "Epoch 632/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -614.7212 - g_loss: 16775.3930\n",
      "Epoch 633/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -684.2526 - g_loss: 17412.5258\n",
      "Epoch 634/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -665.6281 - g_loss: 18287.1478\n",
      "Epoch 635/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -663.1415 - g_loss: 17634.4398\n",
      "Epoch 636/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -660.1033 - g_loss: 17204.7455\n",
      "Epoch 637/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -652.3380 - g_loss: 16564.1496\n",
      "Epoch 638/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -633.0039 - g_loss: 17527.7800\n",
      "Epoch 639/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -617.3772 - g_loss: 16236.3915\n",
      "Epoch 640/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -734.4678 - g_loss: 16275.4085\n",
      "Epoch 641/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -647.5063 - g_loss: 16650.5674\n",
      "Epoch 642/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -648.1253 - g_loss: 17145.9512\n",
      "Epoch 643/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -674.5440 - g_loss: 16492.9991\n",
      "Epoch 644/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -688.8595 - g_loss: 16837.1511\n",
      "Epoch 645/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -651.7720 - g_loss: 17597.5475\n",
      "Epoch 646/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -770.9314 - g_loss: 18081.1657\n",
      "Epoch 647/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -678.6518 - g_loss: 17907.9842\n",
      "Epoch 648/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -667.9761 - g_loss: 17655.9796\n",
      "Epoch 649/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -650.2108 - g_loss: 17444.5683\n",
      "Epoch 650/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -672.5224 - g_loss: 17078.0244\n",
      "Epoch 651/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -649.4288 - g_loss: 17355.4265\n",
      "Epoch 652/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -655.2548 - g_loss: 17481.4166\n",
      "Epoch 653/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -616.0273 - g_loss: 18473.0549\n",
      "Epoch 654/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -683.2765 - g_loss: 18433.4877\n",
      "Epoch 655/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -667.2259 - g_loss: 19223.5496\n",
      "Epoch 656/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -681.9070 - g_loss: 18842.8600\n",
      "Epoch 657/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -609.9184 - g_loss: 17284.2545\n",
      "Epoch 658/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -646.4680 - g_loss: 18072.4103\n",
      "Epoch 659/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -671.5790 - g_loss: 18260.1332\n",
      "Epoch 660/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -624.9855 - g_loss: 18244.6207\n",
      "Epoch 661/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -649.4880 - g_loss: 18065.1927\n",
      "Epoch 662/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -658.3365 - g_loss: 19359.8109\n",
      "Epoch 663/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -681.0971 - g_loss: 18698.3643\n",
      "Epoch 664/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -612.4188 - g_loss: 18184.2920\n",
      "Epoch 665/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -612.7503 - g_loss: 18638.0566\n",
      "Epoch 666/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -680.6268 - g_loss: 18946.2533\n",
      "Epoch 667/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -640.9153 - g_loss: 18521.1906\n",
      "Epoch 668/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -675.5560 - g_loss: 18655.9767\n",
      "Epoch 669/2000\n",
      "95/95 [==============================] - 16s 169ms/step - d_loss: -638.5285 - g_loss: 18378.7297\n",
      "Epoch 670/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -663.0010 - g_loss: 18343.7453\n",
      "Epoch 671/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -663.5549 - g_loss: 18450.4933\n",
      "Epoch 672/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -654.7386 - g_loss: 17887.0293\n",
      "Epoch 673/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -587.0808 - g_loss: 17933.1918\n",
      "Epoch 674/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -702.8985 - g_loss: 17485.1445\n",
      "Epoch 675/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -713.1367 - g_loss: 17029.0731\n",
      "Epoch 676/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -703.6243 - g_loss: 16772.8058\n",
      "Epoch 677/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -677.8481 - g_loss: 17567.9867\n",
      "Epoch 678/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -596.8192 - g_loss: 17492.8433\n",
      "Epoch 679/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -634.5751 - g_loss: 17091.3667\n",
      "Epoch 680/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -635.5619 - g_loss: 18132.6932\n",
      "Epoch 681/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -623.5206 - g_loss: 18400.7079\n",
      "Epoch 682/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -583.7162 - g_loss: 18249.5370\n",
      "Epoch 683/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -662.1657 - g_loss: 18319.2993\n",
      "Epoch 684/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -642.2256 - g_loss: 18956.3173\n",
      "Epoch 685/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -708.7769 - g_loss: 18336.3652\n",
      "Epoch 686/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -598.0594 - g_loss: 18129.2136\n",
      "Epoch 687/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -618.6918 - g_loss: 17916.3782\n",
      "Epoch 688/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -602.5258 - g_loss: 18402.9173\n",
      "Epoch 689/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -694.6378 - g_loss: 17964.5942\n",
      "Epoch 690/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -629.3214 - g_loss: 18865.8681\n",
      "Epoch 691/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -666.1340 - g_loss: 19530.5174\n",
      "Epoch 692/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -675.0057 - g_loss: 20208.3511\n",
      "Epoch 693/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -610.7018 - g_loss: 19647.0462\n",
      "Epoch 694/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -603.8654 - g_loss: 19529.7063\n",
      "Epoch 695/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -612.6074 - g_loss: 19910.4602\n",
      "Epoch 696/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -579.5326 - g_loss: 19581.1537\n",
      "Epoch 697/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -713.6030 - g_loss: 19466.9896\n",
      "Epoch 698/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -624.7788 - g_loss: 18445.9046\n",
      "Epoch 699/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -585.4435 - g_loss: 17546.3073\n",
      "Epoch 700/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -623.7595 - g_loss: 17666.2199\n",
      "Epoch 701/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -623.1536 - g_loss: 18470.2005\n",
      "Epoch 702/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -619.2155 - g_loss: 20016.9153\n",
      "Epoch 703/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -663.5248 - g_loss: 19885.0257\n",
      "Epoch 704/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -629.9096 - g_loss: 18839.5844\n",
      "Epoch 705/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -591.6978 - g_loss: 18730.2104\n",
      "Epoch 706/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -637.9893 - g_loss: 18594.9064\n",
      "Epoch 707/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -660.1608 - g_loss: 19679.2958\n",
      "Epoch 708/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -635.9023 - g_loss: 19588.1377\n",
      "Epoch 709/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -601.8015 - g_loss: 18796.5992\n",
      "Epoch 710/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -645.2916 - g_loss: 19007.1237\n",
      "Epoch 711/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -555.1208 - g_loss: 19673.3933\n",
      "Epoch 712/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -720.8120 - g_loss: 20202.4854\n",
      "Epoch 713/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -571.9434 - g_loss: 21250.5534\n",
      "Epoch 714/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -575.4482 - g_loss: 21067.8817\n",
      "Epoch 715/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -648.9524 - g_loss: 19756.0825\n",
      "Epoch 716/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -634.9902 - g_loss: 19569.6203\n",
      "Epoch 717/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -634.3210 - g_loss: 20346.1141\n",
      "Epoch 718/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -654.9248 - g_loss: 21379.0011\n",
      "Epoch 719/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -632.7831 - g_loss: 20962.2485\n",
      "Epoch 720/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -653.5935 - g_loss: 21928.5814\n",
      "Epoch 721/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -606.1452 - g_loss: 21858.9125\n",
      "Epoch 722/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -651.5303 - g_loss: 22008.2137\n",
      "Epoch 723/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -587.9010 - g_loss: 21385.7269\n",
      "Epoch 724/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -677.1875 - g_loss: 21098.6325\n",
      "Epoch 725/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -662.8945 - g_loss: 21776.9005\n",
      "Epoch 726/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -542.1348 - g_loss: 20038.6516\n",
      "Epoch 727/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -612.3167 - g_loss: 21295.8851\n",
      "Epoch 728/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -578.4599 - g_loss: 21408.3416\n",
      "Epoch 729/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -548.7298 - g_loss: 20206.7533\n",
      "Epoch 730/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -636.7010 - g_loss: 20235.0663\n",
      "Epoch 731/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -644.5878 - g_loss: 20106.5643\n",
      "Epoch 732/2000\n",
      "95/95 [==============================] - 12s 131ms/step - d_loss: -604.0123 - g_loss: 21050.7782\n",
      "Epoch 733/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -628.4376 - g_loss: 20531.4079\n",
      "Epoch 734/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -660.7639 - g_loss: 20513.4131\n",
      "Epoch 735/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -622.2642 - g_loss: 20884.1648\n",
      "Epoch 736/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -602.5715 - g_loss: 21440.1390\n",
      "Epoch 737/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -613.7878 - g_loss: 21803.6061\n",
      "Epoch 738/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -718.8820 - g_loss: 22034.7320\n",
      "Epoch 739/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -666.3499 - g_loss: 22058.2878\n",
      "Epoch 740/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -517.2535 - g_loss: 21501.6456\n",
      "Epoch 741/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -562.0398 - g_loss: 21414.2191\n",
      "Epoch 742/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -526.5346 - g_loss: 21795.3083\n",
      "Epoch 743/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -568.6587 - g_loss: 21578.2509\n",
      "Epoch 744/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -637.8347 - g_loss: 22164.2265\n",
      "Epoch 745/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -599.9104 - g_loss: 21588.7082\n",
      "Epoch 746/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -670.4131 - g_loss: 22374.8596\n",
      "Epoch 747/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -581.1091 - g_loss: 22249.0613\n",
      "Epoch 748/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -636.4707 - g_loss: 22354.7775\n",
      "Epoch 749/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -609.7708 - g_loss: 21457.8425\n",
      "Epoch 750/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -621.0689 - g_loss: 20865.2829\n",
      "Epoch 751/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -504.8186 - g_loss: 20180.0601\n",
      "Epoch 752/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -666.5391 - g_loss: 19964.9130\n",
      "Epoch 753/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -530.4203 - g_loss: 20810.9960\n",
      "Epoch 754/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -736.8064 - g_loss: 21587.3680\n",
      "Epoch 755/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -682.0185 - g_loss: 21353.0462\n",
      "Epoch 756/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -701.5981 - g_loss: 20899.7338\n",
      "Epoch 757/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -628.2488 - g_loss: 20801.8754\n",
      "Epoch 758/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -633.4252 - g_loss: 22053.8262\n",
      "Epoch 759/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -623.1287 - g_loss: 21898.9224\n",
      "Epoch 760/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -541.5659 - g_loss: 21386.5561\n",
      "Epoch 761/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -622.5873 - g_loss: 21809.8538\n",
      "Epoch 762/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -574.8821 - g_loss: 21928.6723\n",
      "Epoch 763/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -660.4593 - g_loss: 20920.1235\n",
      "Epoch 764/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -609.7641 - g_loss: 19738.1693\n",
      "Epoch 765/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -669.5813 - g_loss: 20066.4749\n",
      "Epoch 766/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -540.4672 - g_loss: 19759.3954\n",
      "Epoch 767/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -560.5013 - g_loss: 20144.0494\n",
      "Epoch 768/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -606.9433 - g_loss: 19500.6606\n",
      "Epoch 769/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -596.9749 - g_loss: 18474.1221\n",
      "Epoch 770/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -603.2854 - g_loss: 19163.9221\n",
      "Epoch 771/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -546.5285 - g_loss: 19155.7064\n",
      "Epoch 772/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -521.8098 - g_loss: 17999.7524\n",
      "Epoch 773/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -619.0943 - g_loss: 18239.2406\n",
      "Epoch 774/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -599.2800 - g_loss: 17929.1509\n",
      "Epoch 775/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -592.2903 - g_loss: 16586.0505\n",
      "Epoch 776/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -637.2516 - g_loss: 17376.7066\n",
      "Epoch 777/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -600.4464 - g_loss: 18362.6749\n",
      "Epoch 778/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -596.4613 - g_loss: 18834.3113\n",
      "Epoch 779/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -659.6310 - g_loss: 18388.4138\n",
      "Epoch 780/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -590.8349 - g_loss: 17732.8023\n",
      "Epoch 781/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -604.0950 - g_loss: 18669.1980\n",
      "Epoch 782/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -617.3982 - g_loss: 19330.1626\n",
      "Epoch 783/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -610.4037 - g_loss: 19175.1165\n",
      "Epoch 784/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -580.6543 - g_loss: 19079.3297\n",
      "Epoch 785/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -591.7032 - g_loss: 18531.2863\n",
      "Epoch 786/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -571.0472 - g_loss: 18149.0806\n",
      "Epoch 787/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -599.2729 - g_loss: 18913.4559\n",
      "Epoch 788/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -601.7914 - g_loss: 19631.2062\n",
      "Epoch 789/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -573.4412 - g_loss: 19264.9810\n",
      "Epoch 790/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -485.9536 - g_loss: 20191.1271\n",
      "Epoch 791/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -593.7221 - g_loss: 19678.1231\n",
      "Epoch 792/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -573.9841 - g_loss: 19659.1485\n",
      "Epoch 793/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -548.1887 - g_loss: 19729.9546\n",
      "Epoch 794/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -592.5209 - g_loss: 19674.1450\n",
      "Epoch 795/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -544.3768 - g_loss: 20050.2739\n",
      "Epoch 796/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -525.2584 - g_loss: 18951.9368\n",
      "Epoch 797/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -544.8854 - g_loss: 18936.9115\n",
      "Epoch 798/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -558.3499 - g_loss: 18991.7494\n",
      "Epoch 799/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -624.7789 - g_loss: 19814.2179\n",
      "Epoch 800/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -637.2613 - g_loss: 20046.6366\n",
      "Epoch 801/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -607.5794 - g_loss: 19186.2941\n",
      "Epoch 802/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -638.5919 - g_loss: 19180.9238\n",
      "Epoch 803/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -553.1889 - g_loss: 19798.3277\n",
      "Epoch 804/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -600.8177 - g_loss: 20170.1218\n",
      "Epoch 805/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -601.0899 - g_loss: 20501.8585\n",
      "Epoch 806/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -526.6456 - g_loss: 19859.9756\n",
      "Epoch 807/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -661.3012 - g_loss: 20684.5711\n",
      "Epoch 808/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -587.9360 - g_loss: 20197.9838\n",
      "Epoch 809/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -599.2373 - g_loss: 19332.3908\n",
      "Epoch 810/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -541.6176 - g_loss: 20582.1861\n",
      "Epoch 811/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -597.2170 - g_loss: 19258.7249\n",
      "Epoch 812/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -616.5269 - g_loss: 19498.6365\n",
      "Epoch 813/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -563.3299 - g_loss: 20110.2267\n",
      "Epoch 814/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -634.3782 - g_loss: 20315.0280\n",
      "Epoch 815/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -580.7915 - g_loss: 20237.2463\n",
      "Epoch 816/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -613.7806 - g_loss: 20745.3241\n",
      "Epoch 817/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -574.8605 - g_loss: 21122.9536\n",
      "Epoch 818/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -624.8142 - g_loss: 20774.9632\n",
      "Epoch 819/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -609.5761 - g_loss: 20506.1353\n",
      "Epoch 820/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -578.7790 - g_loss: 20692.5030\n",
      "Epoch 821/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -585.9721 - g_loss: 20718.4030\n",
      "Epoch 822/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -558.6163 - g_loss: 21592.5238\n",
      "Epoch 823/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -622.3509 - g_loss: 20898.4121\n",
      "Epoch 824/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -568.1098 - g_loss: 21737.3223\n",
      "Epoch 825/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -591.7425 - g_loss: 20964.1166\n",
      "Epoch 826/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -557.7412 - g_loss: 21532.5979\n",
      "Epoch 827/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -622.7048 - g_loss: 22813.9585\n",
      "Epoch 828/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -607.3168 - g_loss: 22303.6046\n",
      "Epoch 829/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -543.2691 - g_loss: 20773.0644\n",
      "Epoch 830/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -618.5335 - g_loss: 22218.3025\n",
      "Epoch 831/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -602.9089 - g_loss: 21523.6875\n",
      "Epoch 832/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -521.8512 - g_loss: 21138.3835\n",
      "Epoch 833/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -616.2211 - g_loss: 20950.6325\n",
      "Epoch 834/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -581.7520 - g_loss: 20190.2613\n",
      "Epoch 835/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -518.7637 - g_loss: 19577.0258\n",
      "Epoch 836/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -582.1761 - g_loss: 19461.2766\n",
      "Epoch 837/2000\n",
      "95/95 [==============================] - 17s 179ms/step - d_loss: -610.8647 - g_loss: 18892.9511\n",
      "Epoch 838/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -554.2768 - g_loss: 19695.1167\n",
      "Epoch 839/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -639.8988 - g_loss: 19622.5420\n",
      "Epoch 840/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -614.4228 - g_loss: 20118.9959\n",
      "Epoch 841/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -597.9611 - g_loss: 20351.8581\n",
      "Epoch 842/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -484.1382 - g_loss: 20086.2105\n",
      "Epoch 843/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -576.7478 - g_loss: 21053.0596\n",
      "Epoch 844/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -592.7114 - g_loss: 21735.4776\n",
      "Epoch 845/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -580.2792 - g_loss: 22149.5921\n",
      "Epoch 846/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -575.7009 - g_loss: 22635.1314\n",
      "Epoch 847/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -586.6149 - g_loss: 22062.9785\n",
      "Epoch 848/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -624.5096 - g_loss: 21874.9423\n",
      "Epoch 849/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -620.3312 - g_loss: 21946.1119\n",
      "Epoch 850/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -604.9297 - g_loss: 22177.6470\n",
      "Epoch 851/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -616.2227 - g_loss: 21008.9370\n",
      "Epoch 852/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -564.2094 - g_loss: 20396.2395\n",
      "Epoch 853/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -530.0957 - g_loss: 20533.0649\n",
      "Epoch 854/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -613.9113 - g_loss: 21385.3800\n",
      "Epoch 855/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -603.9135 - g_loss: 21133.0117\n",
      "Epoch 856/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -535.2386 - g_loss: 20627.6021\n",
      "Epoch 857/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -588.6761 - g_loss: 20370.4995\n",
      "Epoch 858/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -558.5289 - g_loss: 19387.2571\n",
      "Epoch 859/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -589.9943 - g_loss: 20265.9393\n",
      "Epoch 860/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -584.7948 - g_loss: 20190.5890\n",
      "Epoch 861/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -577.9896 - g_loss: 20559.7047\n",
      "Epoch 862/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -588.9530 - g_loss: 20489.6892\n",
      "Epoch 863/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -590.5521 - g_loss: 20675.0917\n",
      "Epoch 864/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -587.2760 - g_loss: 20010.2339\n",
      "Epoch 865/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -548.8969 - g_loss: 20502.4576\n",
      "Epoch 866/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -594.9600 - g_loss: 20687.1412\n",
      "Epoch 867/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -618.7954 - g_loss: 20166.8274\n",
      "Epoch 868/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -506.9130 - g_loss: 21479.7773\n",
      "Epoch 869/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -499.6503 - g_loss: 22008.4123\n",
      "Epoch 870/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -607.1868 - g_loss: 21235.2766\n",
      "Epoch 871/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -500.7985 - g_loss: 22771.3569\n",
      "Epoch 872/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -537.7868 - g_loss: 23854.3516\n",
      "Epoch 873/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -535.7217 - g_loss: 22759.2872\n",
      "Epoch 874/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -604.7366 - g_loss: 21850.7991\n",
      "Epoch 875/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -564.3125 - g_loss: 21776.0826\n",
      "Epoch 876/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -585.7418 - g_loss: 22024.1163\n",
      "Epoch 877/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -579.3392 - g_loss: 22878.4611\n",
      "Epoch 878/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -570.8766 - g_loss: 22741.9909\n",
      "Epoch 879/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -561.3562 - g_loss: 22915.2915\n",
      "Epoch 880/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -470.1302 - g_loss: 21925.5145\n",
      "Epoch 881/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -543.2272 - g_loss: 21126.9823\n",
      "Epoch 882/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -556.5837 - g_loss: 22019.4271\n",
      "Epoch 883/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -665.1819 - g_loss: 22624.8016\n",
      "Epoch 884/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -609.0548 - g_loss: 22071.7913\n",
      "Epoch 885/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -568.9334 - g_loss: 21451.0505\n",
      "Epoch 886/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -650.8500 - g_loss: 21651.3561\n",
      "Epoch 887/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -500.1655 - g_loss: 20822.0280\n",
      "Epoch 888/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -638.2330 - g_loss: 21135.0008\n",
      "Epoch 889/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -566.5640 - g_loss: 20011.6813\n",
      "Epoch 890/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -615.1781 - g_loss: 19316.4522\n",
      "Epoch 891/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -553.5933 - g_loss: 19491.9651\n",
      "Epoch 892/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -603.3317 - g_loss: 21287.3137\n",
      "Epoch 893/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -530.7795 - g_loss: 21871.9819\n",
      "Epoch 894/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -538.7385 - g_loss: 21003.7474\n",
      "Epoch 895/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -578.1539 - g_loss: 21701.4741\n",
      "Epoch 896/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -583.0623 - g_loss: 21224.4971\n",
      "Epoch 897/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -533.0215 - g_loss: 21852.3573\n",
      "Epoch 898/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -521.8929 - g_loss: 22344.8927\n",
      "Epoch 899/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -452.3323 - g_loss: 21651.2328\n",
      "Epoch 900/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -554.9048 - g_loss: 20667.0232\n",
      "Epoch 901/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -490.4050 - g_loss: 20142.1284\n",
      "Epoch 902/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -527.7102 - g_loss: 19355.2510\n",
      "Epoch 903/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -565.5235 - g_loss: 19736.8529\n",
      "Epoch 904/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -543.1850 - g_loss: 19886.1334\n",
      "Epoch 905/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -560.2406 - g_loss: 20377.1374\n",
      "Epoch 906/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -565.8629 - g_loss: 20128.0750\n",
      "Epoch 907/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -590.0227 - g_loss: 19360.9195\n",
      "Epoch 908/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -576.5202 - g_loss: 19707.4548\n",
      "Epoch 909/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -575.7392 - g_loss: 19333.5903\n",
      "Epoch 910/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -516.2124 - g_loss: 18849.6343\n",
      "Epoch 911/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -518.1608 - g_loss: 19973.9107\n",
      "Epoch 912/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -523.7526 - g_loss: 19767.5158\n",
      "Epoch 913/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -458.2540 - g_loss: 20178.3231\n",
      "Epoch 914/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -498.0721 - g_loss: 18927.7401\n",
      "Epoch 915/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -527.2124 - g_loss: 19343.1298\n",
      "Epoch 916/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -504.7650 - g_loss: 18953.9737\n",
      "Epoch 917/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -524.9962 - g_loss: 20409.1717\n",
      "Epoch 918/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -555.4401 - g_loss: 21015.7967\n",
      "Epoch 919/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -517.4495 - g_loss: 21260.5691\n",
      "Epoch 920/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -611.5612 - g_loss: 20448.0403\n",
      "Epoch 921/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -564.1191 - g_loss: 19504.9084\n",
      "Epoch 922/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -501.2216 - g_loss: 19362.8841\n",
      "Epoch 923/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -588.2182 - g_loss: 19527.6174\n",
      "Epoch 924/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -602.5024 - g_loss: 19844.3434\n",
      "Epoch 925/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -538.1415 - g_loss: 19703.1619\n",
      "Epoch 926/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -625.5892 - g_loss: 20675.8503\n",
      "Epoch 927/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -582.2583 - g_loss: 21226.4379\n",
      "Epoch 928/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -550.6238 - g_loss: 20451.9644\n",
      "Epoch 929/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -605.6094 - g_loss: 19553.6899\n",
      "Epoch 930/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -557.5839 - g_loss: 20116.5588\n",
      "Epoch 931/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -519.5379 - g_loss: 20018.0229\n",
      "Epoch 932/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -523.8942 - g_loss: 18436.7261\n",
      "Epoch 933/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -621.0269 - g_loss: 18557.1080\n",
      "Epoch 934/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -563.4133 - g_loss: 20050.1915\n",
      "Epoch 935/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -596.0971 - g_loss: 19851.7639\n",
      "Epoch 936/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -547.8871 - g_loss: 18557.1179\n",
      "Epoch 937/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -588.8575 - g_loss: 19489.1024\n",
      "Epoch 938/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -531.2310 - g_loss: 19139.8193\n",
      "Epoch 939/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -618.9130 - g_loss: 18790.6534\n",
      "Epoch 940/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -527.4392 - g_loss: 19312.7133\n",
      "Epoch 941/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -567.2582 - g_loss: 19494.7728\n",
      "Epoch 942/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -547.3036 - g_loss: 19557.1671\n",
      "Epoch 943/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -597.7643 - g_loss: 20538.0478\n",
      "Epoch 944/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -581.7718 - g_loss: 20529.6079\n",
      "Epoch 945/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -575.1860 - g_loss: 20855.2466\n",
      "Epoch 946/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -471.3961 - g_loss: 21394.5957\n",
      "Epoch 947/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -587.4045 - g_loss: 21834.4167\n",
      "Epoch 948/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -559.1567 - g_loss: 21616.3819\n",
      "Epoch 949/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -528.4103 - g_loss: 20660.9536\n",
      "Epoch 950/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -625.5807 - g_loss: 20553.1129\n",
      "Epoch 951/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -505.5562 - g_loss: 20822.3213\n",
      "Epoch 952/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -584.6566 - g_loss: 20719.3517\n",
      "Epoch 953/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -495.3598 - g_loss: 20608.4510\n",
      "Epoch 954/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -483.9140 - g_loss: 20197.2421\n",
      "Epoch 955/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -587.4750 - g_loss: 19208.9604\n",
      "Epoch 956/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -581.0571 - g_loss: 19885.5668\n",
      "Epoch 957/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -593.2492 - g_loss: 20794.7763\n",
      "Epoch 958/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -534.7723 - g_loss: 20725.5920\n",
      "Epoch 959/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -606.6188 - g_loss: 20394.2514\n",
      "Epoch 960/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -520.7694 - g_loss: 20847.3236\n",
      "Epoch 961/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -589.1278 - g_loss: 20269.2871\n",
      "Epoch 962/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -594.0866 - g_loss: 20994.5422\n",
      "Epoch 963/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -566.2284 - g_loss: 22283.7067\n",
      "Epoch 964/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -513.6308 - g_loss: 21772.5898\n",
      "Epoch 965/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -520.3061 - g_loss: 20907.7537\n",
      "Epoch 966/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -512.2469 - g_loss: 20765.8851\n",
      "Epoch 967/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -627.9453 - g_loss: 21226.4435\n",
      "Epoch 968/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -519.0110 - g_loss: 21644.9276\n",
      "Epoch 969/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -496.2132 - g_loss: 20257.0835\n",
      "Epoch 970/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -475.6863 - g_loss: 20334.4638\n",
      "Epoch 971/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -618.2564 - g_loss: 21494.3447\n",
      "Epoch 972/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -487.4078 - g_loss: 20698.8912\n",
      "Epoch 973/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -546.0063 - g_loss: 20540.4318\n",
      "Epoch 974/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -539.6621 - g_loss: 21186.1966\n",
      "Epoch 975/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -549.1435 - g_loss: 22422.5377\n",
      "Epoch 976/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -646.1208 - g_loss: 21709.0519\n",
      "Epoch 977/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -494.5290 - g_loss: 21761.7822\n",
      "Epoch 978/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -577.4044 - g_loss: 22106.4330\n",
      "Epoch 979/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -536.4808 - g_loss: 22352.0145\n",
      "Epoch 980/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -494.2768 - g_loss: 22257.0708\n",
      "Epoch 981/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -544.2309 - g_loss: 21593.0518\n",
      "Epoch 982/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -539.1930 - g_loss: 21432.0288\n",
      "Epoch 983/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -472.7519 - g_loss: 20821.1565\n",
      "Epoch 984/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -542.2909 - g_loss: 20672.9480\n",
      "Epoch 985/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -482.8366 - g_loss: 21114.7209\n",
      "Epoch 986/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -473.4085 - g_loss: 20529.6145\n",
      "Epoch 987/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -472.4316 - g_loss: 19503.2202\n",
      "Epoch 988/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -488.1249 - g_loss: 19290.4150\n",
      "Epoch 989/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -561.3369 - g_loss: 19759.0640\n",
      "Epoch 990/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -562.7923 - g_loss: 20386.7203\n",
      "Epoch 991/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -534.9072 - g_loss: 20367.2888\n",
      "Epoch 992/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -555.9684 - g_loss: 20948.3541\n",
      "Epoch 993/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -537.5577 - g_loss: 20438.3599\n",
      "Epoch 994/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -543.7061 - g_loss: 20596.7459\n",
      "Epoch 995/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -518.3281 - g_loss: 21136.8484\n",
      "Epoch 996/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -519.9912 - g_loss: 22239.9989\n",
      "Epoch 997/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -567.4621 - g_loss: 21582.9298\n",
      "Epoch 998/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -552.5409 - g_loss: 21413.1324\n",
      "Epoch 999/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -600.8909 - g_loss: 20413.8354\n",
      "Epoch 1000/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -535.8800 - g_loss: 20543.6801\n",
      "Epoch 1001/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -539.3526 - g_loss: 20730.6907\n",
      "Epoch 1002/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -529.7562 - g_loss: 19841.6389\n",
      "Epoch 1003/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -561.7141 - g_loss: 19471.6590\n",
      "Epoch 1004/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -577.3908 - g_loss: 20116.2750\n",
      "Epoch 1005/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -528.5851 - g_loss: 20349.5559\n",
      "Epoch 1006/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -524.7339 - g_loss: 19508.8627\n",
      "Epoch 1007/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -534.2495 - g_loss: 20240.4111\n",
      "Epoch 1008/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -553.3705 - g_loss: 20420.3607\n",
      "Epoch 1009/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -563.0525 - g_loss: 20004.6324\n",
      "Epoch 1010/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -628.4183 - g_loss: 20312.4709\n",
      "Epoch 1011/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -542.2055 - g_loss: 19902.5179\n",
      "Epoch 1012/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -590.8540 - g_loss: 19602.2907\n",
      "Epoch 1013/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -536.4993 - g_loss: 19723.0551\n",
      "Epoch 1014/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -617.2381 - g_loss: 19785.6547\n",
      "Epoch 1015/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -543.6565 - g_loss: 20234.7430\n",
      "Epoch 1016/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -536.8936 - g_loss: 19610.5265\n",
      "Epoch 1017/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -568.6522 - g_loss: 20890.4622\n",
      "Epoch 1018/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -579.3400 - g_loss: 20968.5507\n",
      "Epoch 1019/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -538.5707 - g_loss: 20966.6785\n",
      "Epoch 1020/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -483.3730 - g_loss: 20022.4847\n",
      "Epoch 1021/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -538.1028 - g_loss: 20216.9607\n",
      "Epoch 1022/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -500.5272 - g_loss: 19680.9173\n",
      "Epoch 1023/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -571.1094 - g_loss: 19486.0770\n",
      "Epoch 1024/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -566.9715 - g_loss: 19588.6871\n",
      "Epoch 1025/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -616.0751 - g_loss: 19786.9571\n",
      "Epoch 1026/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -537.1970 - g_loss: 19212.5833\n",
      "Epoch 1027/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -574.5248 - g_loss: 19331.6399\n",
      "Epoch 1028/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -591.7996 - g_loss: 20413.9327\n",
      "Epoch 1029/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -532.7260 - g_loss: 20223.1173\n",
      "Epoch 1030/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -549.2210 - g_loss: 21203.5642\n",
      "Epoch 1031/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -533.8468 - g_loss: 21647.0392\n",
      "Epoch 1032/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -558.1078 - g_loss: 20410.9982\n",
      "Epoch 1033/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -526.8865 - g_loss: 20129.8130\n",
      "Epoch 1034/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -542.2791 - g_loss: 20711.0796\n",
      "Epoch 1035/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -591.2183 - g_loss: 20631.0577\n",
      "Epoch 1036/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -608.0304 - g_loss: 20256.1847\n",
      "Epoch 1037/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -474.6280 - g_loss: 20232.6860\n",
      "Epoch 1038/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -580.3660 - g_loss: 20146.1603\n",
      "Epoch 1039/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -572.8778 - g_loss: 19976.1768\n",
      "Epoch 1040/2000\n",
      "95/95 [==============================] - 18s 187ms/step - d_loss: -615.2046 - g_loss: 19948.0546\n",
      "Epoch 1041/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -557.8753 - g_loss: 20917.8984\n",
      "Epoch 1042/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -577.2395 - g_loss: 21732.5554\n",
      "Epoch 1043/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -583.7249 - g_loss: 21637.2182\n",
      "Epoch 1044/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -555.7016 - g_loss: 20696.2709\n",
      "Epoch 1045/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -536.4840 - g_loss: 19501.7647\n",
      "Epoch 1046/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -618.7974 - g_loss: 19929.8051\n",
      "Epoch 1047/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -579.2032 - g_loss: 20691.8155\n",
      "Epoch 1048/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -565.4158 - g_loss: 19717.4240\n",
      "Epoch 1049/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -509.9738 - g_loss: 18847.6496\n",
      "Epoch 1050/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -580.1974 - g_loss: 19760.8752\n",
      "Epoch 1051/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -612.7991 - g_loss: 19711.8578\n",
      "Epoch 1052/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -532.8204 - g_loss: 19918.4382\n",
      "Epoch 1053/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -578.4042 - g_loss: 19295.9238\n",
      "Epoch 1054/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -604.7200 - g_loss: 18189.8831\n",
      "Epoch 1055/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -528.0134 - g_loss: 19144.8643\n",
      "Epoch 1056/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -624.5057 - g_loss: 18665.2462\n",
      "Epoch 1057/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -626.3359 - g_loss: 19357.0275\n",
      "Epoch 1058/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -535.3915 - g_loss: 19479.1443\n",
      "Epoch 1059/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -576.7406 - g_loss: 19517.9955\n",
      "Epoch 1060/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -583.2684 - g_loss: 18940.3780\n",
      "Epoch 1061/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -500.5393 - g_loss: 19947.0142\n",
      "Epoch 1062/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -614.2455 - g_loss: 19086.6322\n",
      "Epoch 1063/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -589.2141 - g_loss: 18727.6522\n",
      "Epoch 1064/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -545.4339 - g_loss: 19063.1296\n",
      "Epoch 1065/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -597.5555 - g_loss: 20165.4614\n",
      "Epoch 1066/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -577.0116 - g_loss: 20101.3466\n",
      "Epoch 1067/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -557.6225 - g_loss: 19407.5803\n",
      "Epoch 1068/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -556.5957 - g_loss: 18644.7857\n",
      "Epoch 1069/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -600.2114 - g_loss: 17299.8446\n",
      "Epoch 1070/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -613.3131 - g_loss: 17682.3429\n",
      "Epoch 1071/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -637.1344 - g_loss: 17166.2272\n",
      "Epoch 1072/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -627.2672 - g_loss: 17923.8691\n",
      "Epoch 1073/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -638.5659 - g_loss: 18294.9486\n",
      "Epoch 1074/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -557.6039 - g_loss: 18245.0797\n",
      "Epoch 1075/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -543.2130 - g_loss: 18480.7909\n",
      "Epoch 1076/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -567.0384 - g_loss: 18180.6021\n",
      "Epoch 1077/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -604.5838 - g_loss: 17809.7789\n",
      "Epoch 1078/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -552.2466 - g_loss: 18781.0660\n",
      "Epoch 1079/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -600.4144 - g_loss: 18244.6208\n",
      "Epoch 1080/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -579.3163 - g_loss: 17551.1068\n",
      "Epoch 1081/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -587.5180 - g_loss: 18589.7778\n",
      "Epoch 1082/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -562.2150 - g_loss: 17895.4819\n",
      "Epoch 1083/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -547.3444 - g_loss: 18421.8869\n",
      "Epoch 1084/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -636.5286 - g_loss: 18691.0442\n",
      "Epoch 1085/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -560.1046 - g_loss: 18244.5805\n",
      "Epoch 1086/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -649.0897 - g_loss: 17812.3042\n",
      "Epoch 1087/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -635.0787 - g_loss: 18466.3257\n",
      "Epoch 1088/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -585.6297 - g_loss: 18744.7885\n",
      "Epoch 1089/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -573.5813 - g_loss: 18524.4512\n",
      "Epoch 1090/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -591.9953 - g_loss: 17924.5404\n",
      "Epoch 1091/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -597.3554 - g_loss: 17498.2198\n",
      "Epoch 1092/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -566.3743 - g_loss: 17927.1083\n",
      "Epoch 1093/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -566.0471 - g_loss: 18332.6326\n",
      "Epoch 1094/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -590.3460 - g_loss: 18857.9717\n",
      "Epoch 1095/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -627.6350 - g_loss: 18832.9127\n",
      "Epoch 1096/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -549.1078 - g_loss: 18942.1751\n",
      "Epoch 1097/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -658.6241 - g_loss: 19534.6197\n",
      "Epoch 1098/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -603.4946 - g_loss: 19482.2300\n",
      "Epoch 1099/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -577.9921 - g_loss: 20100.2713\n",
      "Epoch 1100/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -622.4551 - g_loss: 20284.9990\n",
      "Epoch 1101/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -592.4184 - g_loss: 19550.2212\n",
      "Epoch 1102/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -618.6611 - g_loss: 19833.4013\n",
      "Epoch 1103/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -589.6190 - g_loss: 19116.3640\n",
      "Epoch 1104/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -584.0980 - g_loss: 19880.4863\n",
      "Epoch 1105/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -573.8662 - g_loss: 20041.8183\n",
      "Epoch 1106/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -606.4969 - g_loss: 20340.3968\n",
      "Epoch 1107/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -619.7302 - g_loss: 19969.3481\n",
      "Epoch 1108/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -592.4222 - g_loss: 19238.8314\n",
      "Epoch 1109/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -565.4006 - g_loss: 18452.1658\n",
      "Epoch 1110/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -595.3684 - g_loss: 18973.7193\n",
      "Epoch 1111/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -573.5238 - g_loss: 19253.1366\n",
      "Epoch 1112/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -615.0350 - g_loss: 19810.7821\n",
      "Epoch 1113/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -616.7469 - g_loss: 19391.0851\n",
      "Epoch 1114/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -647.9130 - g_loss: 19212.1717\n",
      "Epoch 1115/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -578.1000 - g_loss: 19484.1852\n",
      "Epoch 1116/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -594.5213 - g_loss: 19043.3415\n",
      "Epoch 1117/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -579.5799 - g_loss: 18873.2672\n",
      "Epoch 1118/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -578.7552 - g_loss: 19135.6740\n",
      "Epoch 1119/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -618.5817 - g_loss: 19109.4562\n",
      "Epoch 1120/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -601.3462 - g_loss: 18997.9942\n",
      "Epoch 1121/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -573.0826 - g_loss: 19326.3140\n",
      "Epoch 1122/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -564.2853 - g_loss: 18840.2338\n",
      "Epoch 1123/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -598.0466 - g_loss: 18061.8039\n",
      "Epoch 1124/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -539.1927 - g_loss: 18114.1538\n",
      "Epoch 1125/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -604.2238 - g_loss: 18510.3425\n",
      "Epoch 1126/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -640.6787 - g_loss: 18597.1856\n",
      "Epoch 1127/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -641.3323 - g_loss: 19559.3116\n",
      "Epoch 1128/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -635.0587 - g_loss: 20025.0256\n",
      "Epoch 1129/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -618.0707 - g_loss: 19999.6992\n",
      "Epoch 1130/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -602.6524 - g_loss: 19080.0481\n",
      "Epoch 1131/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -608.9646 - g_loss: 19717.6409\n",
      "Epoch 1132/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -605.0933 - g_loss: 20099.8869\n",
      "Epoch 1133/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -651.5035 - g_loss: 20087.8534\n",
      "Epoch 1134/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -591.1150 - g_loss: 20838.7782\n",
      "Epoch 1135/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -552.9249 - g_loss: 20519.2729\n",
      "Epoch 1136/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -619.8584 - g_loss: 20852.5801\n",
      "Epoch 1137/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -641.0576 - g_loss: 20401.4318\n",
      "Epoch 1138/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -547.6878 - g_loss: 20761.2082\n",
      "Epoch 1139/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -590.3824 - g_loss: 20802.1740\n",
      "Epoch 1140/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -579.3911 - g_loss: 20484.8541\n",
      "Epoch 1141/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -490.1773 - g_loss: 20532.5484\n",
      "Epoch 1142/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -641.6041 - g_loss: 20811.1627\n",
      "Epoch 1143/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -592.6955 - g_loss: 20940.3154\n",
      "Epoch 1144/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -633.1942 - g_loss: 20724.3836\n",
      "Epoch 1145/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -595.6366 - g_loss: 20704.6289\n",
      "Epoch 1146/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -620.6353 - g_loss: 20605.5312\n",
      "Epoch 1147/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -567.5446 - g_loss: 19535.1877\n",
      "Epoch 1148/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -532.0730 - g_loss: 19338.4321\n",
      "Epoch 1149/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -585.2510 - g_loss: 20719.3342\n",
      "Epoch 1150/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -593.8617 - g_loss: 20203.2008\n",
      "Epoch 1151/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -667.4682 - g_loss: 19654.8842\n",
      "Epoch 1152/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -628.1128 - g_loss: 19227.3624\n",
      "Epoch 1153/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -611.4869 - g_loss: 19546.9560\n",
      "Epoch 1154/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -589.6487 - g_loss: 20227.0623\n",
      "Epoch 1155/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -535.6224 - g_loss: 19882.1937\n",
      "Epoch 1156/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -583.2823 - g_loss: 19307.5650\n",
      "Epoch 1157/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -551.2680 - g_loss: 19253.6413\n",
      "Epoch 1158/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -586.5459 - g_loss: 18898.7400\n",
      "Epoch 1159/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -577.5988 - g_loss: 18745.1323\n",
      "Epoch 1160/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -544.1227 - g_loss: 18553.7894\n",
      "Epoch 1161/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -583.5294 - g_loss: 18899.4655\n",
      "Epoch 1162/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -618.9215 - g_loss: 19323.9462\n",
      "Epoch 1163/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -614.0403 - g_loss: 19210.7551\n",
      "Epoch 1164/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -576.8764 - g_loss: 19009.4712\n",
      "Epoch 1165/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -560.8864 - g_loss: 19278.6068\n",
      "Epoch 1166/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -493.9220 - g_loss: 19207.0919\n",
      "Epoch 1167/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -646.7794 - g_loss: 18803.8798\n",
      "Epoch 1168/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -637.3517 - g_loss: 19791.6177\n",
      "Epoch 1169/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -552.0764 - g_loss: 19738.5087\n",
      "Epoch 1170/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -625.4332 - g_loss: 19410.0466\n",
      "Epoch 1171/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -597.5001 - g_loss: 19269.8355\n",
      "Epoch 1172/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -598.1082 - g_loss: 19764.9850\n",
      "Epoch 1173/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -617.1552 - g_loss: 20089.2161\n",
      "Epoch 1174/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -614.6418 - g_loss: 19576.8811\n",
      "Epoch 1175/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -626.0102 - g_loss: 19497.3921\n",
      "Epoch 1176/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -547.9092 - g_loss: 19121.3410\n",
      "Epoch 1177/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -588.7763 - g_loss: 19321.6401\n",
      "Epoch 1178/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -578.1983 - g_loss: 18430.8064\n",
      "Epoch 1179/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -608.0002 - g_loss: 18869.9199\n",
      "Epoch 1180/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -565.5816 - g_loss: 18983.4101\n",
      "Epoch 1181/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -598.5806 - g_loss: 18742.6575\n",
      "Epoch 1182/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -516.4055 - g_loss: 19033.0400\n",
      "Epoch 1183/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -544.6163 - g_loss: 19202.6050\n",
      "Epoch 1184/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -586.9620 - g_loss: 19568.3960\n",
      "Epoch 1185/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -593.5834 - g_loss: 18568.9234\n",
      "Epoch 1186/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -584.6337 - g_loss: 18608.6233\n",
      "Epoch 1187/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -612.4250 - g_loss: 18895.4792\n",
      "Epoch 1188/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -603.5142 - g_loss: 19495.5748\n",
      "Epoch 1189/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -643.3150 - g_loss: 19060.9308\n",
      "Epoch 1190/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -555.6552 - g_loss: 19661.8987\n",
      "Epoch 1191/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -583.4673 - g_loss: 19801.2971\n",
      "Epoch 1192/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -603.3385 - g_loss: 19009.7019\n",
      "Epoch 1193/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -636.0925 - g_loss: 19277.4313\n",
      "Epoch 1194/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -612.7847 - g_loss: 19278.6643\n",
      "Epoch 1195/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -598.7063 - g_loss: 19517.0727\n",
      "Epoch 1196/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -597.2601 - g_loss: 18740.5726\n",
      "Epoch 1197/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -639.8971 - g_loss: 19628.4658\n",
      "Epoch 1198/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -569.1231 - g_loss: 19550.8720\n",
      "Epoch 1199/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -596.2250 - g_loss: 18858.1492\n",
      "Epoch 1200/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -580.2420 - g_loss: 18960.5857\n",
      "Epoch 1201/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -621.3995 - g_loss: 18837.9227\n",
      "Epoch 1202/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -545.7988 - g_loss: 18670.4179\n",
      "Epoch 1203/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -591.9759 - g_loss: 18903.3637\n",
      "Epoch 1204/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -628.9435 - g_loss: 18836.5440\n",
      "Epoch 1205/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -559.7939 - g_loss: 18974.4776\n",
      "Epoch 1206/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -586.5428 - g_loss: 19244.8690\n",
      "Epoch 1207/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -640.9416 - g_loss: 18239.2902\n",
      "Epoch 1208/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -564.8717 - g_loss: 18825.4411\n",
      "Epoch 1209/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -601.0382 - g_loss: 18606.3169\n",
      "Epoch 1210/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -594.1886 - g_loss: 19404.4098\n",
      "Epoch 1211/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -675.6553 - g_loss: 18627.4620\n",
      "Epoch 1212/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -623.9329 - g_loss: 19164.3128\n",
      "Epoch 1213/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -603.4233 - g_loss: 18713.8459\n",
      "Epoch 1214/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -647.8290 - g_loss: 18237.3009\n",
      "Epoch 1215/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -564.9958 - g_loss: 18446.7975\n",
      "Epoch 1216/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -572.6474 - g_loss: 18419.6267\n",
      "Epoch 1217/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -639.7642 - g_loss: 17945.8927\n",
      "Epoch 1218/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -625.7030 - g_loss: 18198.7983\n",
      "Epoch 1219/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -582.6464 - g_loss: 18944.7537\n",
      "Epoch 1220/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -614.6354 - g_loss: 18102.6805\n",
      "Epoch 1221/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -614.2259 - g_loss: 18440.2381\n",
      "Epoch 1222/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -641.1332 - g_loss: 19224.1487\n",
      "Epoch 1223/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -557.7879 - g_loss: 19307.6079\n",
      "Epoch 1224/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -617.8423 - g_loss: 19100.9288\n",
      "Epoch 1225/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -600.2002 - g_loss: 19039.1722\n",
      "Epoch 1226/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -674.7545 - g_loss: 19078.3965\n",
      "Epoch 1227/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -610.6674 - g_loss: 19006.1354\n",
      "Epoch 1228/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -613.7042 - g_loss: 19098.5663\n",
      "Epoch 1229/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -620.0336 - g_loss: 19274.4980\n",
      "Epoch 1230/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -626.2384 - g_loss: 18556.3308\n",
      "Epoch 1231/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -641.2837 - g_loss: 18234.9933\n",
      "Epoch 1232/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -604.3819 - g_loss: 18414.1361\n",
      "Epoch 1233/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -555.6507 - g_loss: 17981.6075\n",
      "Epoch 1234/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -581.9446 - g_loss: 17305.4353\n",
      "Epoch 1235/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -640.7560 - g_loss: 17657.1299\n",
      "Epoch 1236/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -620.9048 - g_loss: 18026.4563\n",
      "Epoch 1237/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -619.5533 - g_loss: 18047.7341\n",
      "Epoch 1238/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -574.6127 - g_loss: 18452.1375\n",
      "Epoch 1239/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -562.0074 - g_loss: 17366.9678\n",
      "Epoch 1240/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -578.9760 - g_loss: 17045.9706\n",
      "Epoch 1241/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -619.0077 - g_loss: 17227.8753\n",
      "Epoch 1242/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -606.5096 - g_loss: 17569.0158\n",
      "Epoch 1243/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -621.2626 - g_loss: 17447.2154\n",
      "Epoch 1244/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -622.8364 - g_loss: 17210.7631\n",
      "Epoch 1245/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -609.3279 - g_loss: 17772.4141\n",
      "Epoch 1246/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -609.6085 - g_loss: 17348.3950\n",
      "Epoch 1247/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -628.8624 - g_loss: 16939.1909\n",
      "Epoch 1248/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -575.8876 - g_loss: 16019.6285\n",
      "Epoch 1249/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -686.9995 - g_loss: 16302.1427\n",
      "Epoch 1250/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -647.0451 - g_loss: 16165.8990\n",
      "Epoch 1251/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -576.9312 - g_loss: 15892.9316\n",
      "Epoch 1252/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -658.3732 - g_loss: 16573.8249\n",
      "Epoch 1253/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -587.0462 - g_loss: 16773.4938\n",
      "Epoch 1254/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -660.7568 - g_loss: 16847.9358\n",
      "Epoch 1255/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -600.3723 - g_loss: 16366.3496\n",
      "Epoch 1256/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -646.5172 - g_loss: 16614.7284\n",
      "Epoch 1257/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -630.5094 - g_loss: 17010.0391\n",
      "Epoch 1258/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -630.2978 - g_loss: 17117.6406\n",
      "Epoch 1259/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -575.0371 - g_loss: 16555.2505\n",
      "Epoch 1260/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -633.6808 - g_loss: 16149.1149\n",
      "Epoch 1261/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -652.1836 - g_loss: 16128.1071\n",
      "Epoch 1262/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -578.8340 - g_loss: 16080.7662\n",
      "Epoch 1263/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -568.9126 - g_loss: 16172.5861\n",
      "Epoch 1264/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -545.0167 - g_loss: 16016.0542\n",
      "Epoch 1265/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -644.6530 - g_loss: 15489.1403\n",
      "Epoch 1266/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -629.4709 - g_loss: 15635.4026\n",
      "Epoch 1267/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -595.0852 - g_loss: 15281.1956\n",
      "Epoch 1268/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -624.0614 - g_loss: 15656.6220\n",
      "Epoch 1269/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -579.0503 - g_loss: 16184.5830\n",
      "Epoch 1270/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -557.2328 - g_loss: 15869.1630\n",
      "Epoch 1271/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -623.1371 - g_loss: 16278.6396\n",
      "Epoch 1272/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -623.9344 - g_loss: 16799.0820\n",
      "Epoch 1273/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -615.3603 - g_loss: 16875.1606\n",
      "Epoch 1274/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -594.8454 - g_loss: 16961.5744\n",
      "Epoch 1275/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -578.4194 - g_loss: 16681.2763\n",
      "Epoch 1276/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -605.2402 - g_loss: 17066.3478\n",
      "Epoch 1277/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -676.3172 - g_loss: 17165.2950\n",
      "Epoch 1278/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -600.5552 - g_loss: 17471.3728\n",
      "Epoch 1279/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -607.8061 - g_loss: 16521.6500\n",
      "Epoch 1280/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -622.5184 - g_loss: 16085.0345\n",
      "Epoch 1281/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -590.0818 - g_loss: 16163.2581\n",
      "Epoch 1282/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -626.2811 - g_loss: 15978.0095\n",
      "Epoch 1283/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -610.8803 - g_loss: 15034.1592\n",
      "Epoch 1284/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -601.3302 - g_loss: 15030.0586\n",
      "Epoch 1285/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -618.9036 - g_loss: 14991.2976\n",
      "Epoch 1286/2000\n",
      "95/95 [==============================] - 19s 200ms/step - d_loss: -639.7405 - g_loss: 15042.7715\n",
      "Epoch 1287/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -608.0469 - g_loss: 14975.4415\n",
      "Epoch 1288/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -612.0555 - g_loss: 14940.3884\n",
      "Epoch 1289/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -555.7030 - g_loss: 15183.9433\n",
      "Epoch 1290/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -689.5745 - g_loss: 15445.6826\n",
      "Epoch 1291/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -647.0438 - g_loss: 15562.6540\n",
      "Epoch 1292/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -608.9880 - g_loss: 14995.7665\n",
      "Epoch 1293/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -681.9046 - g_loss: 15303.8569\n",
      "Epoch 1294/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -627.0651 - g_loss: 15349.6157\n",
      "Epoch 1295/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -615.9819 - g_loss: 15662.7560\n",
      "Epoch 1296/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -639.3943 - g_loss: 15411.5089\n",
      "Epoch 1297/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -638.7579 - g_loss: 15793.0672\n",
      "Epoch 1298/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -637.2892 - g_loss: 15852.1346\n",
      "Epoch 1299/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -594.6294 - g_loss: 16281.9422\n",
      "Epoch 1300/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -644.9681 - g_loss: 16212.9504\n",
      "Epoch 1301/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -636.0496 - g_loss: 15868.5807\n",
      "Epoch 1302/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -646.3191 - g_loss: 15647.5842\n",
      "Epoch 1303/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -626.3140 - g_loss: 15006.0775\n",
      "Epoch 1304/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -610.4677 - g_loss: 14934.4275\n",
      "Epoch 1305/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -604.7180 - g_loss: 14815.4285\n",
      "Epoch 1306/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -590.0604 - g_loss: 14787.6196\n",
      "Epoch 1307/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -631.4384 - g_loss: 14328.7125\n",
      "Epoch 1308/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -684.8943 - g_loss: 14073.3303\n",
      "Epoch 1309/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -594.2326 - g_loss: 13991.1459\n",
      "Epoch 1310/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -607.5999 - g_loss: 13805.1834\n",
      "Epoch 1311/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -645.8787 - g_loss: 13006.1738\n",
      "Epoch 1312/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -633.7295 - g_loss: 13359.5911\n",
      "Epoch 1313/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -565.8242 - g_loss: 13660.2841\n",
      "Epoch 1314/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -584.3349 - g_loss: 14268.2184\n",
      "Epoch 1315/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -618.6627 - g_loss: 14172.0403\n",
      "Epoch 1316/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -642.2580 - g_loss: 14256.8381\n",
      "Epoch 1317/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -622.3191 - g_loss: 13922.7821\n",
      "Epoch 1318/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -594.4217 - g_loss: 13436.3522\n",
      "Epoch 1319/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -635.5337 - g_loss: 13505.8510\n",
      "Epoch 1320/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -660.1561 - g_loss: 13026.8500\n",
      "Epoch 1321/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -667.3373 - g_loss: 12719.8333\n",
      "Epoch 1322/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -631.3724 - g_loss: 13027.6186\n",
      "Epoch 1323/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -603.2862 - g_loss: 13223.9991\n",
      "Epoch 1324/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -645.4317 - g_loss: 13193.2942\n",
      "Epoch 1325/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -662.9879 - g_loss: 13689.8654\n",
      "Epoch 1326/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -616.4519 - g_loss: 13012.8031\n",
      "Epoch 1327/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -604.9857 - g_loss: 13209.2975\n",
      "Epoch 1328/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -610.6851 - g_loss: 12685.1054\n",
      "Epoch 1329/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -650.8138 - g_loss: 13008.3697\n",
      "Epoch 1330/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -640.5419 - g_loss: 12315.4346\n",
      "Epoch 1331/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -648.0860 - g_loss: 11858.1422\n",
      "Epoch 1332/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -658.1342 - g_loss: 12768.8112\n",
      "Epoch 1333/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -609.3418 - g_loss: 12213.3340\n",
      "Epoch 1334/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -618.3881 - g_loss: 12346.1491\n",
      "Epoch 1335/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -656.9106 - g_loss: 12745.5386\n",
      "Epoch 1336/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -671.9734 - g_loss: 12519.0321\n",
      "Epoch 1337/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -602.9519 - g_loss: 12655.0257\n",
      "Epoch 1338/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -581.7568 - g_loss: 12589.5162\n",
      "Epoch 1339/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -616.5584 - g_loss: 12395.0580\n",
      "Epoch 1340/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -581.3315 - g_loss: 12847.6209\n",
      "Epoch 1341/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -612.5640 - g_loss: 12628.4948\n",
      "Epoch 1342/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -589.4975 - g_loss: 11667.0606\n",
      "Epoch 1343/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -583.0339 - g_loss: 11563.2033\n",
      "Epoch 1344/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -599.6572 - g_loss: 11670.2093\n",
      "Epoch 1345/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -609.7958 - g_loss: 12358.4379\n",
      "Epoch 1346/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -590.5982 - g_loss: 12420.3290\n",
      "Epoch 1347/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -645.9910 - g_loss: 12833.5171\n",
      "Epoch 1348/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -626.9105 - g_loss: 12927.8704\n",
      "Epoch 1349/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -667.8187 - g_loss: 13368.2633\n",
      "Epoch 1350/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -592.2821 - g_loss: 13028.4305\n",
      "Epoch 1351/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -610.5842 - g_loss: 12817.0992\n",
      "Epoch 1352/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -575.0135 - g_loss: 12896.9213\n",
      "Epoch 1353/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -641.6440 - g_loss: 12921.0137\n",
      "Epoch 1354/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -616.8817 - g_loss: 12735.0787\n",
      "Epoch 1355/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -619.4161 - g_loss: 12140.0595\n",
      "Epoch 1356/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -570.7976 - g_loss: 12091.3742\n",
      "Epoch 1357/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -586.3106 - g_loss: 12449.5066\n",
      "Epoch 1358/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -576.4501 - g_loss: 12647.6767\n",
      "Epoch 1359/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -627.2843 - g_loss: 12328.2724\n",
      "Epoch 1360/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -611.0780 - g_loss: 12408.4340\n",
      "Epoch 1361/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -615.9966 - g_loss: 12675.1460\n",
      "Epoch 1362/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -588.8962 - g_loss: 12598.5600\n",
      "Epoch 1363/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -580.3895 - g_loss: 12170.9081\n",
      "Epoch 1364/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -625.6755 - g_loss: 12077.2647\n",
      "Epoch 1365/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -613.4247 - g_loss: 12224.5306\n",
      "Epoch 1366/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -636.4365 - g_loss: 12105.7301\n",
      "Epoch 1367/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -612.6197 - g_loss: 12261.5868\n",
      "Epoch 1368/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -610.8556 - g_loss: 11341.5010\n",
      "Epoch 1369/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -639.3294 - g_loss: 11843.2705\n",
      "Epoch 1370/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -598.9372 - g_loss: 11641.9515\n",
      "Epoch 1371/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -636.2018 - g_loss: 11861.1128\n",
      "Epoch 1372/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -556.1858 - g_loss: 11771.0063\n",
      "Epoch 1373/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -557.5485 - g_loss: 11420.5824\n",
      "Epoch 1374/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -622.0041 - g_loss: 11434.7682\n",
      "Epoch 1375/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -662.2385 - g_loss: 11167.7267\n",
      "Epoch 1376/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -626.1144 - g_loss: 11622.6986\n",
      "Epoch 1377/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -606.0584 - g_loss: 12140.1935\n",
      "Epoch 1378/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -620.1107 - g_loss: 12063.8729\n",
      "Epoch 1379/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -606.2232 - g_loss: 12432.7940\n",
      "Epoch 1380/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -609.8772 - g_loss: 12179.5330\n",
      "Epoch 1381/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -618.2761 - g_loss: 12241.3329\n",
      "Epoch 1382/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -632.5742 - g_loss: 12115.8485\n",
      "Epoch 1383/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -606.3181 - g_loss: 11623.0512\n",
      "Epoch 1384/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -588.8986 - g_loss: 11518.1753\n",
      "Epoch 1385/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -627.0053 - g_loss: 11908.6579\n",
      "Epoch 1386/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -658.1578 - g_loss: 11462.2273\n",
      "Epoch 1387/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -596.1391 - g_loss: 11170.2963\n",
      "Epoch 1388/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -577.6170 - g_loss: 11027.8173\n",
      "Epoch 1389/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -630.3124 - g_loss: 10997.9800\n",
      "Epoch 1390/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -630.5073 - g_loss: 10930.1625\n",
      "Epoch 1391/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -633.5134 - g_loss: 10675.4229\n",
      "Epoch 1392/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -608.0970 - g_loss: 11267.8047\n",
      "Epoch 1393/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -633.6849 - g_loss: 11166.6901\n",
      "Epoch 1394/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -582.8209 - g_loss: 11682.0444\n",
      "Epoch 1395/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -576.5603 - g_loss: 11813.4657\n",
      "Epoch 1396/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -591.1172 - g_loss: 10388.3899\n",
      "Epoch 1397/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -584.1201 - g_loss: 9858.9519\n",
      "Epoch 1398/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -597.3262 - g_loss: 10617.5056\n",
      "Epoch 1399/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -611.2958 - g_loss: 10710.7430\n",
      "Epoch 1400/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -581.3098 - g_loss: 11007.8196\n",
      "Epoch 1401/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -594.8953 - g_loss: 10403.0192\n",
      "Epoch 1402/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -547.1721 - g_loss: 10033.1742\n",
      "Epoch 1403/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -554.6252 - g_loss: 9935.9746\n",
      "Epoch 1404/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -623.2074 - g_loss: 9701.7926\n",
      "Epoch 1405/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -595.7445 - g_loss: 10442.6099\n",
      "Epoch 1406/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -655.1689 - g_loss: 9922.9709\n",
      "Epoch 1407/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -612.4419 - g_loss: 9881.3129\n",
      "Epoch 1408/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -594.4615 - g_loss: 9715.2144\n",
      "Epoch 1409/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -517.8426 - g_loss: 9793.4236\n",
      "Epoch 1410/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -603.2136 - g_loss: 9793.1705\n",
      "Epoch 1411/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -542.6468 - g_loss: 10276.1797\n",
      "Epoch 1412/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -572.4544 - g_loss: 9345.8754\n",
      "Epoch 1413/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -581.9803 - g_loss: 9454.6226\n",
      "Epoch 1414/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -591.9574 - g_loss: 10209.0339\n",
      "Epoch 1415/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -584.8119 - g_loss: 10140.2438\n",
      "Epoch 1416/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -637.5536 - g_loss: 10106.7020\n",
      "Epoch 1417/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -610.6831 - g_loss: 9612.3799\n",
      "Epoch 1418/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -602.1940 - g_loss: 10538.1715\n",
      "Epoch 1419/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -619.8893 - g_loss: 9930.5423\n",
      "Epoch 1420/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -601.2103 - g_loss: 9637.1481\n",
      "Epoch 1421/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -626.5806 - g_loss: 9430.8203\n",
      "Epoch 1422/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -606.1875 - g_loss: 9441.5700\n",
      "Epoch 1423/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -597.4734 - g_loss: 9415.5120\n",
      "Epoch 1424/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -651.1320 - g_loss: 10155.0774\n",
      "Epoch 1425/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -613.5694 - g_loss: 10072.1793\n",
      "Epoch 1426/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -601.4649 - g_loss: 9744.6629\n",
      "Epoch 1427/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -587.1823 - g_loss: 8983.0348\n",
      "Epoch 1428/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -611.7612 - g_loss: 9429.9389\n",
      "Epoch 1429/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -588.6791 - g_loss: 8882.9260\n",
      "Epoch 1430/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -612.2377 - g_loss: 8904.3902\n",
      "Epoch 1431/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -550.0583 - g_loss: 9502.5133\n",
      "Epoch 1432/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -657.0671 - g_loss: 9146.0347\n",
      "Epoch 1433/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -588.4026 - g_loss: 9432.6054\n",
      "Epoch 1434/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -624.5183 - g_loss: 9552.6556\n",
      "Epoch 1435/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -612.9303 - g_loss: 9147.4444\n",
      "Epoch 1436/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -587.6966 - g_loss: 9377.2053\n",
      "Epoch 1437/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -580.2042 - g_loss: 9605.8775\n",
      "Epoch 1438/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -613.6659 - g_loss: 9926.5760\n",
      "Epoch 1439/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -607.4471 - g_loss: 9658.3535\n",
      "Epoch 1440/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -551.5593 - g_loss: 9651.0693\n",
      "Epoch 1441/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -587.0137 - g_loss: 9412.8470\n",
      "Epoch 1442/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -612.7349 - g_loss: 9218.8795\n",
      "Epoch 1443/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -565.3900 - g_loss: 9223.2094\n",
      "Epoch 1444/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -609.8252 - g_loss: 10054.5217\n",
      "Epoch 1445/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -634.8459 - g_loss: 10490.0362\n",
      "Epoch 1446/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -594.2751 - g_loss: 9899.1323\n",
      "Epoch 1447/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -636.8230 - g_loss: 9543.3888\n",
      "Epoch 1448/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -587.2172 - g_loss: 10189.7927\n",
      "Epoch 1449/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -644.1724 - g_loss: 9557.3035\n",
      "Epoch 1450/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -573.1220 - g_loss: 9786.8159\n",
      "Epoch 1451/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -648.3636 - g_loss: 10062.6192\n",
      "Epoch 1452/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -600.7799 - g_loss: 9962.9104\n",
      "Epoch 1453/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -602.1391 - g_loss: 10661.2050\n",
      "Epoch 1454/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -615.1735 - g_loss: 9738.3615\n",
      "Epoch 1455/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -575.4494 - g_loss: 9991.5816\n",
      "Epoch 1456/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -586.3297 - g_loss: 9850.3285\n",
      "Epoch 1457/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -577.3853 - g_loss: 9802.3644\n",
      "Epoch 1458/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -580.2415 - g_loss: 9949.7145\n",
      "Epoch 1459/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -574.7118 - g_loss: 9826.3475\n",
      "Epoch 1460/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -580.4408 - g_loss: 10146.1504\n",
      "Epoch 1461/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -581.3246 - g_loss: 9921.4971\n",
      "Epoch 1462/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -607.5779 - g_loss: 9598.8106\n",
      "Epoch 1463/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -549.9392 - g_loss: 9734.2769\n",
      "Epoch 1464/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -607.2467 - g_loss: 9743.4694\n",
      "Epoch 1465/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -591.5260 - g_loss: 9760.1317\n",
      "Epoch 1466/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -610.9136 - g_loss: 9326.0132\n",
      "Epoch 1467/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -563.6483 - g_loss: 9254.6272\n",
      "Epoch 1468/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -629.7281 - g_loss: 9105.1808\n",
      "Epoch 1469/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -615.3743 - g_loss: 9061.9641\n",
      "Epoch 1470/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -557.7882 - g_loss: 9256.7518\n",
      "Epoch 1471/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -568.3984 - g_loss: 9832.0344\n",
      "Epoch 1472/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -600.1977 - g_loss: 9558.8142\n",
      "Epoch 1473/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -595.0404 - g_loss: 9354.0679\n",
      "Epoch 1474/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -585.0461 - g_loss: 8659.0905\n",
      "Epoch 1475/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -666.9823 - g_loss: 8784.5518\n",
      "Epoch 1476/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -607.0729 - g_loss: 9135.1994\n",
      "Epoch 1477/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -623.7749 - g_loss: 9204.5616\n",
      "Epoch 1478/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -602.2337 - g_loss: 9281.1571\n",
      "Epoch 1479/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -559.3675 - g_loss: 9061.8566\n",
      "Epoch 1480/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -583.6525 - g_loss: 9403.2960\n",
      "Epoch 1481/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -578.6367 - g_loss: 9319.5094\n",
      "Epoch 1482/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -647.8329 - g_loss: 8874.1998\n",
      "Epoch 1483/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -614.5213 - g_loss: 8251.3788\n",
      "Epoch 1484/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -578.7052 - g_loss: 8987.1176\n",
      "Epoch 1485/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -595.0184 - g_loss: 9321.9722\n",
      "Epoch 1486/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -643.0873 - g_loss: 9093.9441\n",
      "Epoch 1487/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -578.0351 - g_loss: 8881.1027\n",
      "Epoch 1488/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -600.3679 - g_loss: 9429.0825\n",
      "Epoch 1489/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -599.7126 - g_loss: 9901.7397\n",
      "Epoch 1490/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -651.2203 - g_loss: 10409.2786\n",
      "Epoch 1491/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -660.6800 - g_loss: 10576.6219\n",
      "Epoch 1492/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -586.6457 - g_loss: 10448.6759\n",
      "Epoch 1493/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -616.7191 - g_loss: 9837.3190\n",
      "Epoch 1494/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -641.0484 - g_loss: 9688.9336\n",
      "Epoch 1495/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -612.8599 - g_loss: 9488.7936\n",
      "Epoch 1496/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -597.7357 - g_loss: 9979.9785\n",
      "Epoch 1497/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -579.3833 - g_loss: 9084.4024\n",
      "Epoch 1498/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -594.1955 - g_loss: 9868.8634\n",
      "Epoch 1499/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -580.4239 - g_loss: 10247.8781\n",
      "Epoch 1500/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -610.1866 - g_loss: 10088.6070\n",
      "Epoch 1501/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -574.5335 - g_loss: 8786.2918\n",
      "Epoch 1502/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -646.7266 - g_loss: 9150.9406\n",
      "Epoch 1503/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -641.4468 - g_loss: 9512.0921\n",
      "Epoch 1504/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -607.4669 - g_loss: 9449.2359\n",
      "Epoch 1505/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -574.1541 - g_loss: 8606.1057\n",
      "Epoch 1506/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -600.2804 - g_loss: 8392.9463\n",
      "Epoch 1507/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -572.0670 - g_loss: 9246.0531\n",
      "Epoch 1508/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -581.8090 - g_loss: 8825.4548\n",
      "Epoch 1509/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -619.4514 - g_loss: 9087.8614\n",
      "Epoch 1510/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -609.0810 - g_loss: 8868.5905\n",
      "Epoch 1511/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -613.7997 - g_loss: 9344.6366\n",
      "Epoch 1512/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -591.8352 - g_loss: 9148.1385\n",
      "Epoch 1513/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -595.2580 - g_loss: 9177.5254\n",
      "Epoch 1514/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -636.6748 - g_loss: 10286.4311\n",
      "Epoch 1515/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -612.2256 - g_loss: 10066.5780\n",
      "Epoch 1516/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -663.7632 - g_loss: 10068.6603\n",
      "Epoch 1517/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -627.6833 - g_loss: 9773.7240\n",
      "Epoch 1518/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -597.3317 - g_loss: 9764.0784\n",
      "Epoch 1519/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -564.8426 - g_loss: 10078.7326\n",
      "Epoch 1520/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -606.6738 - g_loss: 10014.2268\n",
      "Epoch 1521/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -642.9962 - g_loss: 10258.3935\n",
      "Epoch 1522/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -561.8060 - g_loss: 9262.9147\n",
      "Epoch 1523/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -608.0147 - g_loss: 9709.7626\n",
      "Epoch 1524/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -609.0956 - g_loss: 9708.0514\n",
      "Epoch 1525/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -621.2951 - g_loss: 9280.7702\n",
      "Epoch 1526/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -608.9700 - g_loss: 9185.5089\n",
      "Epoch 1527/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -622.9214 - g_loss: 9003.0314\n",
      "Epoch 1528/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -583.7273 - g_loss: 9413.1357\n",
      "Epoch 1529/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -622.4548 - g_loss: 10076.5522\n",
      "Epoch 1530/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -613.7309 - g_loss: 9524.1670\n",
      "Epoch 1531/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -605.4588 - g_loss: 9246.2904\n",
      "Epoch 1532/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -620.1432 - g_loss: 9294.2089\n",
      "Epoch 1533/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -563.1008 - g_loss: 9038.8283\n",
      "Epoch 1534/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -601.4933 - g_loss: 10111.3512\n",
      "Epoch 1535/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -606.7620 - g_loss: 10366.8560\n",
      "Epoch 1536/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -577.5648 - g_loss: 9520.6331\n",
      "Epoch 1537/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -627.0275 - g_loss: 9208.0522\n",
      "Epoch 1538/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -534.6411 - g_loss: 9812.4650\n",
      "Epoch 1539/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -596.2252 - g_loss: 9296.6636\n",
      "Epoch 1540/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -666.3726 - g_loss: 9549.9765\n",
      "Epoch 1541/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -599.6665 - g_loss: 9463.0925\n",
      "Epoch 1542/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -578.8624 - g_loss: 9640.2727\n",
      "Epoch 1543/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -589.5989 - g_loss: 9149.8905\n",
      "Epoch 1544/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -627.7350 - g_loss: 9140.9209\n",
      "Epoch 1545/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -591.7283 - g_loss: 9868.8349\n",
      "Epoch 1546/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -597.3407 - g_loss: 9485.0848\n",
      "Epoch 1547/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -596.9125 - g_loss: 9793.5750\n",
      "Epoch 1548/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -555.2156 - g_loss: 10755.8922\n",
      "Epoch 1549/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -595.2471 - g_loss: 9974.7409\n",
      "Epoch 1550/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -648.6696 - g_loss: 10022.4960\n",
      "Epoch 1551/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -624.6530 - g_loss: 10212.9610\n",
      "Epoch 1552/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -610.9596 - g_loss: 9949.2115\n",
      "Epoch 1553/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -587.0785 - g_loss: 9916.7773\n",
      "Epoch 1554/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -594.5356 - g_loss: 9375.4993\n",
      "Epoch 1555/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -614.1098 - g_loss: 10295.3926\n",
      "Epoch 1556/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -646.0078 - g_loss: 9591.4784\n",
      "Epoch 1557/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -584.2765 - g_loss: 9818.8665\n",
      "Epoch 1558/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -584.8157 - g_loss: 9272.4284\n",
      "Epoch 1559/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -574.0970 - g_loss: 9873.4147\n",
      "Epoch 1560/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -586.1005 - g_loss: 9340.1193\n",
      "Epoch 1561/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -627.1041 - g_loss: 9346.9173\n",
      "Epoch 1562/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -575.6032 - g_loss: 9281.1220\n",
      "Epoch 1563/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -594.3053 - g_loss: 9408.7724\n",
      "Epoch 1564/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -640.2204 - g_loss: 9136.3771\n",
      "Epoch 1565/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -567.7733 - g_loss: 8966.4345\n",
      "Epoch 1566/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -552.7495 - g_loss: 9114.4627\n",
      "Epoch 1567/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -623.2770 - g_loss: 8651.5339\n",
      "Epoch 1568/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -640.1359 - g_loss: 8874.8575\n",
      "Epoch 1569/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -605.9630 - g_loss: 9045.3567\n",
      "Epoch 1570/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -626.2803 - g_loss: 8579.1171\n",
      "Epoch 1571/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -607.3619 - g_loss: 8713.1125\n",
      "Epoch 1572/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -536.4480 - g_loss: 7908.2153\n",
      "Epoch 1573/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -647.7909 - g_loss: 9448.1200\n",
      "Epoch 1574/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -619.6356 - g_loss: 9553.9262\n",
      "Epoch 1575/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -606.3633 - g_loss: 9011.7840\n",
      "Epoch 1576/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -595.6514 - g_loss: 8214.6176\n",
      "Epoch 1577/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -603.6377 - g_loss: 8759.7252\n",
      "Epoch 1578/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -623.6200 - g_loss: 8917.5855\n",
      "Epoch 1579/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -599.4043 - g_loss: 9432.6826\n",
      "Epoch 1580/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -622.3283 - g_loss: 8904.4252\n",
      "Epoch 1581/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -602.5425 - g_loss: 8524.0060\n",
      "Epoch 1582/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -617.0469 - g_loss: 9087.3289\n",
      "Epoch 1583/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -598.4844 - g_loss: 9449.0399\n",
      "Epoch 1584/2000\n",
      "95/95 [==============================] - 20s 214ms/step - d_loss: -641.4752 - g_loss: 9179.6116\n",
      "Epoch 1585/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -624.8537 - g_loss: 9224.9550\n",
      "Epoch 1586/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -630.1565 - g_loss: 8993.1656\n",
      "Epoch 1587/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -571.9424 - g_loss: 8824.2067\n",
      "Epoch 1588/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -599.3118 - g_loss: 8595.4632\n",
      "Epoch 1589/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -612.4107 - g_loss: 8557.7015\n",
      "Epoch 1590/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -568.5357 - g_loss: 8699.9841\n",
      "Epoch 1591/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -600.7024 - g_loss: 9175.7820\n",
      "Epoch 1592/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -589.6940 - g_loss: 8821.8422\n",
      "Epoch 1593/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -632.5291 - g_loss: 8663.4054\n",
      "Epoch 1594/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -598.8054 - g_loss: 9146.2621\n",
      "Epoch 1595/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -632.2814 - g_loss: 9587.7929\n",
      "Epoch 1596/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -571.5468 - g_loss: 9018.8981\n",
      "Epoch 1597/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -645.4791 - g_loss: 8891.7453\n",
      "Epoch 1598/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -632.4281 - g_loss: 9293.3534\n",
      "Epoch 1599/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -620.5642 - g_loss: 9172.5325\n",
      "Epoch 1600/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -595.2487 - g_loss: 9583.2097\n",
      "Epoch 1601/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -587.9122 - g_loss: 9291.0258\n",
      "Epoch 1602/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -618.7311 - g_loss: 8305.4328\n",
      "Epoch 1603/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -570.0689 - g_loss: 8662.4254\n",
      "Epoch 1604/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -640.2193 - g_loss: 8338.0392\n",
      "Epoch 1605/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -623.4705 - g_loss: 8110.3661\n",
      "Epoch 1606/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -639.8963 - g_loss: 8846.1454\n",
      "Epoch 1607/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -591.1745 - g_loss: 8323.2173\n",
      "Epoch 1608/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -626.8218 - g_loss: 7969.2653\n",
      "Epoch 1609/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -584.5503 - g_loss: 8705.3957\n",
      "Epoch 1610/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -582.5737 - g_loss: 8568.9790\n",
      "Epoch 1611/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -625.0797 - g_loss: 8375.1203\n",
      "Epoch 1612/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -601.0254 - g_loss: 7838.3763\n",
      "Epoch 1613/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -594.5595 - g_loss: 8154.2507\n",
      "Epoch 1614/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -642.1249 - g_loss: 8290.3283\n",
      "Epoch 1615/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -616.6610 - g_loss: 8240.4727\n",
      "Epoch 1616/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -590.7553 - g_loss: 8065.0940\n",
      "Epoch 1617/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -624.7028 - g_loss: 8060.6941\n",
      "Epoch 1618/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -590.8907 - g_loss: 7937.4722\n",
      "Epoch 1619/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -612.8573 - g_loss: 7859.5050\n",
      "Epoch 1620/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -613.5031 - g_loss: 7974.2621\n",
      "Epoch 1621/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -604.9217 - g_loss: 7762.8888\n",
      "Epoch 1622/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -658.7555 - g_loss: 7761.4807\n",
      "Epoch 1623/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -617.5737 - g_loss: 7536.4933\n",
      "Epoch 1624/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -614.1252 - g_loss: 7567.1592\n",
      "Epoch 1625/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -623.3454 - g_loss: 7512.3358\n",
      "Epoch 1626/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -634.2142 - g_loss: 7775.4039\n",
      "Epoch 1627/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -592.8527 - g_loss: 7521.6634\n",
      "Epoch 1628/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -591.6168 - g_loss: 7247.7121\n",
      "Epoch 1629/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -590.8523 - g_loss: 8108.7522\n",
      "Epoch 1630/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -610.8812 - g_loss: 8236.7217\n",
      "Epoch 1631/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -580.7483 - g_loss: 7348.6427\n",
      "Epoch 1632/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -563.8207 - g_loss: 7187.6282\n",
      "Epoch 1633/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -628.9869 - g_loss: 7312.8982\n",
      "Epoch 1634/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -569.3608 - g_loss: 7306.5054\n",
      "Epoch 1635/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -569.2450 - g_loss: 7719.8370\n",
      "Epoch 1636/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -567.3700 - g_loss: 7525.2848\n",
      "Epoch 1637/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -640.6221 - g_loss: 7222.0096\n",
      "Epoch 1638/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -645.6056 - g_loss: 7312.7246\n",
      "Epoch 1639/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -609.8459 - g_loss: 7667.0311\n",
      "Epoch 1640/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -627.7316 - g_loss: 7809.9970\n",
      "Epoch 1641/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -592.6691 - g_loss: 7979.1792\n",
      "Epoch 1642/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -581.6353 - g_loss: 8100.9781\n",
      "Epoch 1643/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -624.3011 - g_loss: 8172.3611\n",
      "Epoch 1644/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -627.0709 - g_loss: 7805.4623\n",
      "Epoch 1645/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -607.2862 - g_loss: 7599.2059\n",
      "Epoch 1646/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -615.7906 - g_loss: 7481.0355\n",
      "Epoch 1647/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -635.6907 - g_loss: 7516.7221\n",
      "Epoch 1648/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -611.0858 - g_loss: 7772.7524\n",
      "Epoch 1649/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -659.9587 - g_loss: 7911.9166\n",
      "Epoch 1650/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -648.0996 - g_loss: 8177.6419\n",
      "Epoch 1651/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -617.9236 - g_loss: 7958.0982\n",
      "Epoch 1652/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -614.3714 - g_loss: 7639.5636\n",
      "Epoch 1653/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -571.3673 - g_loss: 7704.5372\n",
      "Epoch 1654/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -666.4820 - g_loss: 8333.7031\n",
      "Epoch 1655/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -646.9016 - g_loss: 8446.5247\n",
      "Epoch 1656/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -625.7610 - g_loss: 7833.4249\n",
      "Epoch 1657/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -591.0569 - g_loss: 8234.0240\n",
      "Epoch 1658/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -634.6148 - g_loss: 8099.4347\n",
      "Epoch 1659/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -644.7138 - g_loss: 7996.1448\n",
      "Epoch 1660/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -610.9386 - g_loss: 7784.9287\n",
      "Epoch 1661/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -599.9693 - g_loss: 7437.2893\n",
      "Epoch 1662/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -599.9300 - g_loss: 7886.9800\n",
      "Epoch 1663/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -631.9492 - g_loss: 8092.3885\n",
      "Epoch 1664/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -624.1433 - g_loss: 7512.9199\n",
      "Epoch 1665/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -695.4493 - g_loss: 8370.1702\n",
      "Epoch 1666/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -618.9792 - g_loss: 9169.9354\n",
      "Epoch 1667/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -562.6973 - g_loss: 8106.0928\n",
      "Epoch 1668/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -618.8138 - g_loss: 7544.6295\n",
      "Epoch 1669/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -556.3062 - g_loss: 7215.9854\n",
      "Epoch 1670/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -634.3706 - g_loss: 7238.4726\n",
      "Epoch 1671/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -645.2175 - g_loss: 7206.1410\n",
      "Epoch 1672/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -632.9652 - g_loss: 7559.3592\n",
      "Epoch 1673/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -651.8490 - g_loss: 8160.3239\n",
      "Epoch 1674/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -571.5145 - g_loss: 7743.9950\n",
      "Epoch 1675/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -601.9623 - g_loss: 8216.8999\n",
      "Epoch 1676/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -624.7564 - g_loss: 7217.4520\n",
      "Epoch 1677/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -623.9028 - g_loss: 7749.7440\n",
      "Epoch 1678/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -596.0290 - g_loss: 8191.1739\n",
      "Epoch 1679/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -583.3305 - g_loss: 7699.6909\n",
      "Epoch 1680/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -618.2511 - g_loss: 6705.0175\n",
      "Epoch 1681/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -590.6349 - g_loss: 6452.5022\n",
      "Epoch 1682/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -565.2571 - g_loss: 6673.2582\n",
      "Epoch 1683/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -599.5875 - g_loss: 7040.8024\n",
      "Epoch 1684/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -593.3989 - g_loss: 7834.0123\n",
      "Epoch 1685/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -634.5902 - g_loss: 8404.0219\n",
      "Epoch 1686/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -652.8585 - g_loss: 7975.6491\n",
      "Epoch 1687/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -617.2629 - g_loss: 7747.6489\n",
      "Epoch 1688/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -670.3618 - g_loss: 8121.5304\n",
      "Epoch 1689/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -609.0344 - g_loss: 7493.9637\n",
      "Epoch 1690/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -635.9212 - g_loss: 7657.4007\n",
      "Epoch 1691/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -609.8021 - g_loss: 7138.1600\n",
      "Epoch 1692/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -596.7386 - g_loss: 7317.9550\n",
      "Epoch 1693/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -567.1377 - g_loss: 7655.4266\n",
      "Epoch 1694/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -633.0172 - g_loss: 7327.4497\n",
      "Epoch 1695/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -624.5413 - g_loss: 8120.3185\n",
      "Epoch 1696/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -601.7032 - g_loss: 7235.7582\n",
      "Epoch 1697/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -576.2930 - g_loss: 7587.8012\n",
      "Epoch 1698/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -670.2631 - g_loss: 7653.7288\n",
      "Epoch 1699/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -610.2134 - g_loss: 7667.3026\n",
      "Epoch 1700/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -593.0631 - g_loss: 8064.9631\n",
      "Epoch 1701/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -633.3693 - g_loss: 8227.2068\n",
      "Epoch 1702/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -606.6524 - g_loss: 7546.0446\n",
      "Epoch 1703/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -606.2737 - g_loss: 7683.6800\n",
      "Epoch 1704/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -611.0334 - g_loss: 7382.9107\n",
      "Epoch 1705/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -644.2780 - g_loss: 8026.0584\n",
      "Epoch 1706/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -599.4272 - g_loss: 7784.5901\n",
      "Epoch 1707/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -629.7390 - g_loss: 7579.0688\n",
      "Epoch 1708/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -627.9304 - g_loss: 7947.3945\n",
      "Epoch 1709/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -612.6827 - g_loss: 7851.2778\n",
      "Epoch 1710/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -637.2578 - g_loss: 7744.6602\n",
      "Epoch 1711/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -591.8781 - g_loss: 7070.3311\n",
      "Epoch 1712/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -660.5081 - g_loss: 7802.9854\n",
      "Epoch 1713/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -589.7948 - g_loss: 7842.5666\n",
      "Epoch 1714/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -589.4572 - g_loss: 7341.2963\n",
      "Epoch 1715/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -629.6080 - g_loss: 7306.7550\n",
      "Epoch 1716/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -669.7246 - g_loss: 7236.5190\n",
      "Epoch 1717/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -654.3514 - g_loss: 7586.8423\n",
      "Epoch 1718/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -617.4165 - g_loss: 6969.2302\n",
      "Epoch 1719/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -598.1804 - g_loss: 6732.0160\n",
      "Epoch 1720/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -624.2588 - g_loss: 6860.2544\n",
      "Epoch 1721/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -664.1865 - g_loss: 8119.2239\n",
      "Epoch 1722/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -637.3861 - g_loss: 8422.4475\n",
      "Epoch 1723/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -637.5031 - g_loss: 7893.0143\n",
      "Epoch 1724/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -612.8121 - g_loss: 7252.6223\n",
      "Epoch 1725/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -669.5673 - g_loss: 8385.0135\n",
      "Epoch 1726/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -595.9816 - g_loss: 8333.9598\n",
      "Epoch 1727/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -610.4967 - g_loss: 7457.5320\n",
      "Epoch 1728/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -637.5331 - g_loss: 7428.7122\n",
      "Epoch 1729/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -648.5394 - g_loss: 7927.2254\n",
      "Epoch 1730/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -629.3940 - g_loss: 7993.3301\n",
      "Epoch 1731/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -623.9535 - g_loss: 7184.4573\n",
      "Epoch 1732/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -647.5455 - g_loss: 7817.4933\n",
      "Epoch 1733/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -625.1252 - g_loss: 7498.7008\n",
      "Epoch 1734/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -601.6798 - g_loss: 7764.0781\n",
      "Epoch 1735/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -590.4264 - g_loss: 7334.0642\n",
      "Epoch 1736/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -642.8109 - g_loss: 6801.4002\n",
      "Epoch 1737/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -648.6477 - g_loss: 7604.1891\n",
      "Epoch 1738/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -635.6098 - g_loss: 7878.3733\n",
      "Epoch 1739/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -663.6808 - g_loss: 8357.9256\n",
      "Epoch 1740/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -625.7630 - g_loss: 8172.6452\n",
      "Epoch 1741/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -603.1666 - g_loss: 8002.2498\n",
      "Epoch 1742/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -664.8721 - g_loss: 8090.8667\n",
      "Epoch 1743/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -613.6371 - g_loss: 8439.5253\n",
      "Epoch 1744/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -657.7217 - g_loss: 8780.7862\n",
      "Epoch 1745/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -642.1322 - g_loss: 8160.2798\n",
      "Epoch 1746/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -611.5922 - g_loss: 7776.4288\n",
      "Epoch 1747/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -605.2020 - g_loss: 7948.4500\n",
      "Epoch 1748/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -649.3465 - g_loss: 8318.1690\n",
      "Epoch 1749/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -558.2355 - g_loss: 8093.1195\n",
      "Epoch 1750/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -642.5388 - g_loss: 7329.3817\n",
      "Epoch 1751/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -632.7097 - g_loss: 7243.5014\n",
      "Epoch 1752/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -629.1398 - g_loss: 8312.2994\n",
      "Epoch 1753/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -645.3575 - g_loss: 8222.7292\n",
      "Epoch 1754/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -623.9792 - g_loss: 8577.6104\n",
      "Epoch 1755/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -651.3370 - g_loss: 8960.3043\n",
      "Epoch 1756/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -635.0524 - g_loss: 9019.5661\n",
      "Epoch 1757/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -697.0737 - g_loss: 9875.9567\n",
      "Epoch 1758/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -588.4626 - g_loss: 8589.9088\n",
      "Epoch 1759/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -624.6399 - g_loss: 8463.2933\n",
      "Epoch 1760/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -645.4552 - g_loss: 8712.8547\n",
      "Epoch 1761/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -618.3013 - g_loss: 9087.4204\n",
      "Epoch 1762/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -680.1626 - g_loss: 9127.6278\n",
      "Epoch 1763/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -636.6853 - g_loss: 8837.0403\n",
      "Epoch 1764/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -617.6556 - g_loss: 8490.0636\n",
      "Epoch 1765/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -636.5515 - g_loss: 8173.2892\n",
      "Epoch 1766/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -657.1720 - g_loss: 7861.6697\n",
      "Epoch 1767/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -714.7295 - g_loss: 9391.8666\n",
      "Epoch 1768/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -625.2558 - g_loss: 8834.2096\n",
      "Epoch 1769/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -650.2465 - g_loss: 8810.9069\n",
      "Epoch 1770/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -650.5818 - g_loss: 9598.6737\n",
      "Epoch 1771/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -718.4117 - g_loss: 9392.3669\n",
      "Epoch 1772/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -665.2754 - g_loss: 9449.8717\n",
      "Epoch 1773/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -602.3466 - g_loss: 8685.0822\n",
      "Epoch 1774/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -622.8084 - g_loss: 9561.1352\n",
      "Epoch 1775/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -671.9353 - g_loss: 10067.1470\n",
      "Epoch 1776/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -596.8433 - g_loss: 9578.9966\n",
      "Epoch 1777/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -566.0603 - g_loss: 8886.0552\n",
      "Epoch 1778/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -595.5068 - g_loss: 8489.7789\n",
      "Epoch 1779/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -569.8282 - g_loss: 8374.2895\n",
      "Epoch 1780/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -593.6211 - g_loss: 7525.8125\n",
      "Epoch 1781/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -646.7646 - g_loss: 7740.1226\n",
      "Epoch 1782/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -647.5095 - g_loss: 8064.0591\n",
      "Epoch 1783/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -633.5289 - g_loss: 8163.2089\n",
      "Epoch 1784/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -639.3678 - g_loss: 8104.1326\n",
      "Epoch 1785/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -642.6184 - g_loss: 7613.0602\n",
      "Epoch 1786/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -600.3735 - g_loss: 7888.6838\n",
      "Epoch 1787/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -602.5031 - g_loss: 8747.3365\n",
      "Epoch 1788/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -657.9792 - g_loss: 8153.3540\n",
      "Epoch 1789/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -637.8670 - g_loss: 8334.2291\n",
      "Epoch 1790/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -635.1780 - g_loss: 8797.2294\n",
      "Epoch 1791/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -598.2452 - g_loss: 9381.9644\n",
      "Epoch 1792/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -600.3289 - g_loss: 9651.9941\n",
      "Epoch 1793/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -620.5093 - g_loss: 9150.7177\n",
      "Epoch 1794/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -625.7166 - g_loss: 8629.1312\n",
      "Epoch 1795/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -678.7137 - g_loss: 8925.7126\n",
      "Epoch 1796/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -591.0238 - g_loss: 8751.1636\n",
      "Epoch 1797/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -662.9808 - g_loss: 8286.2522\n",
      "Epoch 1798/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -625.1604 - g_loss: 7992.3086\n",
      "Epoch 1799/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -617.7636 - g_loss: 7613.4100\n",
      "Epoch 1800/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -620.7205 - g_loss: 8077.6954\n",
      "Epoch 1801/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -609.3686 - g_loss: 8195.1434\n",
      "Epoch 1802/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -623.3992 - g_loss: 8351.8325\n",
      "Epoch 1803/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -571.8999 - g_loss: 8316.0262\n",
      "Epoch 1804/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -601.3731 - g_loss: 7815.7555\n",
      "Epoch 1805/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -609.7484 - g_loss: 7687.2266\n",
      "Epoch 1806/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -670.4480 - g_loss: 7029.5897\n",
      "Epoch 1807/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -634.5180 - g_loss: 6821.0853\n",
      "Epoch 1808/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -621.4746 - g_loss: 7663.9872\n",
      "Epoch 1809/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -593.2534 - g_loss: 7479.5590\n",
      "Epoch 1810/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -670.1681 - g_loss: 6829.3875\n",
      "Epoch 1811/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -630.8835 - g_loss: 7252.5217\n",
      "Epoch 1812/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -646.0054 - g_loss: 6828.3945\n",
      "Epoch 1813/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -663.4402 - g_loss: 7928.1489\n",
      "Epoch 1814/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -615.0226 - g_loss: 8008.3903\n",
      "Epoch 1815/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -649.9229 - g_loss: 8084.3520\n",
      "Epoch 1816/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -596.3985 - g_loss: 7660.3249\n",
      "Epoch 1817/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -667.9465 - g_loss: 7905.1050\n",
      "Epoch 1818/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -665.7161 - g_loss: 8071.8307\n",
      "Epoch 1819/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -585.8481 - g_loss: 8508.7632\n",
      "Epoch 1820/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -659.0878 - g_loss: 8984.7497\n",
      "Epoch 1821/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -627.1861 - g_loss: 8591.2708\n",
      "Epoch 1822/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -663.7630 - g_loss: 8930.1643\n",
      "Epoch 1823/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -701.3672 - g_loss: 8861.2196\n",
      "Epoch 1824/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -620.2887 - g_loss: 8526.4569\n",
      "Epoch 1825/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -638.4309 - g_loss: 8672.5701\n",
      "Epoch 1826/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -653.6306 - g_loss: 8972.3751\n",
      "Epoch 1827/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -641.4754 - g_loss: 8265.0906\n",
      "Epoch 1828/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -604.5132 - g_loss: 7892.0328\n",
      "Epoch 1829/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -613.0058 - g_loss: 8246.1023\n",
      "Epoch 1830/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -642.5795 - g_loss: 7392.8406\n",
      "Epoch 1831/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -664.6998 - g_loss: 8649.9131\n",
      "Epoch 1832/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -647.7834 - g_loss: 8702.2214\n",
      "Epoch 1833/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -633.5896 - g_loss: 8379.2467\n",
      "Epoch 1834/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -632.0632 - g_loss: 8044.5780\n",
      "Epoch 1835/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -654.6737 - g_loss: 7938.7813\n",
      "Epoch 1836/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -618.1524 - g_loss: 7937.9721\n",
      "Epoch 1837/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -654.7860 - g_loss: 7731.8096\n",
      "Epoch 1838/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -609.7849 - g_loss: 7988.9807\n",
      "Epoch 1839/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -633.8897 - g_loss: 7936.3491\n",
      "Epoch 1840/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -664.4432 - g_loss: 8808.1670\n",
      "Epoch 1841/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -659.2898 - g_loss: 8839.5378\n",
      "Epoch 1842/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -656.3978 - g_loss: 9178.2383\n",
      "Epoch 1843/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -660.5014 - g_loss: 9642.3780\n",
      "Epoch 1844/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -636.4334 - g_loss: 9748.8647\n",
      "Epoch 1845/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -638.6582 - g_loss: 9066.2372\n",
      "Epoch 1846/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -648.5665 - g_loss: 9076.8748\n",
      "Epoch 1847/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -647.1383 - g_loss: 8312.1201\n",
      "Epoch 1848/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -690.0478 - g_loss: 8711.2917\n",
      "Epoch 1849/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -650.2226 - g_loss: 8430.7067\n",
      "Epoch 1850/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -577.5262 - g_loss: 8598.3513\n",
      "Epoch 1851/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -696.7138 - g_loss: 8142.0670\n",
      "Epoch 1852/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -633.2655 - g_loss: 8259.0952\n",
      "Epoch 1853/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -581.7070 - g_loss: 8546.9629\n",
      "Epoch 1854/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -674.1938 - g_loss: 8411.4615\n",
      "Epoch 1855/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -625.1627 - g_loss: 8503.4151\n",
      "Epoch 1856/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -600.2789 - g_loss: 7794.2321\n",
      "Epoch 1857/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -650.2245 - g_loss: 7879.0593\n",
      "Epoch 1858/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -620.0777 - g_loss: 8639.9963\n",
      "Epoch 1859/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -584.9627 - g_loss: 9706.3290\n",
      "Epoch 1860/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -616.0521 - g_loss: 9744.0338\n",
      "Epoch 1861/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -674.7034 - g_loss: 7809.2631\n",
      "Epoch 1862/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -610.1587 - g_loss: 8269.1583\n",
      "Epoch 1863/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -658.3449 - g_loss: 9305.6162\n",
      "Epoch 1864/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -583.3602 - g_loss: 9098.5473\n",
      "Epoch 1865/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -597.9616 - g_loss: 8976.5300\n",
      "Epoch 1866/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -667.5238 - g_loss: 8623.8239\n",
      "Epoch 1867/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -642.8933 - g_loss: 8630.5829\n",
      "Epoch 1868/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -596.8422 - g_loss: 7928.2794\n",
      "Epoch 1869/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -647.6651 - g_loss: 8063.5490\n",
      "Epoch 1870/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -615.5882 - g_loss: 9054.8377\n",
      "Epoch 1871/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -586.8637 - g_loss: 9082.4234\n",
      "Epoch 1872/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -603.8621 - g_loss: 8509.7006\n",
      "Epoch 1873/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -628.5708 - g_loss: 7919.8506\n",
      "Epoch 1874/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -602.9250 - g_loss: 8720.9362\n",
      "Epoch 1875/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -587.4882 - g_loss: 8542.9805\n",
      "Epoch 1876/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -613.6731 - g_loss: 7899.9488\n",
      "Epoch 1877/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -607.6702 - g_loss: 8695.6401\n",
      "Epoch 1878/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -629.2341 - g_loss: 8501.1360\n",
      "Epoch 1879/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -646.4790 - g_loss: 8212.1923\n",
      "Epoch 1880/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -643.5528 - g_loss: 8534.3038\n",
      "Epoch 1881/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -660.6275 - g_loss: 8857.5856\n",
      "Epoch 1882/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -637.4759 - g_loss: 9386.4635\n",
      "Epoch 1883/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -623.3896 - g_loss: 8380.8718\n",
      "Epoch 1884/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -650.0821 - g_loss: 7583.7487\n",
      "Epoch 1885/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -667.2314 - g_loss: 8492.4420\n",
      "Epoch 1886/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -616.8678 - g_loss: 8284.1159\n",
      "Epoch 1887/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -674.7261 - g_loss: 7417.2533\n",
      "Epoch 1888/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -618.1351 - g_loss: 8381.6598\n",
      "Epoch 1889/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -584.9904 - g_loss: 7684.1277\n",
      "Epoch 1890/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -590.5707 - g_loss: 7754.7876\n",
      "Epoch 1891/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -610.0260 - g_loss: 7861.0329\n",
      "Epoch 1892/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -622.5194 - g_loss: 7561.1943\n",
      "Epoch 1893/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -667.7581 - g_loss: 7616.7334\n",
      "Epoch 1894/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -688.0042 - g_loss: 8760.9501\n",
      "Epoch 1895/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -622.7605 - g_loss: 8298.5540\n",
      "Epoch 1896/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -594.1678 - g_loss: 8059.5006\n",
      "Epoch 1897/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -645.8189 - g_loss: 8556.7832\n",
      "Epoch 1898/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -629.1566 - g_loss: 8074.2398\n",
      "Epoch 1899/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -620.3001 - g_loss: 7314.9963\n",
      "Epoch 1900/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -624.1173 - g_loss: 8111.6897\n",
      "Epoch 1901/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -619.1772 - g_loss: 7740.3008\n",
      "Epoch 1902/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -599.3028 - g_loss: 7477.0864\n",
      "Epoch 1903/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -662.0759 - g_loss: 7712.3958\n",
      "Epoch 1904/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -632.6940 - g_loss: 7564.4944\n",
      "Epoch 1905/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -632.6534 - g_loss: 7797.5322\n",
      "Epoch 1906/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -628.8308 - g_loss: 8069.7171\n",
      "Epoch 1907/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -636.7509 - g_loss: 7657.7636\n",
      "Epoch 1908/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -623.9306 - g_loss: 7037.9834\n",
      "Epoch 1909/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -590.7541 - g_loss: 7819.9042\n",
      "Epoch 1910/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -617.1304 - g_loss: 8241.8887\n",
      "Epoch 1911/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -659.3387 - g_loss: 7711.0437\n",
      "Epoch 1912/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -632.6708 - g_loss: 8118.3960\n",
      "Epoch 1913/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -659.8230 - g_loss: 7550.3066\n",
      "Epoch 1914/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -629.9282 - g_loss: 7402.3523\n",
      "Epoch 1915/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -663.0128 - g_loss: 8208.5737\n",
      "Epoch 1916/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -653.9076 - g_loss: 7839.1821\n",
      "Epoch 1917/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -588.3223 - g_loss: 7634.2329\n",
      "Epoch 1918/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -670.0167 - g_loss: 7592.2127\n",
      "Epoch 1919/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -642.4217 - g_loss: 8091.8500\n",
      "Epoch 1920/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -636.3277 - g_loss: 7797.2060\n",
      "Epoch 1921/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -643.4502 - g_loss: 7812.9829\n",
      "Epoch 1922/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -658.3173 - g_loss: 8095.6861\n",
      "Epoch 1923/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -628.6383 - g_loss: 7224.6248\n",
      "Epoch 1924/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -635.6310 - g_loss: 8228.7872\n",
      "Epoch 1925/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -636.5066 - g_loss: 7940.9402\n",
      "Epoch 1926/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -673.6703 - g_loss: 8240.3769\n",
      "Epoch 1927/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -615.9203 - g_loss: 8475.6046\n",
      "Epoch 1928/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -632.7160 - g_loss: 8925.2926\n",
      "Epoch 1929/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -599.4570 - g_loss: 8574.1840\n",
      "Epoch 1930/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -670.8994 - g_loss: 8826.0246\n",
      "Epoch 1931/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -635.1256 - g_loss: 8294.9020\n",
      "Epoch 1932/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -655.6054 - g_loss: 8325.2704\n",
      "Epoch 1933/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -600.5453 - g_loss: 8747.8153\n",
      "Epoch 1934/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -619.6827 - g_loss: 8514.1946\n",
      "Epoch 1935/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -651.9319 - g_loss: 8655.1498\n",
      "Epoch 1936/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -665.5338 - g_loss: 8440.9023\n",
      "Epoch 1937/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -646.1764 - g_loss: 8266.6762\n",
      "Epoch 1938/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -628.4996 - g_loss: 8095.6474\n",
      "Epoch 1939/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -612.1275 - g_loss: 8228.6346\n",
      "Epoch 1940/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -618.1224 - g_loss: 8360.3287\n",
      "Epoch 1941/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -605.2326 - g_loss: 8516.5525\n",
      "Epoch 1942/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -648.8771 - g_loss: 7909.5731\n",
      "Epoch 1943/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -640.0041 - g_loss: 7118.1481\n",
      "Epoch 1944/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -642.8879 - g_loss: 8431.4464\n",
      "Epoch 1945/2000\n",
      "95/95 [==============================] - 22s 230ms/step - d_loss: -640.5190 - g_loss: 7350.4701\n",
      "Epoch 1946/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -648.5728 - g_loss: 8271.9737\n",
      "Epoch 1947/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -603.3758 - g_loss: 9045.3303\n",
      "Epoch 1948/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -579.0506 - g_loss: 8166.7038\n",
      "Epoch 1949/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -687.5251 - g_loss: 7967.7583\n",
      "Epoch 1950/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -644.5496 - g_loss: 8035.9673\n",
      "Epoch 1951/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -611.7433 - g_loss: 7089.7428\n",
      "Epoch 1952/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -643.5379 - g_loss: 7839.7541\n",
      "Epoch 1953/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -653.0910 - g_loss: 7836.0876\n",
      "Epoch 1954/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -614.0220 - g_loss: 7637.5649\n",
      "Epoch 1955/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -618.9361 - g_loss: 8300.4744\n",
      "Epoch 1956/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -619.8143 - g_loss: 8115.3131\n",
      "Epoch 1957/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -619.7175 - g_loss: 8503.0795\n",
      "Epoch 1958/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -656.7502 - g_loss: 8738.7851\n",
      "Epoch 1959/2000\n",
      "95/95 [==============================] - 12s 130ms/step - d_loss: -571.7882 - g_loss: 8340.1342\n",
      "Epoch 1960/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -614.0469 - g_loss: 7806.7250\n",
      "Epoch 1961/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -662.6006 - g_loss: 8728.2961\n",
      "Epoch 1962/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -619.9396 - g_loss: 9156.0189\n",
      "Epoch 1963/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -639.6848 - g_loss: 8146.7448\n",
      "Epoch 1964/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -632.7821 - g_loss: 8371.0353\n",
      "Epoch 1965/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -607.7066 - g_loss: 9225.0752\n",
      "Epoch 1966/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -643.4757 - g_loss: 9070.9959\n",
      "Epoch 1967/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -679.3572 - g_loss: 8735.0672\n",
      "Epoch 1968/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -693.4260 - g_loss: 9071.9831\n",
      "Epoch 1969/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -648.5026 - g_loss: 9134.6227\n",
      "Epoch 1970/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -714.6658 - g_loss: 9081.8616\n",
      "Epoch 1971/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -628.2388 - g_loss: 9261.1542\n",
      "Epoch 1972/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -608.8005 - g_loss: 8907.9150\n",
      "Epoch 1973/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -635.1030 - g_loss: 8510.7683\n",
      "Epoch 1974/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -678.3368 - g_loss: 9177.9036\n",
      "Epoch 1975/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -659.5880 - g_loss: 9494.5408\n",
      "Epoch 1976/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -649.4703 - g_loss: 9451.0192\n",
      "Epoch 1977/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -632.9816 - g_loss: 8551.0671\n",
      "Epoch 1978/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -657.4802 - g_loss: 8772.3923\n",
      "Epoch 1979/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -703.9093 - g_loss: 8655.5731\n",
      "Epoch 1980/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -611.0978 - g_loss: 8539.3784\n",
      "Epoch 1981/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -627.3778 - g_loss: 8136.1623\n",
      "Epoch 1982/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -646.8552 - g_loss: 8766.9084\n",
      "Epoch 1983/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -674.6127 - g_loss: 8762.3870\n",
      "Epoch 1984/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -582.9813 - g_loss: 8517.3890\n",
      "Epoch 1985/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -625.1694 - g_loss: 7767.6212\n",
      "Epoch 1986/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -625.6733 - g_loss: 8824.8406\n",
      "Epoch 1987/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -655.7532 - g_loss: 9406.9459\n",
      "Epoch 1988/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -680.9393 - g_loss: 9102.5137\n",
      "Epoch 1989/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -632.9330 - g_loss: 8780.9957\n",
      "Epoch 1990/2000\n",
      "95/95 [==============================] - 12s 129ms/step - d_loss: -668.9915 - g_loss: 8955.1100\n",
      "Epoch 1991/2000\n",
      "95/95 [==============================] - 12s 124ms/step - d_loss: -678.1512 - g_loss: 9402.1435\n",
      "Epoch 1992/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -628.7785 - g_loss: 8571.7963\n",
      "Epoch 1993/2000\n",
      "95/95 [==============================] - 12s 125ms/step - d_loss: -660.4834 - g_loss: 8352.7883\n",
      "Epoch 1994/2000\n",
      "95/95 [==============================] - 12s 128ms/step - d_loss: -647.7826 - g_loss: 8781.0933\n",
      "Epoch 1995/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -648.5609 - g_loss: 9485.0877\n",
      "Epoch 1996/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -675.8422 - g_loss: 9128.7250\n",
      "Epoch 1997/2000\n",
      "95/95 [==============================] - 12s 126ms/step - d_loss: -627.8626 - g_loss: 8735.6220\n",
      "Epoch 1998/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -619.2983 - g_loss: 8951.3587\n",
      "Epoch 1999/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -641.6821 - g_loss: 9526.4944\n",
      "Epoch 2000/2000\n",
      "95/95 [==============================] - 12s 127ms/step - d_loss: -682.8838 - g_loss: 8119.5555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd1914eb358>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training the model.\n",
    "wgan.fit(train_ds, batch_size=BATCH_SIZE, epochs=2000, callbacks=[cbk, tb_cbk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Examples using learned Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 11.5, 11.5, -0.5)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load generator\n",
    "'''\n",
    "generator.compile(optimizer=Adam(lr=0.0008), # per Foster, 2017 RMSprop(lr=0.0008)\n",
    "                          loss=binary_crossentropy,\n",
    "                          metrics=['accuracy'])\n",
    "generator = tf.keras.models.load_model('/data/output/models/dwarfganWGANGPR02/generator-2021-04-04_025322.h5')\n",
    "'''\n",
    "# generate new example of learned representation in latent space\n",
    "try:\n",
    "    generator\n",
    "except NameError:\n",
    "    #get latest generator model save file\n",
    "    folder = pathlib.Path(f'{out_model_dir}/{model_name}')\n",
    "    saves = list(folder.glob('generator*'))\n",
    "    latest = max(saves, key=os.path.getctime)\n",
    "    #load latest generator save file\n",
    "    generator = tf.keras.models.load_model(latest)\n",
    "        \n",
    "noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "res = np.array(generator(noise, training=False)).astype('uint8')\n",
    "\n",
    "#Rescale\n",
    "res = res.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Visualize result\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(res)\n",
    "plt.title(f'Example Generator Output')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
