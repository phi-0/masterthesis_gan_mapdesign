{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DwarfGAN - Deep Learning based Map Design for Dwarf Fortress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "loosely based on example coded provided in Foster, 2019 see:\n",
    "\n",
    "basic GAN\n",
    "\"G:\\Dev\\DataScience\\GDL_code\\models\\GAN.py\"\n",
    "\n",
    "Wasserstein GAN\n",
    "\"G:\\Dev\\DataScience\\GDL_code\\models\\WGAN.py\"\n",
    "\n",
    "Wasserstein GAN with Gradient Penatly\n",
    "\"G:\\Dev\\DataScience\\GDL_code\\models\\WGANGP.py\"\n",
    "\n",
    "\"\"\"\n",
    "# imports\n",
    "from keras.layers import Add, Concatenate, Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D, LayerNormalization\n",
    "from keras.layers.experimental import preprocessing\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.losses import binary_crossentropy, Loss\n",
    "from keras import metrics\n",
    "from functools import partial\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import random\n",
    "\n",
    "#!pip install boto3\n",
    "import boto3 as b3 \n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "############### CONFIG ###################\n",
    "\n",
    "# model name\n",
    "model_name = 'dwarfganWGANGPR08'\n",
    "# folder path to input files (map images)\n",
    "fpath = r'/data2/input'\n",
    "# folder path to tensorboard output\n",
    "tboard_dir = '/data2/output/tensorboard'\n",
    "# folder path for saved model output\n",
    "out_model_dir = '/data2/output/models'\n",
    "# folder for images to be saved during training\n",
    "out_img_dir = '/data2/output/images'\n",
    "# frequency of checkpoint saves (images, model weights) in epochs\n",
    "# CHECKPOINT = 50\n",
    "# use skip connections (additive)?\n",
    "SKIP_ADD = False\n",
    "SKIP_CONCAT = False\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 1000 \n",
    "#BATCH_PER_EPOCH = 20\n",
    "# pre-processed (cropped) images are 1024x1024. We will later resize the images to 256x256 due to memory restrictions.\n",
    "IMAGE_SIZE = (12,12)\n",
    "BATCH_SIZE = 128\n",
    "CRITIC_FACTOR = 5 # number of times the critic is trained more often than the generator. Recommended = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "RELU_SLOPE_C = 0.2\n",
    "RELU_SLOPE_G = 0.2\n",
    "DROPOUT_C = 0.3\n",
    "MOMENTUM_G = 0.9\n",
    "CRIT_LR = 0.0003 # Adjusted learning rates according to two time-scale update rule (TTUR), see Heusel et al., 2017\n",
    "GEN_LR = 0.0001\n",
    "\n",
    "# NOTE: all extracted map PNGs have been saved on a separate virtual disk mapped to '/data' of the virtual machine in use\n",
    "data_dir = pathlib.Path(fpath + '/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Train / Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map images sourced from the DFMA come in a variety of dimensions. In order to create sample images with constant dimensions, as required by tensors, the 100k input samples were run through a python script to randomly crop 10 1024 x 1024 areas per picture. Of those cropped (sub-)images, only the ones which contain structures were retained. This was achieved by filtering out image crops which only contained two or less different colors. With that, the logic mainly filterd out crops which only contained black. This process resulted in 700'000+ (sub-)image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12068 cropped image samples available\n"
     ]
    }
   ],
   "source": [
    "# use pre-processed (cropped) 128 x 128 images\n",
    "data_dir = pathlib.Path(fpath + '/ascii_crops_12/maps')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "print(f'There are {str(len(imgs))} cropped image samples available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random sample input image:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAIAAADZF8uwAAAAMElEQVR4nGNkwA3q6+sdHR3379/PiEfRgQMH9u/f7+joyIRHEUTFqEnEmwQPezwkAFi8SkBBI7BlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=12x12 at 0x7F53084D5CC0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show example sample image (cropped to 128x128)\n",
    "print('A random sample input image:')\n",
    "PIL.Image.open(imgs[random.randint(0,len(imgs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12068 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# creating keras datasets for training and validation - refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "dataset_train = tf.keras.preprocessing.image_dataset_from_directory(  fpath+'/ascii_crops_12',\n",
    "                                                                      image_size=IMAGE_SIZE, \n",
    "                                                                      batch_size=BATCH_SIZE, \n",
    "                                                                      labels=[1.] * len(imgs), # setting all labels to 1.0 (for 'real') as float32\n",
    "                                                                      #label_mode=None, # yields float32 type labels\n",
    "                                                                      seed=1234 #,\n",
    "                                                                      #validation_split=0.8, # due to the large number of images, we split out more into the validation set (which is not being used)\n",
    "                                                                      #subset='training'\n",
    "                                                                    )\n",
    "\n",
    "#dataset_val = tf.keras.preprocessing.image_dataset_from_directory(  fpath,\n",
    "#                                                                    image_size=IMAGE_SIZE, \n",
    "#                                                                    batch_size=BATCH_SIZE, \n",
    "#                                                                    labels=[1.] * len(imgs), # setting all labels to 1.0 (for 'real') as float32\n",
    "#                                                                    #label_mode=None, # yields float32 type labels\n",
    "#                                                                    seed=42,\n",
    "#                                                                    validation_split=0.5,\n",
    "#                                                                    subset='validation'\n",
    "#                                                                    )\n",
    "\n",
    "\n",
    "# refer to https://www.tensorflow.org/tutorials/images/classification\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = dataset_train.cache().prefetch(buffer_size=BATCH_SIZE)\n",
    "#val_ds = dataset_val.cache().prefetch(buffer_size=BATCH_SIZE)\n",
    "#val_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAINING = 12068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Random Sample from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check random images from prepared batches\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1): # take one batch. Here batch_size = 128 examples per batch\n",
    "    for i in range(9): # show first 9 images of batch\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(f'Image Check {i+1}, label {labels[i]}')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATED Model Definition\n",
    "\n",
    "Generally the following changes have been implemented to the architectures:\n",
    "\n",
    "### RUN04\n",
    "- increased number of filters\n",
    "- instead of stagnating number of filters, the number of filters now steadily decreases (generator) / increases (discriminator). The last CONV layer of the discriminator would have 1024 filters however that results in a large number of nodes on the last DENSE layer and therefore an increase of trainable parameters by around 7 milion. Therefore only 512 filters are configured for the the last discriminator CONV layer.\n",
    "- changed dimension of first dense layer in generator after reshape to (8,8,1024) to provide more features to be learned (see Radford et al., 2015)\n",
    "- deactivated biases in first DENSE layer and CONV2DTranspose layers of generator \n",
    "- implemented dropout for all discriminator layers expect first and last\n",
    "- added batchnorm(0.9) for all generator layers (default is 0.99)\n",
    "\n",
    "The following changes have been implemented to the architecture for RUN04:\n",
    "\n",
    "- changed input image crops to 128 x 128\n",
    "- changed critic learning rate to 0.0003 (from 0.0002) so now larger than generator learning rate 0.0002)\n",
    "- increase latent dimension to 128 (from 100)\n",
    "\n",
    "### RUN05\n",
    "\n",
    "general:\n",
    "- filter size: reviewing available tilesets, it became clear that most glyphs in the available tilesets are bigger than the current filter (5x5). So filter size from earlier filters (ie closer to the input/output image) were increased to [16x16], 12x12, 8x8 filters for the first/last 3 conv layers\n",
    "\n",
    "training:\n",
    "\n",
    "- based on Brock et al. (BigGAN) the training batch size was increased to 128\n",
    "- so far, different kinds of tilesets were mixed in with the training set. Now only the original ASCII-based tileset sample were retained (manual selection process)\n",
    "- tensorboard callback implemented\n",
    "\n",
    "generator:\n",
    "- increase initial layer dimensions to (4,4,2048)\n",
    "- added additional convolutional upscaling layer between first (4,4,2048) layer to (8,8,1024)\n",
    "- reactivated 64 filter layer between 64x64x128 and final 128x128x3 layer with kernel size 1\n",
    "- previously, the first convolutional layer in the generator model had a different kernel size (3) VS the other conv layers (5). Now all layers have the same kernel size = 5\n",
    "- momentum decrease to 0.9 (from 0.99)\n",
    "- batchnorm inserted after tanh() activation of last layer (before rescaling)\n",
    "\n",
    "critic:\n",
    "- added a dropout layer between flatten and output layer (last Dense(1) layer)\n",
    "\n",
    "\n",
    "### RUN06\n",
    "\n",
    "critic:\n",
    "- first kernel adjusted to size (10x12), the approximate size of default ASCII tokens\n",
    "- increased number of filters for first conv2d layer to 32-->64\n",
    "- try SpatialDropout2D() layer? --> https://keras.io/api/layers/regularization_layers/spatial_dropout2d/\n",
    "\n",
    "generator:\n",
    "- experimenting with last generator layer to ensure no overlapping windows. Possible with Conv2DTranspose???\n",
    "- added skip connection logic for additive skip and concatenate. For RUN06 this was concatenate skip connections for the last two layers were used\n",
    "- try layer norm along axis -1 (pixelwise normalization see ProGAN paper) for last generator normalization layer\n",
    "\n",
    "\n",
    "### RUN07\n",
    "\n",
    "critic:\n",
    "\n",
    "\n",
    "generator:\n",
    "- removed normalisation for last layer (was pixel-wise layernorm in previous run) --> interferred with colouring\n",
    "- replaced all other normalisations except for first layer with pixel-wise layer norm\n",
    "- kernel size changed to 3x3 in all Conv2DTranspose layers\n",
    "- TO TEST: remove skip-z?\n",
    "\n",
    "\n",
    "### RUN08\n",
    "\n",
    "general:\n",
    "- alternative architecture which produces 12x10 (changed to 12x12 to be symmetric) tiles instead of complete maps\n",
    "\n",
    "\n",
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "def discriminator_model():\n",
    "\n",
    "    # DISCRIMINATOR\n",
    "    # set input variables to variable width + height. Will be cropped in preprocessing [CURRENTLY FIXED TO 256x256]\n",
    "    input_dim = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "\n",
    "    # Input\n",
    "    d_input = Input(shape=input_dim, name='Discriminator_Input')\n",
    "\n",
    "    # ---- REMOVED FOR 256x256 NETWORK ----------\n",
    "    # Keras-based preprocessing. Alternative: RandomCrop()\n",
    "    # use smart_resizing?\n",
    "    #x = tf.keras.preprocessing.image.smart_resize(d_input, (1024, 1024))\n",
    "    #x = preprocessing.Resizing(width=512, \n",
    "    #                           height=512, \n",
    "    #                           name='Preprocessing_Resize'\n",
    "    #                          )(d_input) # Resize to 512 x 512 images\n",
    "\n",
    "    #x = preprocessing.RandomCrop(height=512, \n",
    "    #                            width=512, \n",
    "    #                            name = 'Preprocessing_RandomCrop'\n",
    "    #                           )(d_input)\n",
    "\n",
    "    x = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale'\n",
    "                               )(d_input) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    # Conv2D Layer 0\n",
    "    x = Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = (12,10), \n",
    "            strides = 1,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_0'\n",
    "    )(x)\n",
    "    \n",
    "    # Activation 0 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_0')(x)\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 1\n",
    "    x = Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_1'\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 1\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 1 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_1')(x)\n",
    "\n",
    "    # Dropout 1\n",
    "    x = Dropout(rate = DROPOUT_C)(x)\n",
    "\n",
    "\n",
    "    # Conv2D Layer 2\n",
    "    x = Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            name = 'Discriminator_Conv2D_Layer_2',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 2\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 2 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_2')(x)\n",
    "\n",
    "    # Dropout 2\n",
    "    x = Dropout(rate = DROPOUT_C)(x)\n",
    "\n",
    "\n",
    "    # Conv2D Layer 3\n",
    "    x = Conv2D(\n",
    "            filters = 256,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            name = 'Discriminator_Conv2D_Layer_3',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 3\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 3 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_3')(x)\n",
    "\n",
    "    # Dropout 3\n",
    "    x = Dropout(rate = DROPOUT_C)(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Conv2D Layer 4\n",
    "    x = Conv2D(filters = 512,\n",
    "               kernel_size = 5,\n",
    "               strides = 2,\n",
    "               padding = 'same',\n",
    "               name = 'Discriminator_Conv2D_Layer_4',\n",
    "               kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 4\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 4 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_4')(x)\n",
    "\n",
    "    # Dropout 4\n",
    "    x = Dropout(rate = DROPOUT_C)(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Conv2D Layer 5\n",
    "    x = Conv2D(filters = 512,\n",
    "               kernel_size = 5,\n",
    "               strides = 2,\n",
    "               padding = 'same',\n",
    "               name = 'Discriminator_Conv2D_Layer_5',\n",
    "               kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "    )(x)\n",
    "\n",
    "    # BatchNorm Layer 5\n",
    "    # x = BatchNormalization()(x)\n",
    "\n",
    "    # Activation 5 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_5')(x)\n",
    "\n",
    "    # Dropout 5\n",
    "    #x = Dropout(rate = 0.4)(x)\n",
    "\n",
    "\n",
    "    # OUTPUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(DROPOUT_C)(x)\n",
    "    d_output = Dense(1, \n",
    "                     #activation='sigmoid', \n",
    "                     kernel_initializer = RandomNormal(mean=0, stddev=0.02) # random initialization of weights with normal distribution around 0 with small SD\n",
    "                    )(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Discriminator Model intialization\n",
    "    discriminator = Model(d_input, d_output, name='Discriminator')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "'''\n",
    "\n",
    "def discriminator_model():\n",
    "\n",
    "    # DISCRIMINATOR\n",
    "    # set input variables to variable width + height. Will be cropped in preprocessing [CURRENTLY FIXED TO 256x256]\n",
    "    input_dim = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "\n",
    "    # Input\n",
    "    d_input = Input(shape=input_dim, name='Discriminator_Input')\n",
    "\n",
    "    # ---- REMOVED FOR 256x256 NETWORK ----------\n",
    "    # Keras-based preprocessing. Alternative: RandomCrop()\n",
    "    # use smart_resizing?\n",
    "    #x = tf.keras.preprocessing.image.smart_resize(d_input, (1024, 1024))\n",
    "    #x = preprocessing.Resizing(width=512, \n",
    "    #                           height=512, \n",
    "    #                           name='Preprocessing_Resize'\n",
    "    #                          )(d_input) # Resize to 512 x 512 images\n",
    "\n",
    "    #x = preprocessing.RandomCrop(height=12, \n",
    "    #                            width=12, \n",
    "    #                            name = 'Preprocessing_RandomCrop'\n",
    "    #                           )(d_input)\n",
    "\n",
    "    x = preprocessing.Rescaling(scale=1./127.5, \n",
    "                                offset=-1,\n",
    "                                name='Preprocessing_Rescale'\n",
    "                               )(d_input) # Rescale values from [0,255] to [-1,1] see https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n",
    "\n",
    "\n",
    "    # Conv2D Layer 0\n",
    "    x = Conv2D(\n",
    "            filters = 256,\n",
    "            kernel_size = 1, \n",
    "            strides = 1,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_0'\n",
    "    )(x)\n",
    "    \n",
    "    # Activation 0 - Leaky ReLU\n",
    "    x = LeakyReLU(alpha = RELU_SLOPE_C, name='Activation_0')(x)\n",
    "    \n",
    "    \n",
    "    # Conv2D Layer 1\n",
    "    x = Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            kernel_initializer = RandomNormal(mean=0., stddev=0.02),\n",
    "            name = 'Discriminator_Conv2D_Layer_1'\n",
    "    )(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(DROPOUT_C)(x)\n",
    "    d_output = Dense(1, \n",
    "                     #activation='sigmoid', \n",
    "                     kernel_initializer = RandomNormal(mean=0, stddev=0.02) # random initialization of weights with normal distribution around 0 with small SD\n",
    "                    )(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Discriminator Model intialization\n",
    "    discriminator = Model(d_input, d_output, name='Discriminator')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Discriminator_Input (InputLa [(None, 12, 12, 3)]       0         \n",
      "_________________________________________________________________\n",
      "Preprocessing_Rescale (Resca (None, 12, 12, 3)         0         \n",
      "_________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_0 (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "Activation_0 (LeakyReLU)     (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "Discriminator_Conv2D_Layer_1 (None, 6, 6, 128)         295040    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 4609      \n",
      "=================================================================\n",
      "Total params: 300,673\n",
      "Trainable params: 300,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = discriminator_model()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def generator_model():\n",
    "\n",
    "    # GENERATOR\n",
    "\n",
    "    # set input variable dimensions. Here we will start out with a vector of length 100 for each sample (sampled from a normal distribution, representing the learned latent space)\n",
    "    input_dim = (LATENT_DIM)\n",
    "\n",
    "    # Input\n",
    "    g_input = Input(shape=input_dim, name='Generator_Input')\n",
    "\n",
    "    # Dense Layer 1\n",
    "    x = Dense(np.prod([4,4,2048]), kernel_initializer = RandomNormal(mean=0., stddev=0.02), \n",
    "              use_bias=False)(g_input) # use_bias=False see https://keras.io/examples/generative/wgan_gp/\n",
    "\n",
    "    # Batch Norm Layer 1\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    \n",
    "    # Activation Layer 1\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Reshape into 3D tensor\n",
    "    x = Reshape((4,4,2048))(x)\n",
    "\n",
    "    # Upsampling Layer 1 + Conv2D Layer1\n",
    "    x = Conv2DTranspose(filters=1024, kernel_size=3, padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "    #x = UpSampling2D()(x)\n",
    "    #x = Conv2D(filters=64, kernel_size=5, padding='same', name='Generator_Conv_Layer_1')(x)\n",
    "    # Replacing by Conv2DTranspose Layer\n",
    "    x = Conv2DTranspose(filters=512, kernel_size=3, padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Upsampling Layer 2 + Conv2D Layer2\n",
    "    #x = UpSampling2D()(x)\n",
    "    #x = Conv2D(filters=128, kernel_size=5, padding='same', name='Generator_Conv_Layer_2')(x)\n",
    "    # Replacing by Conv2DTranspose Layer\n",
    "    x = Conv2DTranspose(filters=256, kernel_size=3, padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "\n",
    "    # Batch Norm Layer 3\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "\n",
    "    # Activation Layer 3\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Upsampling Layer 3 + Conv2D Layer3\n",
    "    #x = UpSampling2D()(x)\n",
    "    #x = Conv2D(filters=64, kernel_size=5, padding='same', name='Generator_Conv_Layer_3')(x)\n",
    "    # Replacing by Conv2DTranspose Layer\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=3, padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    # Batch Norm Layer 4\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "\n",
    "    # Activation Layer 4\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Upsampling Layer 4 + Conv2D Layer4\n",
    "    #x = UpSampling2D()(x)\n",
    "    #x = Conv2D(filters=64, kernel_size=5, padding='same', name='Generator_Conv_Layer_4')(x)\n",
    "    # Replacing by Conv2DTranspose Layer\n",
    "    \n",
    "    #removed for 128x128 pictures\n",
    "   \n",
    "    x = Conv2DTranspose(filters=64, kernel_size=3, padding='same', strides=1, \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    #### \n",
    "    # START\n",
    "    # SKIP-Z BLOCK 2\n",
    "    ####\n",
    "    \n",
    "    if SKIP_ADD:\n",
    "        skip = Dense(np.prod([64, 64, 64]), activation='sigmoid')(g_input)\n",
    "        skip = Reshape((64, 64, 64))(skip)\n",
    "\n",
    "        x = Add()([x, skip])\n",
    "        \n",
    "    elif SKIP_CONCAT:\n",
    "        # add initial vector, scale to layer size and reshape\n",
    "        skip = Dense(np.prod([64, 64, 64]))(g_input)\n",
    "        skip = Reshape((64, 64, 64))(skip)\n",
    "        \n",
    "        x = Concatenate()([x, skip]) #use tf.stack() ?\n",
    "        \n",
    "        # reduce output dimensions via 1x1 convolution\n",
    "        x = Conv2D(filters=64, kernel_size=3, padding='same', strides=(1,1),\n",
    "                  kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #### \n",
    "    # END\n",
    "    # SKIP-Z BLOCK 2\n",
    "    ####\n",
    "    \n",
    "\n",
    "    # Batch Norm Layer 5\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "\n",
    "    # Activation Layer 5\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #x = Conv2DTranspose(filters=3, kernel_size=7, padding='same', strides=(2,2), \n",
    "    #                    kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "                        \n",
    "                        \n",
    "    #x = Conv2DTranspose(filters=3, kernel_size=(1,1), strides=(12,10), \n",
    "    #                    kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # non-symmetric kernel with non-overlapping stride?\n",
    "    x = Conv2DTranspose(filters=3, kernel_size=3, padding='same', strides=(2,2), \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### \n",
    "    # START\n",
    "    # SKIP-Z BLOCK 3\n",
    "    ####\n",
    "    \n",
    "    if SKIP_ADD:\n",
    "        skip = Dense(np.prod([128, 128, 3]), activation='sigmoid')(g_input)\n",
    "        skip = Reshape((128, 128, 3))(skip)\n",
    "\n",
    "        x = Add()([x, skip])\n",
    "    elif SKIP_CONCAT:\n",
    "        # add initial vector, scale to layer size and reshape\n",
    "        skip = Dense(np.prod([128, 128, 3]))(g_input)\n",
    "        skip = Reshape((128, 128, 3))(skip)\n",
    "        \n",
    "        x = Concatenate()([x, skip]) #use tf.stack() ?\n",
    "        \n",
    "        # reduce output dimensions via 1x1 convolution\n",
    "        x = Conv2D(filters=3, kernel_size=3, padding='same', strides=(1,1),\n",
    "                  kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #### \n",
    "    # END\n",
    "    # SKIP-Z BLOCK 3\n",
    "    ####\n",
    "   \n",
    "\n",
    "    # Activation Layer 7\n",
    "    #x = LeakyReLU()(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "\n",
    "    # Upsampling Layer 8 + Conv2D Layer8\n",
    "    #x = Conv2D(filters=3, kernel_size=5, padding='same', name='Generator_Conv_Layer_7')(x)\n",
    "    # ---------------------------/\n",
    "    \n",
    "    # tanh activation layer to scale values to [-1:1]\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    # Batch Norm Layer 7\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "\n",
    "    # Output - Rescale Values back to [0:255] since the discriminator will automatically rescale back down to [-1:1] as part of the pre-processing pipeline\n",
    "    g_output = (255 / 2) * (x + 1) \n",
    "\n",
    "\n",
    "    # Generator Model initialization\n",
    "    generator = Model(g_input, g_output, name='Generator')\n",
    "    \n",
    "    \n",
    "    return generator\n",
    "'''\n",
    "\n",
    "def generator_model():\n",
    "    # GENERATOR\n",
    "\n",
    "    # set input variable dimensions. Here we will start out with a vector of length 100 for each sample (sampled from a normal distribution, representing the learned latent space)\n",
    "    input_dim = (LATENT_DIM)\n",
    "\n",
    "    # Input\n",
    "    g_input = Input(shape=input_dim, name='Generator_Input')\n",
    "\n",
    "    # Dense Layer 1\n",
    "    x = Dense(np.prod([4,4,512]), kernel_initializer = RandomNormal(mean=0., stddev=0.02), \n",
    "              use_bias=False)(g_input) # use_bias=False see https://keras.io/examples/generative/wgan_gp/\n",
    "\n",
    "    # Batch Norm Layer 1\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    \n",
    "    # Activation Layer 1\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "\n",
    "    # Reshape into 3D tensor\n",
    "    x = Reshape((4,4,512))(x)\n",
    "\n",
    "    # Upsampling Layer 1 + Conv2D Layer1\n",
    "    x = Conv2DTranspose(filters=512, kernel_size=3, padding='same', strides=3, \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "        # Upsampling Layer 1 + Conv2D Layer1\n",
    "    x = Conv2DTranspose(filters=256, kernel_size=3, padding='same', strides=1, \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Upsampling Layer 1 + Conv2D Layer1\n",
    "    x = Conv2DTranspose(filters=3, kernel_size=1, padding='same', strides=1, \n",
    "                        kernel_initializer = RandomNormal(mean=0., stddev=0.02), use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    # Batch Norm Layer 2\n",
    "    #x = BatchNormalization(momentum = MOMENTUM_G)(x)\n",
    "    #x = LayerNormalization()(x) # performs pixel-wise normalization across all channels\n",
    "    \n",
    "    # Activation Layer 2\n",
    "    #x = LeakyReLU(alpha=RELU_SLOPE_G)(x) # trying leaky ReLU instead of Activation('relu')(x)\n",
    "    \n",
    "    # tanh activation layer to scale values to [-1:1]\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    # Output - Rescale Values back to [0:255] since the discriminator will automatically rescale back down to [-1:1] as part of the pre-processing pipeline\n",
    "    g_output = (255 / 2) * (x + 1) \n",
    "\n",
    "\n",
    "    # Generator Model initialization\n",
    "    generator = Model(g_input, g_output, name='Generator')\n",
    "    \n",
    "    return generator\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Generator_Input (InputLayer) [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8192)              1048576   \n",
      "_________________________________________________________________\n",
      "layer_normalization_23 (Laye (None, 8192)              16384     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DT (None, 12, 12, 512)       2359296   \n",
      "_________________________________________________________________\n",
      "layer_normalization_24 (Laye (None, 12, 12, 512)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DT (None, 12, 12, 256)       1179648   \n",
      "_________________________________________________________________\n",
      "layer_normalization_25 (Laye (None, 12, 12, 256)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DT (None, 12, 12, 3)         768       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12, 12, 3)         0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_8 (TensorF [(None, 12, 12, 3)]       0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Mul_8 (TensorFlo [(None, 12, 12, 3)]       0         \n",
      "=================================================================\n",
      "Total params: 4,606,208\n",
      "Trainable params: 4,606,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SKIP_CONCAT = True\n",
    "#SKIP_ADD = True\n",
    "gen = generator_model()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP (Full) Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we compile the models, we need to implement a custom loss function which uses the Wasserstein distance and a gradient penalty term in order to ensure 1 Lipschitz constraints are followed. A WGAN with GP further involves a slightly more complicated training process which trains the critic (discriminator without sigmoid activation function) by feeding three different kinds of images:\n",
    "\n",
    "1. real images (i.e. available samples)\n",
    "2. 'fake' images (i.e. constructed by the generator)\n",
    "3. random interpolations between real and fake images (i.e. random samples interpolated from values between the fake and real images)\n",
    "\n",
    "The full training process of a critic is depicted below (source: Foster, 2019, p. 122):\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"wgan_gp_critic_training.png\"></img>\n",
    "    <i>Computational Graph for one Discriminator Training Epoch. (Source: Foster, 2019, p.122)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below roughly follows the OOP-based framework set by keras see https://keras.io/examples/generative/wgan_gp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope(): \n",
    "\n",
    "    critic = discriminator_model()\n",
    "    generator = generator_model()\n",
    "\n",
    "    class WGANGP(keras.Model):\n",
    "        def __init__(\n",
    "            self,\n",
    "            critic,\n",
    "            generator,\n",
    "            latent_dim,\n",
    "            tensorboard_callback,\n",
    "            critic_extra_steps=5,\n",
    "            gp_weight=10.0\n",
    "        ):\n",
    "            super(WGANGP, self).__init__()\n",
    "            self.critic = critic\n",
    "            self.generator = generator\n",
    "            self.latent_dim = latent_dim\n",
    "            self.tensorboard_callback = tensorboard_callback\n",
    "            self.d_steps = critic_extra_steps\n",
    "            self.gp_weight = gp_weight\n",
    "            \n",
    "\n",
    "        def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "            super(WGANGP, self).compile()\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.g_optimizer = g_optimizer\n",
    "            self.d_loss_fn = d_loss_fn\n",
    "            self.g_loss_fn = g_loss_fn\n",
    "\n",
    "        def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "            \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "            This loss is calculated on an interpolated image\n",
    "            and added to the discriminator loss.\n",
    "            \"\"\"\n",
    "            # Get the interpolated image\n",
    "            alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "            diff = fake_images - real_images\n",
    "            interpolated = real_images + alpha * diff\n",
    "\n",
    "            with tf.GradientTape() as gp_tape:\n",
    "                gp_tape.watch(interpolated)\n",
    "                # 1. Get the discriminator output for this interpolated image.\n",
    "                pred = self.critic(interpolated, training=True)\n",
    "\n",
    "            # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "            grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "            # 3. Calculate the norm of the gradients.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "            gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "            return gp\n",
    "\n",
    "        def train_step(self, real_images):\n",
    "            #checking whether we handed a tuple of (numpy) data to .fit().\n",
    "            #if not, the data must be a tf.data.Dataset generator that yields batches of datasets (data, labels)\n",
    "            if isinstance(real_images, tuple):\n",
    "                real_images = real_images[0]\n",
    "\n",
    "            # Get the batch size\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "            # For each batch, we are going to perform the\n",
    "            # following steps as laid out in the original paper:\n",
    "            # 1. Train the generator and get the generator loss\n",
    "            # 2. Train the discriminator and get the discriminator loss\n",
    "            # 3. Calculate the gradient penalty\n",
    "            # 4. Multiply this gradient penalty with a constant weight factor = self.discriminator_extra_steps = 5 (default value)\n",
    "            # 5. Add the gradient penalty to the discriminator loss\n",
    "            # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "            # Train the discriminator first. The original paper recommends training\n",
    "            # the discriminator for `x` more steps (typically 5) as compared to generator\n",
    "            \n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.critic)\n",
    "            \n",
    "            for i in range(self.d_steps):\n",
    "                # Get the latent vector\n",
    "                random_latent_vectors = tf.random.normal(\n",
    "                    shape=(batch_size, self.latent_dim)\n",
    "                )\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Generate fake images from the latent vector\n",
    "                    fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                    # Get the logits for the fake images\n",
    "                    fake_logits = self.critic(fake_images, training=True)\n",
    "                    # Get the logits for the real images\n",
    "                    real_logits = self.critic(real_images, training=True)\n",
    "\n",
    "                    # Calculate the discriminator loss using the fake and real image logits\n",
    "                    d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                    # Calculate the gradient penalty\n",
    "                    gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                    # Add the gradient penalty to the original discriminator loss\n",
    "                    d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "                # Get the gradients w.r.t the discriminator loss\n",
    "                d_gradient = tape.gradient(d_loss, self.critic.trainable_variables)\n",
    "                # Update the weights of the discriminator using the discriminator optimizer\n",
    "                self.d_optimizer.apply_gradients(\n",
    "                    zip(d_gradient, self.critic.trainable_variables)\n",
    "                )\n",
    "\n",
    "            # Train the generator\n",
    "            # set tensorboard model\n",
    "            self.tensorboard_callback.set_model(self.generator)\n",
    "            \n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images using the generator\n",
    "                generated_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the discriminator logits for fake images\n",
    "                gen_img_logits = self.critic(generated_images, training=True)\n",
    "                # Calculate the generator loss\n",
    "                g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "            # Get the gradients w.r.t the generator loss\n",
    "            gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "            # Update the weights of the generator using the generator optimizer\n",
    "            self.g_optimizer.apply_gradients(\n",
    "                zip(gen_gradient, self.generator.trainable_variables)\n",
    "            )\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "        \n",
    "    class GANMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, num_img=5, latent_dim=128):\n",
    "            self.num_img = num_img\n",
    "            self.latent_dim = latent_dim\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None): #on_epoch_end(self, epoch, logs=None):\n",
    "            '''\n",
    "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "            generated_images = self.model.generator(random_latent_vectors)\n",
    "            #generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "            for i in range(self.num_img):\n",
    "                img = generated_images[i].numpy()\n",
    "                img = keras.preprocessing.image.array_to_img(img)\n",
    "                img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "            '''\n",
    "            \n",
    "            # Sample generator output for num_img images\n",
    "            noise = np.random.normal(0, 1, (self.num_img, self.latent_dim))\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            #!!!NOT NECESSARY ANYMORE AS IMPLEMENTED AS PART OF THE MODEL!!!\n",
    "            #gen_imgs = 0.5 * (gen_imgs + 1)  #scale back to [0:1]\n",
    "            gen_imgs = gen_imgs.reshape((self.num_img, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "            # save n example images\n",
    "            for i in range(self.num_img):\n",
    "                fig = plt.figure(figsize=(10, 10))\n",
    "                plt.imshow(gen_imgs[i].astype('uint8'))\n",
    "                plt.title(f'Example Generator Output')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # adjust path based on whether execution is local or on linux VM\n",
    "                if pathlib.Path(f'{out_img_dir}/{model_name}').exists():\n",
    "                    fig.savefig(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png')\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    #mkdir\n",
    "                    os.mkdir(f'{out_img_dir}/{model_name}')\n",
    "                    #save\n",
    "                    fig.savefig(f'{out_img_dir}/{model_name}/sample_image_epoch{epoch+1}-{i+1}.png')\n",
    "                    plt.close()\n",
    "                    \n",
    "            # save corresponding model\n",
    "            now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "            \n",
    "            if pathlib.Path(f'{out_model_dir}/{model_name}').exists():\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5')        \n",
    "            else:\n",
    "                #make dir\n",
    "                os.mkdir(f'{out_model_dir}/{model_name}')\n",
    "                #write\n",
    "                #gan.save(f'{out_model_dir}/{model_name}/full-gan-{now}.h5')\n",
    "                #critic.save(f'{out_model_dir}/{model_name}/critic-{now}.h5') \n",
    "                generator.save(f'{out_model_dir}/{model_name}/generator-{now}.h5') \n",
    "        \n",
    "        \n",
    "    # Instantiate the optimizer for both networks\n",
    "    # (learning_rate=0.0002, beta_1=0.5 are recommended) as per Radford et al. 2016 pp. 3-4\n",
    "    generator_optimizer = Adam(\n",
    "        learning_rate=GEN_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "    critic_optimizer = Adam(\n",
    "        learning_rate=CRIT_LR, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def critic_loss(real_img, fake_img):\n",
    "        real_loss = tf.reduce_mean(real_img)\n",
    "        fake_loss = tf.reduce_mean(fake_img)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(fake_img):\n",
    "        return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "    # Instantiate the custome `GANMonitor` Keras callback.\n",
    "    cbk = GANMonitor(num_img=5, latent_dim=LATENT_DIM)\n",
    "    \n",
    "    # Instantiate the tensorboard tf.keras callback\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "    tb_cbk = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = f'{tboard_dir}/{model_name}_{now}', \n",
    "        write_graph = False,\n",
    "        write_images = True,\n",
    "        histogram_freq = 1) \n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGANGP(\n",
    "        critic=critic,\n",
    "        generator=generator,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        tensorboard_callback=tb_cbk,\n",
    "        critic_extra_steps=CRITIC_FACTOR,\n",
    "        gp_weight=GRADIENT_PENALTY_WEIGHT\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "        d_optimizer=critic_optimizer,\n",
    "        g_optimizer=generator_optimizer,\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=critic_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 2/95 [..............................] - ETA: 1:22 - d_loss: 9.4092 - g_loss: -0.4594WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1369s vs `on_train_batch_end` time: 1.6278s). Check your callbacks.\n",
      "95/95 [==============================] - 16s 170ms/step - d_loss: -995.8523 - g_loss: -530.9116\n",
      "Epoch 2/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -3187.9337 - g_loss: -2413.2217\n",
      "Epoch 3/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -2532.0000 - g_loss: -3705.3821\n",
      "Epoch 4/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -1119.7365 - g_loss: -6114.8227\n",
      "Epoch 5/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -4939.0529 - g_loss: -7876.1672\n",
      "Epoch 6/200\n",
      "95/95 [==============================] - 14s 143ms/step - d_loss: -6068.6594 - g_loss: -12799.7958\n",
      "Epoch 7/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -4236.1170 - g_loss: 160.1066\n",
      "Epoch 8/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: 2795.7059 - g_loss: -9558.1907\n",
      "Epoch 9/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -5797.3348 - g_loss: -15931.5420\n",
      "Epoch 10/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -5327.1466 - g_loss: 1628.0798\n",
      "Epoch 11/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: 449.6918 - g_loss: -11537.4056\n",
      "Epoch 12/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -5895.5453 - g_loss: -13559.4965\n",
      "Epoch 13/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -1748.8688 - g_loss: 1966.0192\n",
      "Epoch 14/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -3938.6184 - g_loss: -9487.6454\n",
      "Epoch 15/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -6159.4104 - g_loss: -4411.8692\n",
      "Epoch 16/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: 4048.2094 - g_loss: -4922.6350\n",
      "Epoch 17/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -540.1662 - g_loss: -238.2636\n",
      "Epoch 18/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -824.8290 - g_loss: -3002.3688\n",
      "Epoch 19/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -428.3086 - g_loss: 1460.0263\n",
      "Epoch 20/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -1036.8727 - g_loss: -921.2748\n",
      "Epoch 21/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -1622.1991 - g_loss: -2245.0867\n",
      "Epoch 22/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: 227.5526 - g_loss: -3162.3893\n",
      "Epoch 23/200\n",
      "95/95 [==============================] - 13s 141ms/step - d_loss: -441.8348 - g_loss: -390.5523\n",
      "Epoch 24/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: 392.9613 - g_loss: -3361.9500\n",
      "Epoch 25/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -2516.1585 - g_loss: -5474.6686\n",
      "Epoch 26/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -4106.0920 - g_loss: -6565.4392\n",
      "Epoch 27/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -3623.8509 - g_loss: -6932.0023\n",
      "Epoch 28/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -4172.8708 - g_loss: -11500.0028\n",
      "Epoch 29/200\n",
      "95/95 [==============================] - 13s 141ms/step - d_loss: -5916.2884 - g_loss: -6419.7516\n",
      "Epoch 30/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: 425.8094 - g_loss: -13270.8629\n",
      "Epoch 31/200\n",
      "95/95 [==============================] - 13s 135ms/step - d_loss: -4458.5634 - g_loss: -11449.9515\n",
      "Epoch 32/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -2562.6268 - g_loss: -14544.2205\n",
      "Epoch 33/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -1425.3029 - g_loss: -17883.6201\n",
      "Epoch 34/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -10650.4207 - g_loss: -13126.0588\n",
      "Epoch 35/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -6677.8574 - g_loss: -16460.9664\n",
      "Epoch 36/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -5906.3069 - g_loss: -15942.8894\n",
      "Epoch 37/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -4425.2193 - g_loss: -17609.8954\n",
      "Epoch 38/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -5692.6803 - g_loss: -19940.8156\n",
      "Epoch 39/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -5623.1023 - g_loss: -22414.7712\n",
      "Epoch 40/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -6149.1926 - g_loss: -21708.2644\n",
      "Epoch 41/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -6811.0756 - g_loss: -20269.7548\n",
      "Epoch 42/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -8100.6972 - g_loss: -20902.7431\n",
      "Epoch 43/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -7354.1864 - g_loss: -22630.4694\n",
      "Epoch 44/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: 235.7438 - g_loss: -31900.8236\n",
      "Epoch 45/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -463.7195 - g_loss: -32205.7376\n",
      "Epoch 46/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -5929.5360 - g_loss: -28230.5037\n",
      "Epoch 47/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -8599.4411 - g_loss: -26562.7624\n",
      "Epoch 48/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -9114.1070 - g_loss: -27374.9565\n",
      "Epoch 49/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -8439.3348 - g_loss: -29473.8101\n",
      "Epoch 50/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -8520.6902 - g_loss: -31057.3396\n",
      "Epoch 51/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -8197.8217 - g_loss: -33187.4358\n",
      "Epoch 52/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -8864.6720 - g_loss: -33906.8125\n",
      "Epoch 53/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -12031.1632 - g_loss: -32121.6271\n",
      "Epoch 54/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -21759.2061 - g_loss: -23620.6819\n",
      "Epoch 55/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -9691.8945 - g_loss: -36460.9887\n",
      "Epoch 56/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -7671.8291 - g_loss: -35223.5371\n",
      "Epoch 57/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -7984.5668 - g_loss: -31559.8079\n",
      "Epoch 58/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -14196.1939 - g_loss: -34398.7018\n",
      "Epoch 59/200\n",
      "95/95 [==============================] - 13s 135ms/step - d_loss: -8025.8882 - g_loss: -37761.4596\n",
      "Epoch 60/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -588.5129 - g_loss: -39760.1172\n",
      "Epoch 61/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: 379.3328 - g_loss: -15640.0772\n",
      "Epoch 62/200\n",
      "95/95 [==============================] - 13s 135ms/step - d_loss: -555.6705 - g_loss: 5014.1023\n",
      "Epoch 63/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -501.8830 - g_loss: 13022.3865\n",
      "Epoch 64/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -127.5849 - g_loss: -5056.9930\n",
      "Epoch 65/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -144.1086 - g_loss: -16835.3451\n",
      "Epoch 66/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -53.7455 - g_loss: -4544.5938\n",
      "Epoch 67/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -19.8425 - g_loss: -3655.2031\n",
      "Epoch 68/200\n",
      "95/95 [==============================] - 14s 147ms/step - d_loss: -269.1816 - g_loss: -9991.3164\n",
      "Epoch 69/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -142.9289 - g_loss: -13413.7009\n",
      "Epoch 70/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -430.9008 - g_loss: -6350.1927\n",
      "Epoch 71/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -222.8959 - g_loss: -4913.4256\n",
      "Epoch 72/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -387.8108 - g_loss: -16000.6330\n",
      "Epoch 73/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -200.0801 - g_loss: -17583.4200\n",
      "Epoch 74/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -388.2631 - g_loss: -0.1757\n",
      "Epoch 75/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -82.4607 - g_loss: 3803.2634\n",
      "Epoch 76/200\n",
      "95/95 [==============================] - 13s 141ms/step - d_loss: -603.9637 - g_loss: -10719.3761\n",
      "Epoch 77/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -377.1261 - g_loss: -16636.8617\n",
      "Epoch 78/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: 120.5764 - g_loss: 547.5482\n",
      "Epoch 79/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -528.8211 - g_loss: 18882.9197\n",
      "Epoch 80/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -262.1848 - g_loss: 4513.8737\n",
      "Epoch 81/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -813.6127 - g_loss: -5616.5801\n",
      "Epoch 82/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: 94.5617 - g_loss: 9122.8447\n",
      "Epoch 83/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -419.6747 - g_loss: 15727.3618\n",
      "Epoch 84/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -1012.4506 - g_loss: -314.7677\n",
      "Epoch 85/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: 38.8343 - g_loss: -3446.9523\n",
      "Epoch 86/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -482.7689 - g_loss: 16222.7456\n",
      "Epoch 87/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -292.5168 - g_loss: -2397.7012\n",
      "Epoch 88/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -1881.7971 - g_loss: 626.2396\n",
      "Epoch 89/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: 379.2914 - g_loss: -14562.2459\n",
      "Epoch 90/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -340.8269 - g_loss: 18467.1791\n",
      "Epoch 91/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -275.5005 - g_loss: 18253.4970\n",
      "Epoch 92/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -338.6461 - g_loss: 18669.0459\n",
      "Epoch 93/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -518.8922 - g_loss: 17508.5876\n",
      "Epoch 94/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -441.7857 - g_loss: 22048.1252\n",
      "Epoch 95/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -348.2904 - g_loss: 15946.9189\n",
      "Epoch 96/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -252.0142 - g_loss: 20705.4601\n",
      "Epoch 97/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -338.8180 - g_loss: 19418.4678\n",
      "Epoch 98/200\n",
      "95/95 [==============================] - 14s 147ms/step - d_loss: -622.2406 - g_loss: 15519.8988\n",
      "Epoch 99/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -172.6097 - g_loss: 23952.4964\n",
      "Epoch 100/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -300.7191 - g_loss: 22076.7612\n",
      "Epoch 101/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -270.2936 - g_loss: 33286.5282\n",
      "Epoch 102/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -268.5304 - g_loss: 42900.5739\n",
      "Epoch 103/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -511.1141 - g_loss: 42231.3424\n",
      "Epoch 104/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -235.9817 - g_loss: 42808.5488\n",
      "Epoch 105/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -280.0128 - g_loss: 44838.6670\n",
      "Epoch 106/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -397.9847 - g_loss: 53011.2376\n",
      "Epoch 107/200\n",
      "95/95 [==============================] - 14s 142ms/step - d_loss: -290.3947 - g_loss: 62614.5468\n",
      "Epoch 108/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -322.8725 - g_loss: 61679.0510\n",
      "Epoch 109/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -388.2801 - g_loss: 65077.8965\n",
      "Epoch 110/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -362.2154 - g_loss: 65532.7385\n",
      "Epoch 111/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -269.8854 - g_loss: 75608.6689\n",
      "Epoch 112/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -322.4362 - g_loss: 80096.5005\n",
      "Epoch 113/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -319.9262 - g_loss: 80991.5508\n",
      "Epoch 114/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -471.5067 - g_loss: 80202.6938\n",
      "Epoch 115/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -225.4498 - g_loss: 83372.0679\n",
      "Epoch 116/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -418.2195 - g_loss: 93926.1909\n",
      "Epoch 117/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -286.4843 - g_loss: 93665.4016\n",
      "Epoch 118/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -381.6359 - g_loss: 92929.0199\n",
      "Epoch 119/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -192.8027 - g_loss: 94841.6446\n",
      "Epoch 120/200\n",
      "95/95 [==============================] - 13s 141ms/step - d_loss: -247.5458 - g_loss: 97767.7170\n",
      "Epoch 121/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -222.8102 - g_loss: 102491.8695\n",
      "Epoch 122/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -236.5476 - g_loss: 104288.8328\n",
      "Epoch 123/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -317.7569 - g_loss: 104263.3525\n",
      "Epoch 124/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -270.1469 - g_loss: 110535.4989\n",
      "Epoch 125/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -210.9586 - g_loss: 116111.9237\n",
      "Epoch 126/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -144.1271 - g_loss: 116441.2188\n",
      "Epoch 127/200\n",
      "95/95 [==============================] - 13s 141ms/step - d_loss: -191.2479 - g_loss: 117176.9194\n",
      "Epoch 128/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -197.1704 - g_loss: 123370.2292\n",
      "Epoch 129/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -177.4697 - g_loss: 125917.4393\n",
      "Epoch 130/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -59.6905 - g_loss: 127214.4957\n",
      "Epoch 131/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -180.2326 - g_loss: 128194.9185\n",
      "Epoch 132/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -237.5076 - g_loss: 129593.8763\n",
      "Epoch 133/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -323.9916 - g_loss: 131873.6324\n",
      "Epoch 134/200\n",
      "95/95 [==============================] - 14s 150ms/step - d_loss: -255.1960 - g_loss: 132594.0442\n",
      "Epoch 135/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -195.0924 - g_loss: 130592.6781\n",
      "Epoch 136/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -51.9751 - g_loss: 130032.6299\n",
      "Epoch 137/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -305.3714 - g_loss: 128154.6711\n",
      "Epoch 138/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -222.4379 - g_loss: 126695.3168\n",
      "Epoch 139/200\n",
      "95/95 [==============================] - 13s 141ms/step - d_loss: -303.8880 - g_loss: 126896.5419\n",
      "Epoch 140/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -212.1286 - g_loss: 127278.5994\n",
      "Epoch 141/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -281.8141 - g_loss: 125490.7804\n",
      "Epoch 142/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -249.3187 - g_loss: 125008.8798\n",
      "Epoch 143/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -198.7267 - g_loss: 125110.7824\n",
      "Epoch 144/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -184.8380 - g_loss: 125302.9568\n",
      "Epoch 145/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -275.4660 - g_loss: 123709.9714\n",
      "Epoch 146/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -207.9729 - g_loss: 123525.1348\n",
      "Epoch 147/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -239.9014 - g_loss: 123980.1909\n",
      "Epoch 148/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -243.9363 - g_loss: 123910.8645\n",
      "Epoch 149/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -212.7094 - g_loss: 123835.3735\n",
      "Epoch 150/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -234.7013 - g_loss: 126088.2143\n",
      "Epoch 151/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -149.4569 - g_loss: 126106.7963\n",
      "Epoch 152/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -160.6994 - g_loss: 125719.2905\n",
      "Epoch 153/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -162.2185 - g_loss: 125576.1101\n",
      "Epoch 154/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -221.7328 - g_loss: 126577.7337\n",
      "Epoch 155/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -224.8063 - g_loss: 124284.5215\n",
      "Epoch 156/200\n",
      "95/95 [==============================] - 13s 142ms/step - d_loss: -157.7780 - g_loss: 124127.6923\n",
      "Epoch 157/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -143.3565 - g_loss: 125595.0702\n",
      "Epoch 158/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -193.1276 - g_loss: 123041.7065\n",
      "Epoch 159/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -105.6840 - g_loss: 124046.8579\n",
      "Epoch 160/200\n",
      "95/95 [==============================] - 13s 141ms/step - d_loss: -178.5102 - g_loss: 124008.1932\n",
      "Epoch 161/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -193.5977 - g_loss: 123247.9284\n",
      "Epoch 162/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -21.9989 - g_loss: 122784.8971\n",
      "Epoch 163/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -249.9417 - g_loss: 122738.0978\n",
      "Epoch 164/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -105.5561 - g_loss: 124756.6582\n",
      "Epoch 165/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -232.6909 - g_loss: 126464.3407\n",
      "Epoch 166/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -155.8758 - g_loss: 127769.3547\n",
      "Epoch 167/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -59.8367 - g_loss: 127178.2945\n",
      "Epoch 168/200\n",
      "95/95 [==============================] - 13s 135ms/step - d_loss: -81.7744 - g_loss: 125184.3865\n",
      "Epoch 169/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -47.2761 - g_loss: 123994.9998\n",
      "Epoch 170/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -258.4048 - g_loss: 123462.1558\n",
      "Epoch 171/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -230.6958 - g_loss: 120788.2133\n",
      "Epoch 172/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -158.6452 - g_loss: 120920.0313\n",
      "Epoch 173/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -369.5657 - g_loss: 121804.1543\n",
      "Epoch 174/200\n",
      "95/95 [==============================] - 13s 141ms/step - d_loss: -139.2696 - g_loss: 123306.4340\n",
      "Epoch 175/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -168.9859 - g_loss: 122425.1787\n",
      "Epoch 176/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -214.3660 - g_loss: 121386.0111\n",
      "Epoch 177/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -97.4476 - g_loss: 121606.9200\n",
      "Epoch 178/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -281.3941 - g_loss: 122083.0293\n",
      "Epoch 179/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -166.9246 - g_loss: 121305.8424\n",
      "Epoch 180/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -225.5050 - g_loss: 119673.0164\n",
      "Epoch 181/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -152.5320 - g_loss: 118306.4049\n",
      "Epoch 182/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -180.1329 - g_loss: 119251.4937\n",
      "Epoch 183/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -249.6505 - g_loss: 119733.5334\n",
      "Epoch 184/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -96.3474 - g_loss: 119360.9831\n",
      "Epoch 185/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -192.5064 - g_loss: 119680.6688\n",
      "Epoch 186/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -136.9351 - g_loss: 120439.7537\n",
      "Epoch 187/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -74.2044 - g_loss: 120098.0247\n",
      "Epoch 188/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -183.4044 - g_loss: 119040.5229\n",
      "Epoch 189/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -78.1540 - g_loss: 119602.2817\n",
      "Epoch 190/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -187.0516 - g_loss: 119110.0939\n",
      "Epoch 191/200\n",
      "95/95 [==============================] - 13s 136ms/step - d_loss: -132.9451 - g_loss: 118102.9501\n",
      "Epoch 192/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -232.7049 - g_loss: 118006.8690\n",
      "Epoch 193/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -115.3454 - g_loss: 118386.5175\n",
      "Epoch 194/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -311.8026 - g_loss: 117480.9662\n",
      "Epoch 195/200\n",
      "95/95 [==============================] - 13s 140ms/step - d_loss: -314.2137 - g_loss: 117149.7715\n",
      "Epoch 196/200\n",
      "95/95 [==============================] - 13s 139ms/step - d_loss: -213.5174 - g_loss: 117562.9910\n",
      "Epoch 197/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -233.2696 - g_loss: 116978.8416\n",
      "Epoch 198/200\n",
      "95/95 [==============================] - 13s 141ms/step - d_loss: -169.8313 - g_loss: 114385.0990\n",
      "Epoch 199/200\n",
      "95/95 [==============================] - 13s 137ms/step - d_loss: -158.6101 - g_loss: 115211.4917\n",
      "Epoch 200/200\n",
      "95/95 [==============================] - 13s 138ms/step - d_loss: -188.9988 - g_loss: 116800.0692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f53084cfc50>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training the model.\n",
    "wgan.fit(train_ds, batch_size=BATCH_SIZE, epochs=200, callbacks=[cbk, tb_cbk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Examples using learned Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 11.5, 11.5, -0.5)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load generator\n",
    "'''\n",
    "generator.compile(optimizer=Adam(lr=0.0008), # per Foster, 2017 RMSprop(lr=0.0008)\n",
    "                          loss=binary_crossentropy,\n",
    "                          metrics=['accuracy'])\n",
    "generator = tf.keras.models.load_model('/data/output/models/dwarfganWGANGPR02/generator-2021-04-04_025322.h5')\n",
    "'''\n",
    "# generate new example of learned representation in latent space\n",
    "try:\n",
    "    generator\n",
    "except NameError:\n",
    "    #get latest generator model save file\n",
    "    folder = pathlib.Path(f'{out_model_dir}/{model_name}')\n",
    "    saves = list(folder.glob('generator*'))\n",
    "    latest = max(saves, key=os.path.getctime)\n",
    "    #load latest generator save file\n",
    "    generator = tf.keras.models.load_model(latest)\n",
    "        \n",
    "noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "res = np.array(generator(noise, training=False)).astype('uint8')\n",
    "\n",
    "#Rescale\n",
    "res = res.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Visualize result\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(res)\n",
    "plt.title(f'Example Generator Output')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes made after 10 epoche run (V0.0)\n",
    "---\n",
    "- <img src=\"https://cdn1.iconfinder.com/data/icons/warnings-and-dangers/400/Warning-02-512.png\" width=20></img> kernel initializer in discriminator Conv2D layers set to RandomNormal(mean=0., stddev=0.02) (before default value = glorot_uniform aka Xavier)\n",
    "\n",
    "### Changes to be made after 100 epoche run (for V0.5)\n",
    "---\n",
    "- Batch size 32 --> 64. Potentially use increasing batch size (rather than decay of learning rate) see: https://stackoverflow.com/questions/50606995/how-to-change-the-batch-size-during-training  --> might lead to memory problems in current 44GB RAM VM\n",
    "- Dynamically increase batch size instead of decaying learning rate to accelarate training see https://arxiv.org/abs/1711.00489\n",
    "- check some filters on different Conv2D layers --> use smaller filter to detect relatively small features?\n",
    "- adjust filter size for some layers (5x5 -> 10x10?) --> shrinking size too fast is not helpful. maybe add additional conv layers to shrink more slowely?\n",
    "- <img src=\"https://cdn1.iconfinder.com/data/icons/warnings-and-dangers/400/Warning-02-512.png\" width=20></img> Discriminator Optimizer Adam -> RMSprop\n",
    "- <img src=\"https://cdn1.iconfinder.com/data/icons/warnings-and-dangers/400/Warning-02-512.png\" width=20></img> Added discriminator.trainable = True/False flags to training functions for discriminator/generator (GAN) to ensure discriminator is actually trainable\n",
    "- <img src=\"https://cdn1.iconfinder.com/data/icons/warnings-and-dangers/400/Warning-02-512.png\" width=20></img> So far only 3652 images were used for training (single image maps and map archives starting with X, Y or Z). The next training cycle will include maps starting with U, V or W --> added all available samples\n",
    "- <img src=\"https://cdn1.iconfinder.com/data/icons/warnings-and-dangers/400/Warning-14-512.png\" width=20></img> add 'sample_img()' function to execute every 'sample_interval' - added but not tested\n",
    "- <img src=\"https://cdn1.iconfinder.com/data/icons/warnings-and-dangers/400/Warning-14-512.png\" width=20></img> tune polarization method in input data pipeline which scales pictures to 1024 x 1024? OR randomly crop a 1024 x 1024 piece of the input image to better preserve size of objects rather than scaling everything to 1024x1024 ==> random crop changed to 512x512 due to OOM error in training on GPUs\n",
    "==> pre-processing applied to input 100k input images of different size: random cropped 10 1024x1024 regions and dropping all black etc. images, generating over 700k cropped samples\n",
    "- <img src=\"https://cdn1.iconfinder.com/data/icons/warnings-and-dangers/400/Warning-02-512.png\" width=20></img> implement tensorboard for model monitoring during training\n",
    "- try zero centering (mean subtracting) input data?\n",
    "- use Randomized ReLU instead of leaky see: https://arxiv.org/pdf/1505.00853.pdf\n",
    "- add another loop in train_gan() over different steps within training loop?\n",
    "- <img src=\"https://cdn1.iconfinder.com/data/icons/warnings-and-dangers/400/Warning-02-512.png\" width=20></img> add slight randomness to labels (see Chollet p. 307)\n",
    "- implement wasserstein GAN with gradient penalty (WGAN-GP, see  Foster p. 115 ff)\n",
    "- use Frechet Inception Distance (FIP) for evaluation of GAN performance (see https://arxiv.org/abs/1706.08500 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes - Besprechung Vorstudie 15.01.2021\n",
    "\n",
    "- Ausdrücke aus der Game Welt evtl. näher erläutern erläutern bspw. \"Game Assets\"\n",
    "- Spezifizierung Forschungsfragen --> Ziele etwas zu offen bspw. was heisst \"sufficiently able to learn\" --> include Frechet Inception Distance (FIP)\n",
    "    -- konkretisieren\n",
    "    -- 2 Milestones (konkret) formulieren für Ergebnis mit Termin pro Milestones für Vorstellung dieser Milestones (mit Thomas Koller im CC inkl. Vorstudie) --> Für Herrn Birbaumer am liebsten <b>Donnerstag Nachmittag</b>:\n",
    "        --- Milestone 1: Frechet Distance für ersten milestone 1 + WGAN-GP? Einführung nochmals Vorstudie als überblick\n",
    "        --- Milestone 2: ???\n",
    "- Bewertung von Games / Vergleichsstudie --> wie können Game Devs dazu bewogen werden diese zu verwenden, was sind das für Kriterien\n",
    "- 2-3 Wochen vor Abgabe \"Abschlusspresentation\" einplanen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
