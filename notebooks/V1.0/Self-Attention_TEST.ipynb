{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Attention, Input, Conv2D, Flatten, Dense, Multiply, Activation, Lambda, Permute, RepeatVector, merge\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1]\n",
      "   [1]\n",
      "   [1]\n",
      "   [1]]\n",
      "\n",
      "  [[1]\n",
      "   [5]\n",
      "   [5]\n",
      "   [1]]\n",
      "\n",
      "  [[1]\n",
      "   [1]\n",
      "   [1]\n",
      "   [1]]]\n",
      "\n",
      "\n",
      " [[[1]\n",
      "   [1]\n",
      "   [1]\n",
      "   [1]]\n",
      "\n",
      "  [[1]\n",
      "   [5]\n",
      "   [5]\n",
      "   [1]]\n",
      "\n",
      "  [[1]\n",
      "   [1]\n",
      "   [1]\n",
      "   [1]]]]\n"
     ]
    }
   ],
   "source": [
    "img_batch = np.array([[\n",
    "                [1,1,1,1],\n",
    "                [1,5,5,1],\n",
    "                [1,1,1,1]\n",
    "                ],\n",
    "                [\n",
    "                [1,1,1,1],\n",
    "                [1,5,5,1],\n",
    "                [1,1,1,1]\n",
    "                ]]\n",
    "            )\n",
    "img_batch = img_batch.reshape((2,3,4,1))\n",
    "print(img_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention based implementation with Attention() layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 3, 4, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 3, 4, 1)      2           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 3, 4, 1)      2           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 3, 4, 1)      2           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 3, 4, 1)      1           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 3, 4, 1)      0           attention_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 7\n",
      "Trainable params: 7\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(3,4,1))\n",
    "\n",
    "query = Conv2D(filters=1, kernel_size=1, use_bias=False)(inp) #filters = in_dimension//8\n",
    "key = Conv2D(filters=1, kernel_size=1, use_bias=False)(inp) #filters = in_dimension//8 ==> TRANSPOSE per Torrado et al. 2019? --> transpose seems not necessary as the matmul() operation within Attention() appears to transpose automatically. Paper seems to transpose QUERY rather than KEY!!\n",
    "value = Conv2D(filters=1, kernel_size=1, use_bias=False)(inp)\n",
    "\n",
    "out = Attention(use_scale=True)([query, key, value]) #scale adds a learnable parameter applied to the attention scores\n",
    "out = Activation(\"sigmoid\")(out) #necessary? att 1x1 conv?\n",
    "test = Model(inp, out)\n",
    "\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Closer to Zhang et al. 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 3, 4, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 3, 4, 1)      2           input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 3, 4, 1)      2           input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 3, 4, 1)      2           input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 3, 4, 1)      1           conv2d_46[0][0]                  \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 3, 4, 1)      2           attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lin_scaler_6 (LinScaler)        (None, 3, 4, 1)      1           conv2d_48[0][0]                  \n",
      "                                                                 input_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class LinScaler(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(LinScaler, self).__init__()\n",
    "        self.scale = tf.Variable(trainable=True, name='AttentionMap_ScaleFactor', initial_value=0.)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        att_map = inputs[0]\n",
    "        att_input = inputs[1]\n",
    "        return self.scale * att_map + att_input\n",
    "\n",
    "inp = Input(shape=(3,4,1))\n",
    "\n",
    "#linear combinations implemented as convolutional layer (input to self-attention layer)\n",
    "query = Conv2D(filters=inp.shape[-1]/8, kernel_size=1)(inp) #filters = in_dimension//8\n",
    "key = Conv2D(filters=inp.shape[-1]/8, kernel_size=1)(inp) #filters = in_dimension//8 ==> TRANSPOSE per Torrado et al. 2019? --> transpose seems not necessary as the matmul() operation within Attention() appears to transpose the second parameters automatically. Paper seems to transpose QUERY rather than KEY!!\n",
    "value = Conv2D(filters=inp.shape[-1], kernel_size=1)(inp)\n",
    "\n",
    "#scale necessary? not necessarily equal to scale parameter in Zhang et al.?\n",
    "out = Attention(use_scale=True)([key, query, value]) #switched key + query since the second parameter gets transposed (query in Torrado et al.)\n",
    "out = Conv2D(filters=out.shape[-1], kernel_size=1)(out) #output of Attention layer\n",
    "\n",
    "#scaling and adding initial feature map\n",
    "#scale = tf.Variable(trainable=True, name='AttentionMap_ScaleFactor', initial_value=0.) #this variable does not appear to be tracked --> create custom layer by subclassing tf.keras.layers.Layer: https://keras.io/api/layers/core_layers/lambda/\n",
    "#out = scale * out + inp\n",
    "out = LinScaler()([out, inp])\n",
    "\n",
    "test = Model(inp, out)\n",
    "\n",
    "test.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABOVE DOES NOT WORK DUE TO TRANSPOSE / MATMUL NOT BEING COMMUTATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom layer implementation self-attention\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim):\n",
    "        super(LinScaler, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.k = 8\n",
    "        self.scale = tf.Variable(trainable=True, name='AttentionMap_ScaleFactor', initial_value=0.)\n",
    "        \n",
    "        # linear combination of input to query f(x), key g(x) and value h(x)\n",
    "        self.query = Conv2D(filters=self.dim[-1]/self.k, kernel_size=1)(inputs) #filters = in_dimension//8\n",
    "        self.key = Conv2D(filters=self.dim[-1]/self.k, kernel_size=1)(inputs) #filters = in_dimension//8 ==> TRANSPOSE per Torrado et al. 2019? --> transpose seems not necessary as the matmul() operation within Attention() appears to transpose the second parameters automatically. Paper seems to transpose QUERY rather than KEY!!\n",
    "        self.value = Conv2D(filters=self.dim[-1], kernel_size=1)(inputs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "               \n",
    "        # matmul transposed query with key\n",
    "        p_query = tf.transpose(self.query, perm=[0,2,1]))\n",
    "        attention = K.batch_dot(p_query, self.key)\n",
    "        attention = Activation('sigmoid')(attention)\n",
    "        \n",
    "        out = K.batch_dot(self.value, tf.transpose(attention, perm=[0,2,1]))\n",
    "        \n",
    "        return self.scale * out + self.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-headed attention with query, key and value are the same\n",
    "== self-attent see https://keras.io/api/layers/attention_layers/multi_head_attention/\n",
    "\n",
    "--> only available in tf 2.4.1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4, 1), dtype=float32, numpy=\n",
       "array([[[[0.18451074],\n",
       "         [0.18451074],\n",
       "         [0.18451074],\n",
       "         [0.18451074]],\n",
       "\n",
       "        [[0.00062261],\n",
       "         [0.0005926 ],\n",
       "         [0.0005926 ],\n",
       "         [0.00062261]],\n",
       "\n",
       "        [[0.18451074],\n",
       "         [0.18451074],\n",
       "         [0.18451074],\n",
       "         [0.18451074]]],\n",
       "\n",
       "\n",
       "       [[[0.18451074],\n",
       "         [0.18451074],\n",
       "         [0.18451074],\n",
       "         [0.18451074]],\n",
       "\n",
       "        [[0.00062261],\n",
       "         [0.0005926 ],\n",
       "         [0.0005926 ],\n",
       "         [0.00062261]],\n",
       "\n",
       "        [[0.18451074],\n",
       "         [0.18451074],\n",
       "         [0.18451074],\n",
       "         [0.18451074]]]], dtype=float32)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"repeat_vector_28/Tile:0\", shape=(None, 256, 12), dtype=float32)\n",
      "Tensor(\"permute_26/transpose:0\", shape=(None, 12, 256), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Operands could not be broadcast together with shapes (3, 4, 1) (12, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-bf31298b4b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# multiply weight with lstm layer o/p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# Get the attention adjusted output state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 926\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2644\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m       \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_elemwise_op_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;31m# If the inputs have different ranks, we have to reshape them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# to make them broadcastable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/merge.py\u001b[0m in \u001b[0;36m_compute_elemwise_op_output_shape\u001b[0;34m(self, shape1, shape2)\u001b[0m\n\u001b[1;32m     83\u001b[0m           raise ValueError(\n\u001b[1;32m     84\u001b[0m               \u001b[0;34m'Operands could not be broadcast '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m               'together with shapes ' + str(shape1) + ' ' + str(shape2))\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moutput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operands could not be broadcast together with shapes (3, 4, 1) (12, 256)"
     ]
    }
   ],
   "source": [
    "# checking against manual implementation\n",
    "input1 = Input(shape=(3,4,1))\n",
    "e=Dense(1, activation='tanh')(input1)\n",
    "# Now do all the softmax business taking the above o/p\n",
    "e=Flatten()(e)\n",
    "a=Activation('softmax')(e)\n",
    "temp=RepeatVector(256)(a)\n",
    "print(temp)\n",
    "temp=Permute([2,1])(temp)\n",
    "print(temp)\n",
    "# multiply weight with lstm layer o/p\n",
    "output = merge.Multiply()([input1, temp])\n",
    "# Get the attention adjusted output state\n",
    "output = Lambda(lambda values: K.sum(values, axis=1))(output)\n",
    "\n",
    "test2 = Model(input1, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
