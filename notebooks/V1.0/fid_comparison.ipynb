{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and Compare FID Scores of Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from numpy import cov\n",
    "#from numpy import trace\n",
    "#from numpy import iscomplexobj\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define FID function\n",
    "\n",
    "based on https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images, reference='fid_reference_values'):    # calculate activations for images to compare to established baseline\n",
    "    act = model.predict(images)\n",
    "    # calculate mean and covariance statistics\n",
    "    with open(reference, 'rb') as f:\n",
    "        mu1 = pickle.load(f)\n",
    "        sigma1 = pickle.load(f)\n",
    "        mu2, sigma2 = act.mean(axis=0), np.cov(act, rowvar=False)\n",
    "        # calculate sum squared difference between means\n",
    "        ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "        # calculate sqrt of product between cov\n",
    "        covmean = sqrtm(sigma1.dot(sigma2))\n",
    "        # check and correct imaginary numbers from sqrt\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        # calculate score\n",
    "        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    \n",
    "    return fid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare random generated images (DCGAN) against baseline\n",
    "\n",
    "We load a random subset of generated images from a previous run into a tensor and run that through the calculate_fid() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load generated images\n",
    "IMAGE_SIZE = (299, 299) # here we specify the expected input size of Inception V3 to let image_dataset_from_directory() automatically resize the images\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_dir = pathlib.Path('/data/output/images/dwarfgan001')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "check = tf.keras.preprocessing.image_dataset_from_directory(  data_dir,\n",
    "                                                                      image_size=IMAGE_SIZE, \n",
    "                                                                      batch_size=BATCH_SIZE, \n",
    "                                                                      #labels=[0.] * len(imgs), # setting all labels to 0 (for 'fake'), not relevant here\n",
    "                                                                      #label_mode=None, # yields float32 type labels\n",
    "                                                                      seed=42,\n",
    "                                                                      validation_split=0.99, #only 20 images available but split has to be < 1 \n",
    "                                                                      subset='validation'\n",
    "                                                                    )\n",
    "\n",
    "result_dcgan = calculate_fid(model, check)\n",
    "print(f'We see that the result is quite large with a FID score of: {round(result_dcgan,2)}. A perfect imitation would score a FID score close to 0.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FID for Real Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load real images\n",
    "IMAGE_SIZE = (299, 299) # here we specify the expected input size of Inception V3 to let image_dataset_from_directory() automatically resize the images\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_dir = pathlib.Path('/data/input/crops_small/')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "check = tf.keras.preprocessing.image_dataset_from_directory(  data_dir,\n",
    "                                                              image_size=IMAGE_SIZE, \n",
    "                                                              batch_size=BATCH_SIZE, \n",
    "                                                              #labels=[0.] * len(imgs), # setting all labels to 0 (for 'fake'), not relevant here\n",
    "                                                              #label_mode=None, # yields float32 type labels\n",
    "                                                              seed=42,\n",
    "                                                              validation_split=0.025, #only 2.5% of 700'000 images as reference \n",
    "                                                              subset='validation'\n",
    "                                                            )\n",
    "\n",
    "result_real = calculate_fid(model, check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Here we see a much lower FID score of: {round(result_real,2)}. Due to the variety of pictures, a score of 0 is unlikely.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare WGAN-GP RUN02 Images to Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load real images\n",
    "IMAGE_SIZE = (299, 299) # here we specify the expected input size of Inception V3 to let image_dataset_from_directory() automatically resize the images\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_dir = '/data/output/images/WGANGPR02FID/'\n",
    "#imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "check = tf.keras.preprocessing.image_dataset_from_directory(  data_dir,\n",
    "                                                              image_size=IMAGE_SIZE, \n",
    "                                                              batch_size=BATCH_SIZE, \n",
    "                                                              #labels=[0.] * len(imgs), # setting all labels to 0 (for 'fake'), not relevant here\n",
    "                                                              #label_mode=None, # yields float32 type labels\n",
    "                                                              seed=42\n",
    "                                                              #validation_split=0.025, #only 2.5% of 700'000 images as reference \n",
    "                                                              #subset='validation'\n",
    "                                                            )\n",
    "\n",
    "result_wgangp = calculate_fid(model, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Here we see a higher FID score of: {round(result_wgangp,2)} compared to the initial FID score of {round(result_dcgan,2)} from the DCGAN model and {round(result_real, 2)} of the real image reference score.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
