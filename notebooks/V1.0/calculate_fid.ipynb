{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Fréchet Inception Distance (FID) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#from numpy import cov\n",
    "#from numpy import trace\n",
    "#from numpy import iscomplexobj\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FID\n",
    "\n",
    "The FID score was introduced by Heusel et al. (2017, see: https://arxiv.org/abs/1706.08500 ) in order to improve on the currently established inception score (IS) for the evaluation of image generation DL methods, specifically when applying GAN architectures. FID - in comparison to IS - has the ability to evaluate the quality of generated images by comparing a statistical distribution of a latent representation feature vector based on the InceptionV3 model with the same statistical distribution of the original images.\n",
    "\n",
    "FID achieves this by using the Fréchet distance between the two distributions representing the real and generated images as follows:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"FID.PNG\"></img><br>\n",
    "    <i>source: Heusel et al, 2017, p.11 adapted by Hui, 2018 (see: https://jonathan-hui.medium.com/gan-how-to-measure-gan-performance-64b988c47732)</i>\n",
    "</div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "where:<br>\n",
    "<pre>\n",
    "    x = real images<br>\n",
    "    g = generated images<br>\n",
    "    µx = mean of the multivariate Gaussian distribution representing the latent vector of real images<br>\n",
    "    µg = mean of the multivariate Gaussian distribution representing the latent vector of generated images<br>\n",
    "    Σx = covariance matrix for real images<br>\n",
    "    Σg = covariance matrix for generated images<br>\n",
    "    Tr() = the trace() function defined as the sum of diagonal elements of a square matrix<br>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, calculating a FID score for two sets of images requires the following steps:\n",
    "\n",
    "1. Load <i>Inception V3</i> model\n",
    "2. Modify <i>Inception V3</i> so that we discard the output layer (classification into image categories), only keeping a max pooling layer representing the latent space vector.\n",
    "3. Calculate the <i>mean (µ)</i> and <i>variance (Σ)</i> based on the latent space vector of the real (training) images\n",
    "4. Generate images (here using a GAN)\n",
    "5. Calculate the <i>mean (µ)</i> and <i>variance (Σ)</i> based on the latent space vector of the generated (\"fake\") images\n",
    "6. Calculate FID based on Fréchet Distance between the two statistical distributions\n",
    "\n",
    "The process for calculating FID applied to this use case is shown in the graphic below (own depiction):\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"FID_process.png\"></img>\n",
    "<i>FID Computational Graph. (Source: own depiction)</i>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "NOTE: due to out-of-date code provided by the paper's authors, FID score calculation has been implemented from the ground up roughly following the following implementation: https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Inception V3 and Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3)) #Inception V3 expects 299 x 299 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Real Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 726300 files belonging to 1 classes.\n",
      "Using 72630 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# load training dataset (full)\n",
    "IMAGE_SIZE = (299, 299) # here we specify the expected input size of Inception V3 to let image_dataset_from_directory() automatically resize the images\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_dir = pathlib.Path('/data/input/crops')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "#we split out a subset of about 10 percent of the samples as scoring 700'000+ images with FID will take a very long time\n",
    "dataset_train = tf.keras.preprocessing.image_dataset_from_directory(  '/data/input/',\n",
    "                                                                      image_size=IMAGE_SIZE, \n",
    "                                                                      batch_size=BATCH_SIZE, \n",
    "                                                                      labels=[1.] * len(imgs), # setting all labels to 1 (for 'real')\n",
    "                                                                      #label_mode=None, # yields float32 type labels\n",
    "                                                                      seed=42,\n",
    "                                                                      validation_split=0.1,\n",
    "                                                                      subset='validation'\n",
    "                                                                    )\n",
    "\n",
    "#NOTE: we do not need to worry about the dtype of our image data as the above will cast it to float32, as expected by Inception V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Distribution Parameters for Real Images (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As scoring around 70k (10% of total samples in the validation set above) of the available samples through Inception V3 takes a very long time, the resulting activations are used to calculate the required statistical parameters for FID (mean and covariance matrix) only once and are then persisted (pickled) for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0269691  1.5217992  2.3997712  ... 0.48547718 0.23129249 0.59598255] [[ 2.45910410e+00  7.15231822e-02  2.31537616e-01 ... -1.07793145e-01\n",
      "   2.55754755e-01  7.01196827e-02]\n",
      " [ 7.15231822e-02  3.87113008e+00  1.51734897e+00 ...  9.13199350e-01\n",
      "  -7.01373881e-02  6.90343250e-02]\n",
      " [ 2.31537616e-01  1.51734897e+00  4.54826168e+00 ...  3.98259162e-01\n",
      "  -3.85136469e-02  1.94263442e-01]\n",
      " ...\n",
      " [-1.07793145e-01  9.13199350e-01  3.98259162e-01 ...  9.08558728e-01\n",
      "  -3.81071381e-02  3.56840394e-03]\n",
      " [ 2.55754755e-01 -7.01373881e-02 -3.85136469e-02 ... -3.81071381e-02\n",
      "   3.05500014e-01  2.00811970e-02]\n",
      " [ 7.01196827e-02  6.90343250e-02  1.94263442e-01 ...  3.56840394e-03\n",
      "   2.00811970e-02  1.98472330e+00]]\n"
     ]
    }
   ],
   "source": [
    "activations = model.predict(dataset_train, batch_size=BATCH_SIZE)\n",
    "mux, sigmax = activations.mean(axis=0), cov(activations, rowvar=False)\n",
    "print(mux, sigmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the code above is a 2048 dimensional vector with all means for each latent space dimension (<i>mux</i>) and a 2048 x 2048 dimensional matriax holding the covariances for each dimension pair of the latent space (<i>sigmax</i>). We will serialize these values into a pickle object so we that this time consuming calculation does not have to be repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n",
      "(2048, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(mux.shape)\n",
    "print(sigmax.shape)\n",
    "\n",
    "# we serialize these reference values into pickle objects\n",
    "with open('fid_reference_values', 'wb') as f:\n",
    "    pickle.dump(mux, f)\n",
    "    pickle.dump(sigmax, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define FID function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images, reference='fid_reference_values'):    # calculate activations for images to compare to established baseline\n",
    "    act = model.predict(images)\n",
    "    # calculate mean and covariance statistics\n",
    "    with open(reference, 'rb') as f:\n",
    "        mu1 = pickle.load(f)\n",
    "        sigma1 = pickle.load(f)\n",
    "        mu2, sigma2 = act.mean(axis=0), np.cov(act, rowvar=False)\n",
    "        # calculate sum squared difference between means\n",
    "        ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "        # calculate sqrt of product between cov\n",
    "        covmean = sqrtm(sigma1.dot(sigma2))\n",
    "        # check and correct imaginary numbers from sqrt\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        # calculate score\n",
    "        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    \n",
    "    return fid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare random generated images (DCGAN) against baseline\n",
    "\n",
    "We load a random subset of generated images from a previous run into a tensor and run that through the calculate_fid() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 files belonging to 2 classes.\n",
      "Using 19 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# load generated images\n",
    "IMAGE_SIZE = (299, 299) # here we specify the expected input size of Inception V3 to let image_dataset_from_directory() automatically resize the images\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_dir = pathlib.Path('/data/output/images/dwarfgan001')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "check = tf.keras.preprocessing.image_dataset_from_directory(  data_dir,\n",
    "                                                                      image_size=IMAGE_SIZE, \n",
    "                                                                      batch_size=BATCH_SIZE, \n",
    "                                                                      #labels=[0.] * len(imgs), # setting all labels to 0 (for 'fake'), not relevant here\n",
    "                                                                      #label_mode=None, # yields float32 type labels\n",
    "                                                                      seed=42,\n",
    "                                                                      validation_split=0.99, #only 20 images available but split has to be < 1 \n",
    "                                                                      subset='validation'\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see that the result is quite large with a FID score of: 280675.25. A perfect imitation would score a FID score close to 0.\n"
     ]
    }
   ],
   "source": [
    "result_dcgan = calculate_fid(model, check)\n",
    "print(f'We see that the result is quite large with a FID score of: {round(result_dcgan,2)}. A perfect imitation would score a FID score close to 0.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FID for Real Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 488044 files belonging to 1 classes.\n",
      "Using 12201 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# load real images\n",
    "IMAGE_SIZE = (299, 299) # here we specify the expected input size of Inception V3 to let image_dataset_from_directory() automatically resize the images\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_dir = pathlib.Path('/data/input/crops_small/')\n",
    "imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "check = tf.keras.preprocessing.image_dataset_from_directory(  data_dir,\n",
    "                                                              image_size=IMAGE_SIZE, \n",
    "                                                              batch_size=BATCH_SIZE, \n",
    "                                                              #labels=[0.] * len(imgs), # setting all labels to 0 (for 'fake'), not relevant here\n",
    "                                                              #label_mode=None, # yields float32 type labels\n",
    "                                                              seed=42,\n",
    "                                                              validation_split=0.025, #only 2.5% of 700'000 images as reference \n",
    "                                                              subset='validation'\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_real = calculate_fid(model, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we see a much lower FID score of: 9096.42. Due to the variety of pictures, a score of 0 is unlikely.\n"
     ]
    }
   ],
   "source": [
    "print(f'Here we see a much lower FID score of: {round(result_real,2)}. Due to the variety of pictures, a score of 0 is unlikely.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare WGAN-GP RUN02 Images to Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# load real images\n",
    "IMAGE_SIZE = (299, 299) # here we specify the expected input size of Inception V3 to let image_dataset_from_directory() automatically resize the images\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_dir = '/data/output/images/WGANGPR02FID/'\n",
    "#imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "check = tf.keras.preprocessing.image_dataset_from_directory(  data_dir,\n",
    "                                                              image_size=IMAGE_SIZE, \n",
    "                                                              batch_size=BATCH_SIZE, \n",
    "                                                              #labels=[0.] * len(imgs), # setting all labels to 0 (for 'fake'), not relevant here\n",
    "                                                              #label_mode=None, # yields float32 type labels\n",
    "                                                              seed=42\n",
    "                                                              #validation_split=0.025, #only 2.5% of 700'000 images as reference \n",
    "                                                              #subset='validation'\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_wgangp = calculate_fid(model, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we see a higher FID score of: 584124.89 compared to the initial FID score of 280675.25 from the DCGAN model and 9096.42 of the real image reference score.\n"
     ]
    }
   ],
   "source": [
    "print(f'Here we see a higher FID score of: {round(result_wgangp,2)} compared to the initial FID score of {round(result_dcgan,2)} from the DCGAN model and {round(result_real, 2)} of the real image reference score.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
