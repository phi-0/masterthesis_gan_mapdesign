{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please choose one of the available models \n",
      "['DCGAN1024', 'DCGAN256', 'WGANGP-RUN01', 'WGANGP-RUN02', 'WGANGP-RUN03', 'WGANGP-RUN05', 'WGANGP-RUN06', 'WGANGP-RUN07', 'WGANGP-RUN08', 'WGANGP-RUN09', 'WGANGP-RUN09Tiles']\n",
      " WGANGP-RUN09Tiles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You chose WGANGP-RUN09Tiles\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "#from keras import backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import PIL\n",
    "import random\n",
    "\n",
    "##### START Initialization #####\n",
    "\n",
    "######################################################################\n",
    "#                       - Available Models -                         #\n",
    "#     The script will download models from the public S3 bucket      #\n",
    "######################################################################\n",
    "# each model (key) contains a list with [path, dimensions] (value)\n",
    "if pathlib.Path('/data/output/models/').exists() and pathlib.Path('/data2/output/models/').exists():\n",
    "    #load local saves if available\n",
    "    models = {'DCGAN1024': ['/data/output/models/dwarfgan001',1024],\n",
    "         'DCGAN256': ['/data/output/models/dwarfgan001-256/',256],\n",
    "         'WGANGP-RUN01': ['/data/output/models/dwarfganWGANGPR01/',256],\n",
    "         'WGANGP-RUN02': ['/data/output/models/dwarfganWGANGPR02/',256],\n",
    "         'WGANGP-RUN03': ['/data/output/models/dwarfganWGANGPR03/',256],\n",
    "         #'WGANGP-RUN04': ['/data2/output/models/dwarfganWGANGPR04/',128],\n",
    "         'WGANGP-RUN05': ['/data2/output/models/dwarfganWGANGPR05/',128],\n",
    "         'WGANGP-RUN06': ['/data2/output/models/dwarfganWGANGPR06/',128],\n",
    "         'WGANGP-RUN07': ['/data2/output/models/dwarfganWGANGPR07/',128],\n",
    "         'WGANGP-RUN08': ['/data2/output/models/dwarfganWGANGPR08/',128],\n",
    "         'WGANGP-RUN09': ['/data2/output/models/dwarfganWGANGPR09NoTiles/',12],\n",
    "         'WGANGP-RUN09Tiles': ['/data2/output/models/dwarfganWGANGPR09Tiles02/',12]\n",
    "    }\n",
    "else:\n",
    "    downloads = ['https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/DCGAN1024/generator-2021-03-17_104329.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/DCGAN256/generator-2021-03-24_174741.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN01/generator-2021-03-31_180243.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN02/generator-2021-04-04_025322.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN03/generator-2021-04-09_011627.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN05/generator-2021-04-17_090503.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN06/generator-2021-04-20_050013.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN07/generator-2021-04-22_230202.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN08/generator-2021-05-01_091206.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN09/generator-2021-05-16_233612.h5'            \n",
    "            ]\n",
    "\n",
    "    wd = os.getcwd()\n",
    "    for i, url in enumerate(downloads):\n",
    "        #create subdirectory if it does not exist yet\n",
    "        if pathlib.Path(f'{wd}/models/model_{i+1}').exists():\n",
    "            pass\n",
    "        else:\n",
    "            if not pathlib.Path(f'{wd}/models').exists():\n",
    "                os.mkdir(f'{wd}/models')\n",
    "            os.mkdir(f'{wd}/models/model_{i+1}')\n",
    "\n",
    "        urllib.request.urlretrieve(url, f'{wd}/models/model_{i+1}/generator.h5')\n",
    "        print(f'Finished downloading save file: {url}')\n",
    "\n",
    "    models = {'DCGAN1024': [f'{wd}/models/model_1/',1024],\n",
    "             'DCGAN256': [f'{wd}/models/model_2/',256],\n",
    "             'WGANGP-RUN01': [f'{wd}/models/model_3/',256],\n",
    "             'WGANGP-RUN02': [f'{wd}/models/model_4/',256],\n",
    "             'WGANGP-RUN03': [f'{wd}/models/model_5/',256],\n",
    "             #'WGANGP-RUN04': ['/data2/output/models/dwarfganWGANGPR04/',128],\n",
    "             'WGANGP-RUN05': [f'{wd}/models/model_6/',128],\n",
    "             'WGANGP-RUN06': [f'{wd}/models/model_7/',128],\n",
    "             'WGANGP-RUN07': [f'{wd}/models/model_8/',128],\n",
    "             'WGANGP-RUN08': [f'{wd}/models/model_9/',128],\n",
    "             'WGANGP-RUN09': [f'{wd}/models/model_10/',12],\n",
    "             'WGANGP-RUN09Tiles': [f'{wd}/models/model_11/',12]\n",
    "             }\n",
    "\n",
    "##### END Initialization #####\n",
    "\n",
    "\n",
    "# Model choice\n",
    "def choose_model():\n",
    "    m = input(f'Please choose one of the available models \\n{[k for k in models.keys()]}\\n')\n",
    "    while m not in models.keys():\n",
    "        m = input(f'you chose an invalid option {m}. Please choose one of the available models:')\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Map generation with selected model\n",
    "def generate_map(model_info, model):#Determine latent dimension\n",
    "    #determine latent dimension size\n",
    "    if model in ['DCGAN1024','DCGAN256','WGANGP-RUN01','WGANGP-RUN02','WGANGP-RUN03','WGANGP-RUN04']:\n",
    "        LATENT_DIM = 100\n",
    "    else:\n",
    "        LATENT_DIM = 128\n",
    "\n",
    "    #determine image dimensions\n",
    "    IMAGE_SIZE = [model_info[model][1], model_info[model][1]]\n",
    "        \n",
    "    # load generator\n",
    "    try:\n",
    "        generator\n",
    "    except NameError:\n",
    "        #get latest generator model save file\n",
    "        folder = pathlib.Path(f'{model_info[model][0]}')\n",
    "        saves = list(folder.glob('generator*'))\n",
    "        latest = max(saves, key=os.path.getctime)\n",
    "        #load latest generator save file\n",
    "        generator = tf.keras.models.load_model(latest)\n",
    "\n",
    "    #Determine mechanism to create new images (special case for Self-Attention Model WGANGP-RUN08)\n",
    "    if model == 'WGANGP-RUN09Tiles':\n",
    "        \n",
    "        #tile load function\n",
    "        def load_tiles():\n",
    "            tiles = []\n",
    "\n",
    "            data_dir = pathlib.Path('/data2/input/tiles/800x600/')\n",
    "            imgs = list(data_dir.glob('*.png'))\n",
    "\n",
    "            # show example sample image (cropped to 128x128)\n",
    "            PIL.Image.open(imgs[random.randint(0,len(imgs))])\n",
    "\n",
    "            for img in imgs:\n",
    "                tiles.append(np.asarray(PIL.Image.open(img)).astype('uint8'))\n",
    "\n",
    "            #reshape\n",
    "            ds_tiles = np.array(tiles)\n",
    "            ds_tiles = ds_tiles.reshape((1,12,10,256))\n",
    "            ds_tiles = ds_tiles.repeat(repeats=512//2, axis=0)\n",
    "\n",
    "            return ds_tiles\n",
    "        \n",
    "        tiles = load_tiles()\n",
    "        tiles = tiles[0].reshape((1,12,10,256)) #resphae into same shape as noise input\n",
    "        noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "        res = np.array(generator.predict({'Generator_Input':noise,'Tiles_Input':tiles})).astype('uint8')\n",
    "        #Rescale\n",
    "        res = res.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "        # Visualize result\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(res[:,0:11,:]) #cropping the image to 12x10 \n",
    "        plt.title(f'Example Generator Output')\n",
    "        plt.axis('off')\n",
    "        \n",
    "    elif model != 'WGANGP-RUN08':    \n",
    "        # generate new example of learned representation in latent space\n",
    "        noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "        res = np.array(generator(noise, training=False)).astype('uint8')\n",
    "\n",
    "        #Rescale\n",
    "        res = res.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "        # Visualize result\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(res)\n",
    "        plt.title(f'Example Generator Output')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        \n",
    "    \n",
    "    #WGANGP-RUN08: slightly different logic due to SelfAttention custom layer\n",
    "    else: \n",
    "        \n",
    "        # we first need to re-initialize the custom layer SelfAttention\n",
    "        # custom layer implementation self-attention\n",
    "        class SelfAttention(tf.keras.layers.Layer):\n",
    "            def __init__(self, dim, batch_size):\n",
    "                super(SelfAttention, self).__init__()\n",
    "                self.dim = dim\n",
    "                self.batch = batch_size\n",
    "                self.gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "                self.k = 8\n",
    "                # Alternative? tf.Variable() to self.add_weight(shape=(1,), trainable=True, initializer=tf.zeros_initializer())\n",
    "                self.scale = tf.Variable(trainable=True, name='AttentionMap_ScaleFactor', initial_value=0.).numpy() #.numpy() necessary to enable serialization into JSON for model save\n",
    "\n",
    "                self.q_conv = Conv2D(filters=self.dim[-1]/self.k, kernel_size=1, data_format='channels_first')\n",
    "                self.k_conv = Conv2D(filters=self.dim[-1]/self.k, kernel_size=1, data_format='channels_first')\n",
    "                self.v_conv = Conv2D(filters=self.dim[-1], kernel_size=1, data_format='channels_first')\n",
    "\n",
    "\n",
    "            def build(self, input_shape):\n",
    "\n",
    "                self.batchsize = input_shape[0]\n",
    "\n",
    "\n",
    "            def call(self, inputs):\n",
    "\n",
    "                self.gpus = 1 # not using distributed training due to difficulties with fixed batch size layer\n",
    "\n",
    "                batchsize, height, width, channels = inputs.shape\n",
    "\n",
    "                #batchsize, height, width, channels = inputs.shape\n",
    "                if batchsize is None:\n",
    "                    batchsize = self.batch\n",
    "\n",
    "                # adjusting to implementation from https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py to use \"channel last\"\n",
    "                inp = tf.reshape(inputs, shape=[tf.cast(batchsize/self.gpus, tf.int32), self.dim[-1], self.dim[1], self.dim[2]])\n",
    "\n",
    "                # linear combination of input to query f(x), key g(x) and value h(x)\n",
    "                query = self.q_conv(inp) #filters = in_dimension//8\n",
    "                key = self.k_conv(inp) #filters = in_dimension//8 ==> TRANSPOSE per Torrado et al. 2019? --> transpose seems not necessary as the matmul() operation within Attention() appears to transpose the second parameters automatically. Paper seems to transpose QUERY rather than KEY!!\n",
    "                value = self.v_conv(inp)\n",
    "\n",
    "                # projections\n",
    "                query = tf.reshape(query, shape=[tf.cast(batchsize/self.gpus, tf.int32), -1, self.dim[1] * self.dim[2]]) # width * height = N\n",
    "                key = tf.reshape(key, shape=[tf.cast(batchsize/self.gpus, tf.int32), -1, self.dim[1] * self.dim[2]]) # width * height = N\n",
    "                value = tf.reshape(value, shape=[tf.cast(batchsize/self.gpus, tf.int32), -1, self.dim[1] * self.dim[2]]) # width * height = N\n",
    "\n",
    "                # matmul transposed query with key\n",
    "                t_query = tf.transpose(query, perm=[0,2,1])\n",
    "                attention = K.batch_dot(t_query, key)\n",
    "                attention = Activation('softmax')(attention)\n",
    "                #print(attention.shape) # B x N x N - OK!\n",
    "\n",
    "                out = K.batch_dot(value, tf.transpose(attention, perm=[0,2,1]))\n",
    "                out = tf.reshape(out, shape=[tf.cast(batchsize/self.gpus, tf.int32), self.dim[-1], self.dim[1], self.dim[2]])\n",
    "                #print(out.shape) # B x C x N - OK!\n",
    "                # what about last 1x1 convolution? https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py does not use it?\n",
    "                #out = Conv2D(filters=out.shape[-1], kernel_size=1)(out)\n",
    "                out = self.scale * out + inp\n",
    "\n",
    "                #print(out.shape) # B x C x H x W - OK!\n",
    "\n",
    "                return tf.reshape(out, shape=[tf.cast(batchsize/self.gpus, tf.int32), self.dim[1], self.dim[2], self.dim[-1]]) # reshape back to \"channel_last\" --> B x H x W x C - OK!\n",
    "\n",
    "            def get_config(self):\n",
    "                \"\"\"\n",
    "                to enable model saving, the behaviour of the \"get_config()\" method \n",
    "                has to be overidden due to the usage of custom attributes\n",
    "\n",
    "                \"\"\"\n",
    "                config = super(SelfAttention, self).get_config()\n",
    "                config.update({\"dim\": self.dim,\n",
    "                               \"batch\": self.batch,\n",
    "                               \"gpus\": self.gpus,\n",
    "                               \"k\": self.k,\n",
    "                               \"scale\": self.scale,\n",
    "                               \"q_conv\": self.q_conv,\n",
    "                               \"k_conv\": self.k_conv,\n",
    "                               \"v_conv\": self.v_conv\n",
    "                              })\n",
    "\n",
    "                return config\n",
    "\n",
    "        # Generate new map\n",
    "        noise = tf.random.normal(shape=(16, LATENT_DIM)) #changed self.num_img to BATCH_SIZE due to ATTENTION MECHANISM\n",
    "        res = generator.predict(noise)\n",
    "\n",
    "        #select 1 and display\n",
    "        for i, img in enumerate(res):\n",
    "            if i == 1:\n",
    "\n",
    "                # Visualize result\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                plt.imshow(img.astype('uint8'))\n",
    "                plt.title(f'Example Generator Output')\n",
    "                plt.axis('off')\n",
    "                \n",
    "\n",
    "# Run script\n",
    "model = choose_model()\n",
    "print(f'You chose {model}')\n",
    "\n",
    "generate_map(models, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
