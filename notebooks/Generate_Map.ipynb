{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please choose one of the available models \n",
      "['DCGAN1024', 'DCGAN256', 'WGANGP-RUN01', 'WGANGP-RUN02', 'WGANGP-RUN03', 'WGANGP-RUN05', 'WGANGP-RUN06', 'WGANGP-RUN07', 'WGANGP-RUN08', 'WGANGP-RUN09', 'WGANGP-RUN09Tiles']\n",
      " WGANGP-RUN09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You chose WGANGP-RUN09\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "#from keras import backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "\n",
    "##### START Initialization #####\n",
    "\n",
    "######################################################################\n",
    "#                       - Available Models -                         #\n",
    "#     The script will download models from the public S3 bucket      #\n",
    "######################################################################\n",
    "# each model (key) contains a list with [path, dimensions] (value)\n",
    "if pathlib.Path('/data/output/models/').exists() and pathlib.Path('/data2/output/models/').exists():\n",
    "    #load local saves if available\n",
    "    models = {'DCGAN1024': ['/data/output/models/dwarfgan001',1024],\n",
    "         'DCGAN256': ['/data/output/models/dwarfgan001-256/',256],\n",
    "         'WGANGP-RUN01': ['/data/output/models/dwarfganWGANGPR01/',256],\n",
    "         'WGANGP-RUN02': ['/data/output/models/dwarfganWGANGPR02/',256],\n",
    "         'WGANGP-RUN03': ['/data/output/models/dwarfganWGANGPR03/',256],\n",
    "         #'WGANGP-RUN04': ['/data2/output/models/dwarfganWGANGPR04/',128],\n",
    "         'WGANGP-RUN05': ['/data2/output/models/dwarfganWGANGPR05/',128],\n",
    "         'WGANGP-RUN06': ['/data2/output/models/dwarfganWGANGPR06/',128],\n",
    "         'WGANGP-RUN07': ['/data2/output/models/dwarfganWGANGPR07/',128],\n",
    "         'WGANGP-RUN08': ['/data2/output/models/dwarfganWGANGPR08/',128],\n",
    "         'WGANGP-RUN09': ['/data2/output/models/dwarfganWGANGPR09NoTiles/',12],\n",
    "         'WGANGP-RUN09Tiles': ['/data2/output/models/dwarfganWGANGPR09Tiles/',12]\n",
    "    }\n",
    "else:\n",
    "    downloads = ['https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/DCGAN1024/generator-2021-03-17_104329.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/DCGAN256/generator-2021-03-24_174741.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN01/generator-2021-03-31_180243.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN02/generator-2021-04-04_025322.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN03/generator-2021-04-09_011627.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN05/generator-2021-04-17_090503.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN06/generator-2021-04-20_050013.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN07/generator-2021-04-22_230202.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN08/generator-2021-05-01_091206.h5',\n",
    "             'https://os.zhdk.cloud.switch.ch/swift/v1/storage_hil/models/WGANGP-RUN09/generator-2021-05-16_233612.h5'            \n",
    "            ]\n",
    "\n",
    "    wd = os.getcwd()\n",
    "    for i, url in enumerate(downloads):\n",
    "        #create subdirectory if it does not exist yet\n",
    "        if pathlib.Path(f'{wd}/models/model_{i+1}').exists():\n",
    "            pass\n",
    "        else:\n",
    "            if not pathlib.Path(f'{wd}/models').exists():\n",
    "                os.mkdir(f'{wd}/models')\n",
    "            os.mkdir(f'{wd}/models/model_{i+1}')\n",
    "\n",
    "        urllib.request.urlretrieve(url, f'{wd}/models/model_{i+1}/generator.h5')\n",
    "        print(f'Finished downloading save file: {url}')\n",
    "\n",
    "    models = {'DCGAN1024': [f'{wd}/models/model_1/',1024],\n",
    "             'DCGAN256': [f'{wd}/models/model_2/',256],\n",
    "             'WGANGP-RUN01': [f'{wd}/models/model_3/',256],\n",
    "             'WGANGP-RUN02': [f'{wd}/models/model_4/',256],\n",
    "             'WGANGP-RUN03': [f'{wd}/models/model_5/',256],\n",
    "             #'WGANGP-RUN04': ['/data2/output/models/dwarfganWGANGPR04/',128],\n",
    "             'WGANGP-RUN05': [f'{wd}/models/model_6/',128],\n",
    "             'WGANGP-RUN06': [f'{wd}/models/model_7/',128],\n",
    "             'WGANGP-RUN07': [f'{wd}/models/model_8/',128],\n",
    "             'WGANGP-RUN08': [f'{wd}/models/model_9/',128],\n",
    "             'WGANGP-RUN09': [f'{wd}/models/model_10/',12],\n",
    "             'WGANGP-RUN09Tiles': [f'{wd}/models/model_11/',12]\n",
    "             }\n",
    "\n",
    "##### END Initialization #####\n",
    "\n",
    "\n",
    "# Model choice\n",
    "def choose_model():\n",
    "    m = input(f'Please choose one of the available models \\n{[k for k in models.keys()]}\\n')\n",
    "    while m not in models.keys():\n",
    "        m = input(f'you chose an invalid option {m}. Please choose one of the available models:')\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Map generation with selected model\n",
    "def generate_map(model_info, model):#Determine latent dimension\n",
    "    #determine latent dimension size\n",
    "    if model in ['DCGAN1024','DCGAN256','WGANGP-RUN01','WGANGP-RUN02','WGANGP-RUN03','WGANGP-RUN04']:\n",
    "        LATENT_DIM = 100\n",
    "    else:\n",
    "        LATENT_DIM = 128\n",
    "\n",
    "    #determine image dimensions\n",
    "    IMAGE_SIZE = [model_info[model][1], model_info[model][1]]\n",
    "        \n",
    "    # load generator\n",
    "    try:\n",
    "        generator\n",
    "    except NameError:\n",
    "        #get latest generator model save file\n",
    "        folder = pathlib.Path(f'{model_info[model][0]}')\n",
    "        saves = list(folder.glob('generator*'))\n",
    "        latest = max(saves, key=os.path.getctime)\n",
    "        #load latest generator save file\n",
    "        generator = tf.keras.models.load_model(latest)\n",
    "\n",
    "    #Determine mechanism to create new images (special case for Self-Attention Model WGANGP-RUN08)\n",
    "    if model != 'WGANGP-RUN08':    \n",
    "        # generate new example of learned representation in latent space\n",
    "        noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "        res = np.array(generator(noise, training=False)).astype('uint8')\n",
    "\n",
    "        #Rescale\n",
    "        res = res.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "        # Visualize result\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(res)\n",
    "        plt.title(f'Example Generator Output')\n",
    "        plt.axis('off')\n",
    "\n",
    "    #WGANGP-RUN08: slightly different logic due to SelfAttention custom layer\n",
    "    else: \n",
    "        \n",
    "        # we first need to re-initialize the custom layer SelfAttention\n",
    "        # custom layer implementation self-attention\n",
    "        class SelfAttention(tf.keras.layers.Layer):\n",
    "            def __init__(self, dim, batch_size):\n",
    "                super(SelfAttention, self).__init__()\n",
    "                self.dim = dim\n",
    "                self.batch = batch_size\n",
    "                self.gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "                self.k = 8\n",
    "                # Alternative? tf.Variable() to self.add_weight(shape=(1,), trainable=True, initializer=tf.zeros_initializer())\n",
    "                self.scale = tf.Variable(trainable=True, name='AttentionMap_ScaleFactor', initial_value=0.).numpy() #.numpy() necessary to enable serialization into JSON for model save\n",
    "\n",
    "                self.q_conv = Conv2D(filters=self.dim[-1]/self.k, kernel_size=1, data_format='channels_first')\n",
    "                self.k_conv = Conv2D(filters=self.dim[-1]/self.k, kernel_size=1, data_format='channels_first')\n",
    "                self.v_conv = Conv2D(filters=self.dim[-1], kernel_size=1, data_format='channels_first')\n",
    "\n",
    "\n",
    "            def build(self, input_shape):\n",
    "\n",
    "                self.batchsize = input_shape[0]\n",
    "\n",
    "\n",
    "            def call(self, inputs):\n",
    "\n",
    "                self.gpus = 1 # not using distributed training due to difficulties with fixed batch size layer\n",
    "\n",
    "                batchsize, height, width, channels = inputs.shape\n",
    "\n",
    "                #batchsize, height, width, channels = inputs.shape\n",
    "                if batchsize is None:\n",
    "                    batchsize = self.batch\n",
    "\n",
    "                # adjusting to implementation from https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py to use \"channel last\"\n",
    "                inp = tf.reshape(inputs, shape=[tf.cast(batchsize/self.gpus, tf.int32), self.dim[-1], self.dim[1], self.dim[2]])\n",
    "\n",
    "                # linear combination of input to query f(x), key g(x) and value h(x)\n",
    "                query = self.q_conv(inp) #filters = in_dimension//8\n",
    "                key = self.k_conv(inp) #filters = in_dimension//8 ==> TRANSPOSE per Torrado et al. 2019? --> transpose seems not necessary as the matmul() operation within Attention() appears to transpose the second parameters automatically. Paper seems to transpose QUERY rather than KEY!!\n",
    "                value = self.v_conv(inp)\n",
    "\n",
    "                # projections\n",
    "                query = tf.reshape(query, shape=[tf.cast(batchsize/self.gpus, tf.int32), -1, self.dim[1] * self.dim[2]]) # width * height = N\n",
    "                key = tf.reshape(key, shape=[tf.cast(batchsize/self.gpus, tf.int32), -1, self.dim[1] * self.dim[2]]) # width * height = N\n",
    "                value = tf.reshape(value, shape=[tf.cast(batchsize/self.gpus, tf.int32), -1, self.dim[1] * self.dim[2]]) # width * height = N\n",
    "\n",
    "                # matmul transposed query with key\n",
    "                t_query = tf.transpose(query, perm=[0,2,1])\n",
    "                attention = K.batch_dot(t_query, key)\n",
    "                attention = Activation('softmax')(attention)\n",
    "                #print(attention.shape) # B x N x N - OK!\n",
    "\n",
    "                out = K.batch_dot(value, tf.transpose(attention, perm=[0,2,1]))\n",
    "                out = tf.reshape(out, shape=[tf.cast(batchsize/self.gpus, tf.int32), self.dim[-1], self.dim[1], self.dim[2]])\n",
    "                #print(out.shape) # B x C x N - OK!\n",
    "                # what about last 1x1 convolution? https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py does not use it?\n",
    "                #out = Conv2D(filters=out.shape[-1], kernel_size=1)(out)\n",
    "                out = self.scale * out + inp\n",
    "\n",
    "                #print(out.shape) # B x C x H x W - OK!\n",
    "\n",
    "                return tf.reshape(out, shape=[tf.cast(batchsize/self.gpus, tf.int32), self.dim[1], self.dim[2], self.dim[-1]]) # reshape back to \"channel_last\" --> B x H x W x C - OK!\n",
    "\n",
    "            def get_config(self):\n",
    "                \"\"\"\n",
    "                to enable model saving, the behaviour of the \"get_config()\" method \n",
    "                has to be overidden due to the usage of custom attributes\n",
    "\n",
    "                \"\"\"\n",
    "                config = super(SelfAttention, self).get_config()\n",
    "                config.update({\"dim\": self.dim,\n",
    "                               \"batch\": self.batch,\n",
    "                               \"gpus\": self.gpus,\n",
    "                               \"k\": self.k,\n",
    "                               \"scale\": self.scale,\n",
    "                               \"q_conv\": self.q_conv,\n",
    "                               \"k_conv\": self.k_conv,\n",
    "                               \"v_conv\": self.v_conv\n",
    "                              })\n",
    "\n",
    "                return config\n",
    "\n",
    "        # Generate new map\n",
    "        noise = tf.random.normal(shape=(16, LATENT_DIM)) #changed self.num_img to BATCH_SIZE due to ATTENTION MECHANISM\n",
    "        res = generator.predict(noise)\n",
    "\n",
    "        #select 1 and display\n",
    "        for i, img in enumerate(res):\n",
    "            if i == 1:\n",
    "\n",
    "                # Visualize result\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                plt.imshow(img.astype('uint8'))\n",
    "                plt.title(f'Example Generator Output')\n",
    "                plt.axis('off')\n",
    "                \n",
    "\n",
    "# Run script\n",
    "model = choose_model()\n",
    "print(f'You chose {model}')\n",
    "\n",
    "generate_map(models, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
